{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbMpqPm1mPC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtlMx4kv1mPK"
      },
      "source": [
        "# Assignment 2 execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feevAAsT1mPL",
        "outputId": "f072fecc-26c8-4e60-c458-3a45322b866d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: pandas in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (1.23.3)\n",
            "Requirement already satisfied: matplotlib in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (3.5.3)\n",
            "Requirement already satisfied: transformers in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (4.25.1)\n",
            "Collecting dataset\n",
            "  Downloading dataset-1.5.2-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tensorflow_addons in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (0.18.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (4.37.1)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: requests in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
            "Collecting sqlalchemy>=1.3.2\n",
            "  Downloading SQLAlchemy-1.4.45-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=0.6.2\n",
            "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.4/210.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting banal>=1.0.1\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: typeguard>=2.7 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow_addons) (2.13.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.9/535.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2022.6.15.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.1)\n",
            "Installing collected packages: banal, Mako, greenlet, sqlalchemy, alembic, dataset\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.1 banal-1.0.6 dataset-1.5.2 greenlet-2.0.1 sqlalchemy-1.4.45\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy matplotlib transformers dataset tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJSsWQg_1mPO"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mXm1Fpq1mPP"
      },
      "source": [
        "### Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oOqlik_a1mPP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sskRUOtB1mPR"
      },
      "outputs": [],
      "source": [
        "data_folder = 'Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CzsJWqI61mPR"
      },
      "outputs": [],
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path=data_folder, url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path=data_folder, url_path=test_url, suffix='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsM9znVV1mPT"
      },
      "source": [
        "### Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VRsmY5Wn1mPV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from os import path\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lI74kMng1mPW"
      },
      "outputs": [],
      "source": [
        "def loadDataset(filename):\n",
        "    with open(path.join(data_folder, filename)) as file_obj:\n",
        "        df = json.load(file_obj)[\"data\"]\n",
        "    print(f'{len(df)} stories / {len(df[0][\"questions\"])} questions in the first row')\n",
        "\n",
        "    storyDType = pd.CategoricalDtype(pd.unique([story[\"story\"] for story in df]))\n",
        "    print(f\"{storyDType.categories.size} distinct stories\")\n",
        "\n",
        "    sourceDType = pd.CategoricalDtype(pd.unique([story[\"source\"] for story in df]))\n",
        "    print(f\"{sourceDType.categories.size} distinct sources: {sourceDType.categories}\")\n",
        "\n",
        "    df = np.array([\n",
        "        [\n",
        "            sourceDType.categories.get_loc(story[\"source\"]), # Sources factorization\n",
        "            storyDType.categories.get_loc(story[\"story\"]), # Sources factorization\n",
        "            story[\"questions\"][question_index][\"input_text\"],\n",
        "            story[\"answers\"][question_index][\"input_text\"],\n",
        "            story[\"answers\"][question_index][\"span_text\"],\n",
        "        ]\n",
        "        for story in df\n",
        "        for question_index in range(len(story[\"questions\"]))\n",
        "        if story[\"answers\"][question_index][\"input_text\"] != 'unknown'\n",
        "    ])\n",
        "    print(f'{df.shape} question-answer pairs x columns')\n",
        "    print(f'First row: {df[0]}')\n",
        "    \n",
        "    # https://marcobonzanini.com/2021/09/15/tips-for-saving-memory-with-pandas/\n",
        "    # https://pandas.pydata.org/docs/user_guide/categorical.html\n",
        "    df = pd.DataFrame({\n",
        "        \"source\": pd.Series(pd.Categorical.from_codes(df[:,0].astype(np.int16), dtype=sourceDType)),\n",
        "        \"p\": pd.Series(pd.Categorical.from_codes(df[:,1].astype(np.int16), dtype=storyDType)),\n",
        "        \"q\": df[:,2],\n",
        "        \"a\": df[:,3],\n",
        "        \"span\": df[:,4],\n",
        "    })\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsZrgF5d1mPX",
        "outputId": "be4d1ff5-50ea-4be6-acae-e94a3c9e7a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7199 stories / 20 questions in the first row\n",
            "6605 distinct stories\n",
            "5 distinct sources: Index(['wikipedia', 'cnn', 'gutenberg', 'race', 'mctest'], dtype='object')\n",
            "(107276, 5) question-answer pairs x columns\n",
            "First row: ['0' '0' 'When was the Vat formally opened?'\n",
            " 'It was formally established in 1475' 'Formally established in 1475']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "source    107276\n",
              "p         107276\n",
              "q         107276\n",
              "a         107276\n",
              "span      107276\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = loadDataset(\"train.json\")\n",
        "train_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOQSe3Sc1mPY",
        "outputId": "edeb81f9-a1d5-4428-c0f9-2d02209b097d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6605"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.unique(train_df[\"p\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJaLGY_p1mPZ",
        "outputId": "9cb5b869-dc9f-4970-f598-e08c253f6e5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99470"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.unique(train_df[\"span\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ml0A1b1mPZ",
        "outputId": "0828de65-b08d-4ba7-bf8f-8d49e4d1b648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.unique(train_df[\"source\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "G9VjurfZ1mPZ",
        "outputId": "fc167add-da37-46c7-f06f-4a06671db4a6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>p</th>\n",
              "      <th>q</th>\n",
              "      <th>a</th>\n",
              "      <th>span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>When was the Vat formally opened?</td>\n",
              "      <td>It was formally established in 1475</td>\n",
              "      <td>Formally established in 1475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what is the library for?</td>\n",
              "      <td>research</td>\n",
              "      <td>he Vatican Library is a research library</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>for what subjects?</td>\n",
              "      <td>history, and law</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>and?</td>\n",
              "      <td>philosophy, science and theology</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what was started in 2014?</td>\n",
              "      <td>a  project</td>\n",
              "      <td>March 2014, the Vatican Library began an initi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      source                                                  p  \\\n",
              "0  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "1  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "2  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "3  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "4  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "\n",
              "                                   q                                    a  \\\n",
              "0  When was the Vat formally opened?  It was formally established in 1475   \n",
              "1           what is the library for?                             research   \n",
              "2                 for what subjects?                     history, and law   \n",
              "3                               and?     philosophy, science and theology   \n",
              "4          what was started in 2014?                           a  project   \n",
              "\n",
              "                                                span  \n",
              "0                       Formally established in 1475  \n",
              "1           he Vatican Library is a research library  \n",
              "2  Vatican Library is a research library for hist...  \n",
              "3  Vatican Library is a research library for hist...  \n",
              "4  March 2014, the Vatican Library began an initi...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrRkHhES1mPa",
        "outputId": "218ad7b4-c182-4874-c4b0-875dd094e372"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index          128\n",
              "source      107764\n",
              "p         14241201\n",
              "q          9110271\n",
              "a          7714559\n",
              "span      12090637\n",
              "dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.memory_usage(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v5aer1n41mPa"
      },
      "outputs": [],
      "source": [
        "#test_df = loadDataset(\"test.json\")\n",
        "#test_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRs7YHKY1mPa"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0GmOXam1mPb"
      },
      "source": [
        "### Check unanswerable questions in the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aq524zd1mPb",
        "outputId": "883fe58a-cd80-4177-8288-5b1af3372e6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idx = (train_df.a == 'unknown')\n",
        "unanswerable = train_df[idx]\n",
        "unanswerable.q.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTdjLFYI1mPb"
      },
      "source": [
        "All unanswerable questions in the Train Dataset have been already removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maBslb2n1mPb"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "VH5Ep77e1mPb",
        "outputId": "32875116-43ba-458c-c9dc-53d0f233f914"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"Lassiter, will you be my rider?\" Jane had asked him. \\n\\n\"I reckon so,\" he had replied. \\n\\nFew as the words were, Jane knew how infinitely much they implied. She wanted him to take charge of her cattle and horse and ranges, and save them if that were possible. Yet, though she could not have spoken aloud all she meant, she was perfectly honest with herself. Whatever the price to be paid, she must keep Lassiter close to her; she must shield from him the man who had led Milly Erne to Cottonwoods. In her fear she so controlled her mind that she did not whisper this Mormon\\'s name to her own soul, she did not even think it. Besides, beyond this thing she regarded as a sacred obligation thrust upon her, was the need of a helper, of a friend, of a champion in this critical time. If she could rule this gun-man, as Venters had called him, if she could even keep him from shedding blood, what strategy to play his flame and his presence against the game of oppression her churchmen were waging against her? Never would she forget the effect on Tull and his men when Venters shouted Lassiter\\'s name. If she could not wholly control Lassiter, then what she could do might put off the fatal day. \\n\\nOne of her safe racers was a dark bay, and she called him Bells because of the way he struck his iron shoes on the stones. When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes. A rider\\'s love of a thoroughbred shone in them. Round and round Bells he walked, plainly weakening all the time in his determination not to take one of Jane\\'s favorite racers. '"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"p\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zYcgBVcG1mPc",
        "outputId": "ce6f5d0a-c187-4974-d064-67c85083e179"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Was Lassiter impressed with the horse?'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"q\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5zvn9Tv01mPc",
        "outputId": "daccf01a-e414-4589-e90d-ffc44ce82d7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"a\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NokHfoLm1mPc",
        "outputId": "ee9e76c8-d783-4820-8642-6c21d7cf68eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"span\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JskMPh-Y1mPc",
        "outputId": "de1a0b5d-e4f7-4187-c83a-044861ea1c10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'gutenberg'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"source\"][42]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution statistics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8lIuR9RO1mPd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_df[\"source\"].hist()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Occurrences of 25 most popular stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_6463/3127963000.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  story_counts[:25].plot(kind=\"bar\", figsize=(15,5))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "story_counts = train_df[\"p\"].cat.codes.value_counts(sort=True)\n",
        "story_counts[:25].plot(kind=\"bar\", figsize=(15,5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Occurrences of 25 least popular stories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_6463/3421603498.py:1: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  story_counts[-25:-1].plot(kind=\"bar\", figsize=(15,5))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "story_counts[-25:-1].plot(kind=\"bar\", figsize=(15,5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Histogram of story popularities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "story_counts.hist(log=True,bins=75,figsize=(15,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm2QCJbR1mPd"
      },
      "source": [
        "## Train-Validation-Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lmr_hj9-1mPd"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SICbZ6vB1mPd"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wuh0_ic1mPd"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4pKuTsYt1mPd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-31 16:31:49.632965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-12-31 16:31:51.065293: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:51.065318: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-12-31 16:31:51.204447: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-12-31 16:31:53.199587: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:53.200261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:53.200292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from typing import List, Dict, Callable\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YutAeLih1mPe"
      },
      "outputs": [],
      "source": [
        "def predict_data(model: keras.Model,\n",
        "                x: np.ndarray,\n",
        "                prediction_info: Dict):\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "    print(f'Starting prediction: \\n{prediction_info}')\n",
        "    print(f'Predicting on {x.shape[0]} samples')\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CS_dd2On1mPe"
      },
      "outputs": [],
      "source": [
        "def compute_f1(model: keras.Model, \n",
        "             x: np.ndarray, \n",
        "             y: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute F1_score on the given data with corresponding labels\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: data in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "\n",
        "    :return\n",
        "        score: f1_macro_score\n",
        "    \"\"\"\n",
        "    #predictions on the x set\n",
        "    prediction_info = {\n",
        "        'batch_size': 64,\n",
        "        'verbose': 1\n",
        "    }\n",
        "    y_pred = predict_data(model=model, x=x, prediction_info=prediction_info)\n",
        "\n",
        "    #compute argmax to take the best class for each sample\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    #compute the f1_macro\n",
        "    score = f1_score(y, y_pred, average ='macro')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1-akLLJo1mPe"
      },
      "outputs": [],
      "source": [
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "61_GSxvB1mPe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-31 16:31:57.309692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2022-12-31 16:31:57.309924: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.310131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.310381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.358632: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.358743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.358860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-12-31 16:31:57.358877: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm import tqdm\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aFTVbjH1mPf"
      },
      "source": [
        "### Question generation $f_\\theta(P, Q)$ with text passage $P$ and question $Q$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxoRFA2j1mPf"
      },
      "source": [
        "### Seq2Seq LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OZecoZhu1mPf"
      },
      "outputs": [],
      "source": [
        "class MyTrainer(object):\n",
        "    \"\"\"\n",
        "    Simple wrapper class\n",
        "\n",
        "    train_op -> uses tf.GradientTape to compute the loss\n",
        "    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, max_length):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.max_length = max_length\n",
        "        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
        "\n",
        "    @tf.function\n",
        "    def compute_loss(self, logits, target):\n",
        "        loss = self.ce(y_true=target, y_pred=logits)\n",
        "        mask = tf.logical_not(tf.math.equal(target, 0))\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_op(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n",
        "                                                                 'hidden_state': inputs['encoder_state']})\n",
        "\n",
        "            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n",
        "            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n",
        "\n",
        "            decoder.attention.setup_memory(encoder_output)\n",
        "\n",
        "            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n",
        "            predicted = self.decoder({'input_ids': decoder_input,\n",
        "                                      'initial_state': decoder_initial_state}).rnn_output\n",
        "\n",
        "            loss = self.compute_loss(logits=predicted, target=real_target)\n",
        "\n",
        "        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
        "        return loss, grads\n",
        "\n",
        "    @tf.function\n",
        "    def batch_fit(self, inputs):\n",
        "        loss, grads = self.train_op(inputs=inputs)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def generate(self, input_ids):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        encoder_initial_state = [tf.zeros((batch_size, self.encoder.encoder_units)),\n",
        "                                 tf.zeros((batch_size, self.encoder.encoder_units))]\n",
        "        encoder_output, encoder_h, encoder_s = self.encoder({\n",
        "            'input_ids': input_ids,\n",
        "            'hidden_state': encoder_initial_state\n",
        "        })\n",
        "\n",
        "        start_tokens = tf.fill([batch_size], tokenizer.word_index['<start>'])\n",
        "        end_token = tokenizer.word_index['<end>']\n",
        "\n",
        "        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n",
        "                                                    sampler=greedy_sampler,\n",
        "                                                    output_layer=self.decoder.generation_dense,\n",
        "                                                    maximum_iterations=self.max_length)\n",
        "        self.decoder.attention.setup_memory(encoder_output)\n",
        "\n",
        "        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n",
        "        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n",
        "        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n",
        "                                         start_tokens=start_tokens,\n",
        "                                         end_token=end_token,\n",
        "                                         initial_state=decoder_initial_state)\n",
        "        return outputs\n",
        "\n",
        "    def translate(self, generated):\n",
        "        return tokenizer.sequences_to_texts(generated.sample_id.numpy())\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encoder_units = encoder_units\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                   output_dim=embedding_dim)\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(self.encoder_units,\n",
        "                                                 return_sequences=True,\n",
        "                                                 return_state=True)\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        input_ids = inputs['input_ids']\n",
        "        input_emb = self.embedding(input_ids)\n",
        "        encoder_output, lstm_hidden, lstm_states = self.encoder_lstm(input_emb, initial_state=inputs['hidden_state'])\n",
        "        return encoder_output, lstm_hidden, lstm_states\n",
        "\n",
        "    def initialize(self, batch_size):\n",
        "        return [tf.zeros((batch_size, self.encoder_units)), tf.zeros((batch_size, self.encoder_units))]\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                   output_dim=embedding_dim)\n",
        "        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n",
        "\n",
        "        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n",
        "                                                       memory=None,\n",
        "                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n",
        "\n",
        "        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n",
        "                                                                 self.attention,\n",
        "                                                                 attention_layer_size=self.decoder_units)\n",
        "\n",
        "        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n",
        "                                                sampler=self.sampler,\n",
        "                                                output_layer=self.generation_dense)\n",
        "\n",
        "    def build_initial_state(self, batch_size, encoder_state):\n",
        "        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "        initial_state = initial_state.clone(cell_state=encoder_state)\n",
        "        return initial_state\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        input_ids = inputs['input_ids']\n",
        "        input_emb = self.embedding(input_ids)\n",
        "        decoder_output, _, _ = self.decoder(input_emb,\n",
        "                                            initial_state=inputs['initial_state'],\n",
        "                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n",
        "        return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "t29CV-S41mPg",
        "outputId": "b15fcf00-b73a-41ca-f7aa-954a058ad5d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-31 16:31:57.526536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 6, 16) -- (2, 16) -- (2, 16)\n",
            "(2, 5, 16)\n"
          ]
        }
      ],
      "source": [
        "# Sample\n",
        "input_sample = [\n",
        "    \"hello there how is it going\",\n",
        "    \"this assignment is hellish\"\n",
        "]\n",
        "output_sample = [\n",
        "    \"<start> it is going well <end>\",\n",
        "    \"<start> I agree <end>\"\n",
        "]\n",
        "\n",
        "batch_size = len(input_sample)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(input_sample + output_sample)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "encoded_input_sample = tokenizer.texts_to_sequences(input_sample)\n",
        "max_input_length = max([len(item) for item in encoded_input_sample])\n",
        "\n",
        "encoded_output_sample = tokenizer.texts_to_sequences(output_sample)\n",
        "max_output_length = max([len(item) for item in encoded_output_sample])\n",
        "\n",
        "max_sequence_length = max(max_input_length, max_output_length)\n",
        "\n",
        "encoded_input_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_input_sample,\n",
        "                                                                        padding='post',\n",
        "                                                                        maxlen=max_sequence_length)\n",
        "encoded_output_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_output_sample,\n",
        "                                                                        padding='post',\n",
        "                                                                        maxlen=max_sequence_length)\n",
        "\n",
        "# Test encoder\n",
        "encoder = Encoder(vocab_size=vocab_size,\n",
        "                    embedding_dim=50,\n",
        "                    encoder_units=16)\n",
        "\n",
        "sample_hidden = encoder.initialize(batch_size=batch_size)\n",
        "encoder_sample_batch = {\n",
        "    'input_ids': tf.convert_to_tensor(encoded_input_sample, dtype=tf.int32),\n",
        "    'hidden_state': sample_hidden\n",
        "}\n",
        "\n",
        "sample_output, sample_h, sample_c = encoder(inputs=encoder_sample_batch)\n",
        "print(f'{sample_output.shape} -- {sample_h.shape} -- {sample_c.shape}')\n",
        "\n",
        "# Test decoder\n",
        "decoder = Decoder(vocab_size=vocab_size,\n",
        "                    embedding_dim=50,\n",
        "                    decoder_units=16,\n",
        "                    batch_size=batch_size,\n",
        "                    max_sequence_length=max_sequence_length)\n",
        "decoder.attention.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(batch_size, [sample_h, sample_c])\n",
        "\n",
        "decoder_sample_batch = {\n",
        "    'input_ids': tf.convert_to_tensor(encoded_output_sample, tf.int32),\n",
        "    'initial_state': initial_state\n",
        "}\n",
        "sample_decoder_outputs = decoder(decoder_sample_batch).rnn_output\n",
        "print(f'{sample_decoder_outputs.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SZhSq6V41mPg"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "trainer = MyTrainer(encoder=encoder,\n",
        "                    decoder=decoder,\n",
        "                    max_length=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "d5lLS5pc1mPg",
        "outputId": "d8da62b6-89fe-47fe-ef85-061c004f53c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 2.215960741043091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:07<01:30,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['it it <end> this <UNK> it', 'going i agree agree it it']\n",
            "Loss - 2.2101049423217773\n",
            "Translated - ['it it it <end> this it', 'i agree agree it it <end>']\n",
            "Loss - 2.204186201095581\n",
            "Translated - ['it it it it it it', 'i agree it it <end> agree']\n",
            "Loss - 2.1981472969055176\n",
            "Translated - ['it it it it it it', 'i agree it it <end> <end>']\n",
            "Loss - 2.1919312477111816\n",
            "Translated - ['it it it it it it', 'i agree it it <end> <end>']\n",
            "Loss - 2.18548583984375\n",
            "Translated - ['it it it it it it', 'i agree it it <end> <end>']\n",
            "Loss - 2.1787631511688232\n",
            "Translated - ['it it it it it it', 'i agree it it <end> <end>']\n",
            "Loss - 2.1717166900634766\n",
            "Translated - ['it it it it it it', 'i <end> agree <end> <end> <end>']\n",
            "Loss - 2.1643009185791016\n",
            "Translated - ['it it it it it it', 'i <end> <end> agree <end> <end>']\n",
            "Loss - 2.1564693450927734\n",
            "Translated - ['it it it it it it', 'i <end> <end> <end> <end> <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 16/100 [00:08<00:21,  3.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 2.1481735706329346\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.13936185836792\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.129979133605957\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.119966506958008\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.1092593669891357\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.097790241241455\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.085484027862549\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.0722622871398926\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.058039903640747\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 27/100 [00:08<00:07,  9.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 2.0427277088165283\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.026232957839966\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 2.008460283279419\n",
            "Translated - ['it it it it it it', 'it it it it it it']\n",
            "Loss - 1.989315390586853\n",
            "Translated - ['it it it it it it', 'it it it it <end> <end>']\n",
            "Loss - 1.9687080383300781\n",
            "Translated - ['it it it it it it', 'it it it <end> <end> <end>']\n",
            "Loss - 1.946556806564331\n",
            "Translated - ['it it it it it it', 'it it <end> <end> <end> <end>']\n",
            "Loss - 1.9227972030639648\n",
            "Translated - ['it it it it it it', 'it it <end> <end> <end> <end>']\n",
            "Loss - 1.8973890542984009\n",
            "Translated - ['it it it it it it', 'it it <end> <end> <end> <end>']\n",
            "Loss - 1.8703285455703735\n",
            "Translated - ['it it it it it it', 'it it <end> <end> <end> <end>']\n",
            "Loss - 1.8416589498519897\n",
            "Translated - ['it it it it it it', 'it it <end> <end> <end> <end>']\n",
            "Loss - 1.8114821910858154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 38/100 [00:08<00:03, 17.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['it it it it it <end>', 'it <end> <end> <end> <end> <end>']\n",
            "Loss - 1.7799694538116455\n",
            "Translated - ['it it it it <end>', 'it <end> <end> <end> <end>']\n",
            "Loss - 1.7473640441894531\n",
            "Translated - ['it it it <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.7139848470687866\n",
            "Translated - ['it it it <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.6802200078964233\n",
            "Translated - ['it it it <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.6465171575546265\n",
            "Translated - ['it it <end>', 'it <end> <end>']\n",
            "Loss - 1.6133644580841064\n",
            "Translated - ['it it <end>', 'it <end> <end>']\n",
            "Loss - 1.5812695026397705\n",
            "Translated - ['it it <end>', 'it <end> <end>']\n",
            "Loss - 1.5507242679595947\n",
            "Translated - ['it it <end>', 'it <end> <end>']\n",
            "Loss - 1.5221532583236694\n",
            "Translated - ['it it <end>', 'it <end> <end>']\n",
            "Loss - 1.4958441257476807\n",
            "Translated - ['it <end>', 'it <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 48/100 [00:08<00:02, 25.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 1.4718739986419678\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.4500696659088135\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.430027723312378\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.4111862182617188\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.392928123474121\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.374693751335144\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.3560700416564941\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.3368308544158936\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.3169281482696533\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.2964471578598022\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.2755507230758667\n",
            "Translated - ['it <end>', 'it <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 60/100 [00:08<00:01, 36.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 1.254421591758728\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.233214259147644\n",
            "Translated - ['it <end>', 'it <end>']\n",
            "Loss - 1.2120234966278076\n",
            "Translated - ['it it well <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.190889596939087\n",
            "Translated - ['it it well <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.1698271036148071\n",
            "Translated - ['it it well <end>', 'it <end> <end> <end>']\n",
            "Loss - 1.148860216140747\n",
            "Translated - ['it it well <end>', 'i <end> <end> <end>']\n",
            "Loss - 1.128030776977539\n",
            "Translated - ['it it well <end>', 'i <end> <end> <end>']\n",
            "Loss - 1.1073830127716064\n",
            "Translated - ['it is well <end>', 'i <end> <end> <end>']\n",
            "Loss - 1.086936593055725\n",
            "Translated - ['it going well <end>', 'i <end> <end> <end>']\n",
            "Loss - 1.0666611194610596\n",
            "Translated - ['it going well <end>', 'i agree <end> <end>']\n",
            "Loss - 1.046464204788208\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [00:09<00:00, 46.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 1.026198387145996\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 1.0056911706924438\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.9847881197929382\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.9633939862251282\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.9414979815483093\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.9191766977310181\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.8965771794319153\n",
            "Translated - ['it going going <end>', 'i agree <end> <end>']\n",
            "Loss - 0.8738818168640137\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.8512600660324097\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.8288224935531616\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.8066062927246094\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.7846027612686157\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.7628008127212524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [00:09<00:00, 52.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.7412184476852417\n",
            "Translated - ['it going going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.7199094295501709\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6989539861679077\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6784437894821167\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6584662199020386\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6390868425369263\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6203412413597107\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.6022309064865112\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.5847277641296387\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.567787766456604\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.55136638879776\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.5354311466217041\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 0.5199663043022156\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.5049706101417542\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.49044856429100037\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.47640281915664673\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.46282821893692017\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.44971227645874023\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.43703755736351013\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.4247874319553375\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.41294923424720764\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.40151482820510864\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.3904794752597809\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.3798389434814453\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n",
            "Loss - 0.36958619952201843\n",
            "Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    encoder_hidden_state = encoder.initialize(batch_size=batch_size)\n",
        "    batch = {\n",
        "        'encoder_input_ids': encoded_input_sample,\n",
        "        'encoder_state': encoder_hidden_state,\n",
        "        'decoder_target': encoded_output_sample\n",
        "    }\n",
        "    loss = trainer.batch_fit(batch)\n",
        "    print(f'Loss - {loss}')\n",
        "\n",
        "    generated = trainer.generate(input_ids=encoded_input_sample)\n",
        "    translated = trainer.translate(generated)\n",
        "    print(f'Translated - {translated}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Akd5eb5N1mPh"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia7JgdiF1mPh"
      },
      "source": [
        "### Seq2Seq Bert-Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tIz7Z9QV1mPh"
      },
      "outputs": [],
      "source": [
        "class MyTrainer(object):\n",
        "    \"\"\"\n",
        "    Simple wrapper class\n",
        "\n",
        "    train_op -> uses tf.GradientTape to compute the loss\n",
        "    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, max_length):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.max_length = max_length\n",
        "        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
        "\n",
        "    @tf.function\n",
        "    def compute_loss(self, logits, target):\n",
        "        loss = self.ce(y_true=target, y_pred=logits)\n",
        "        mask = tf.logical_not(tf.math.equal(target, 0))\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_op(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n",
        "                                                                 'attention_mask': inputs['encoder_attention_mask']})\n",
        "\n",
        "            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n",
        "            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n",
        "\n",
        "            decoder.attention.setup_memory(encoder_output)\n",
        "\n",
        "            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n",
        "            predicted = self.decoder({'input_ids': decoder_input,\n",
        "                                      'initial_state': decoder_initial_state}).rnn_output\n",
        "\n",
        "            loss = self.compute_loss(logits=predicted, target=real_target)\n",
        "\n",
        "        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
        "        return loss, grads\n",
        "\n",
        "    @tf.function\n",
        "    def batch_fit(self, inputs):\n",
        "        loss, grads = self.train_op(inputs=inputs)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    # @tf.function\n",
        "    def generate(self, input_ids, attention_mask=None):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        encoder_output, encoder_h, encoder_s = self.encoder({\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        })\n",
        "\n",
        "        start_tokens = tf.fill([batch_size], output_tokenizer.word_index['<start>'])\n",
        "        end_token = output_tokenizer.word_index['<end>']\n",
        "\n",
        "        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n",
        "                                                    sampler=greedy_sampler,\n",
        "                                                    output_layer=self.decoder.generation_dense,\n",
        "                                                    maximum_iterations=self.max_length)\n",
        "        self.decoder.attention.setup_memory(encoder_output)\n",
        "\n",
        "        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n",
        "        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n",
        "        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n",
        "                                         start_tokens=start_tokens,\n",
        "                                         end_token=end_token,\n",
        "                                         initial_state=decoder_initial_state)\n",
        "        return outputs\n",
        "\n",
        "    def translate(self, generated):\n",
        "        return output_tokenizer.sequences_to_texts(generated.sample_id.numpy())\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, model_name, decoder_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
        "        self.reducer = tf.keras.layers.Dense(decoder_units)\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        model_output = self.model(inputs)\n",
        "        all_outputs = model_output[0]\n",
        "        pooled_output = model_output[1]\n",
        "        pooled_output = self.reducer(pooled_output)\n",
        "        return all_outputs, pooled_output, pooled_output\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                   output_dim=embedding_dim)\n",
        "        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n",
        "\n",
        "        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n",
        "                                                       memory=None,\n",
        "                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n",
        "\n",
        "        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n",
        "                                                                 self.attention,\n",
        "                                                                 attention_layer_size=self.decoder_units)\n",
        "\n",
        "        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n",
        "                                                sampler=self.sampler,\n",
        "                                                output_layer=self.generation_dense)\n",
        "\n",
        "    def build_initial_state(self, batch_size, encoder_state):\n",
        "        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
        "        initial_state = initial_state.clone(cell_state=encoder_state)\n",
        "        return initial_state\n",
        "\n",
        "    def call(self, inputs, training=False, **kwargs):\n",
        "        input_ids = inputs['input_ids']\n",
        "        input_emb = self.embedding(input_ids)\n",
        "        decoder_output, _, _ = self.decoder(input_emb,\n",
        "                                            initial_state=inputs['initial_state'],\n",
        "                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n",
        "        return decoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vV1rlQqR1mPi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForQuestionAnswering, AutoTokenizer, AutoConfig\n",
        "\n",
        "model_name = 'prajjwal1/bert-tiny'\n",
        "\n",
        "#config = AutoConfig.from_pretrained(model_name)\n",
        "#model = BertForQuestionAnswering.from_pretrained(model_name, config=config)\n",
        "input_tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswqcr_m1mPi"
      },
      "source": [
        "The next block of code is an example of encoding of a question-context pair: in this case, the question is the first part of the encoding, and the context is the second part. There are two special tokens: [CLS] token at the start of the encoding, [SEP] token between the question and the context, and at the end of the encoding.\n",
        "\n",
        "In this case the context is the *span*, to provide a better example that explains the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gxqLjPlL1mPi",
        "outputId": "8b7d0cea-e17f-44ac-abb6-290888dc468a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Was Lassiter impressed with the horse?\n",
            "When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes.\n",
            "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
            "101\t[CLS]\n",
            "2001\twas\n",
            "27333\tlass\n",
            "21646\t##iter\n",
            "7622\timpressed\n",
            "2007\twith\n",
            "1996\tthe\n",
            "3586\thorse\n",
            "1029\t?\n",
            "102\t[SEP]\n",
            "2043\twhen\n",
            "15333\tje\n",
            "4103\t##rd\n",
            "2419\tled\n",
            "2041\tout\n",
            "2023\tthis\n",
            "10944\tslender\n",
            "1010\t,\n",
            "17950\tbeautifully\n",
            "2328\tbuilt\n",
            "3586\thorse\n",
            "27333\tlass\n",
            "21646\t##iter\n",
            "3402\tsuddenly\n",
            "2150\tbecame\n",
            "2035\tall\n",
            "2159\teyes\n",
            "1012\t.\n",
            "102\t[SEP]\n"
          ]
        }
      ],
      "source": [
        "line = 42\n",
        "\n",
        "encoded_question = input_tokenizer(train_df['q'][line], return_tensors='tf', padding=True)\n",
        "print(train_df['q'][line])\n",
        "\n",
        "encoded_span = input_tokenizer(train_df['span'][line], return_tensors='tf', padding=True)\n",
        "print(train_df['span'][line])\n",
        "\n",
        "encoded_qs = input_tokenizer(train_df['q'][line], train_df['span'][line], return_tensors='tf', padding=True)\n",
        "\n",
        "print('= '*40)\n",
        "for idx, tok in zip(encoded_qs.input_ids.numpy()[0], input_tokenizer.convert_ids_to_tokens(encoded_qs.input_ids[0])):\n",
        "    print(\"{}\\t{}\".format(idx, tok))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZk4xQk1mPi"
      },
      "source": [
        "Lets encode a part of the dataset in sentences of: [CLS] question [SEP] passage [SEP]. Otherwise, the training would be very slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xBycfrq51mPj"
      },
      "outputs": [],
      "source": [
        "max_length = 512  # The maximum length of a feature (question and context)\n",
        "doc_stride = (\n",
        "    128  # The authorized overlap between two part of the context when splitting\n",
        ")\n",
        "sentences = 20\n",
        "sample = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ZhjywqdM1mPj"
      },
      "outputs": [],
      "source": [
        "# Input\n",
        "qs = train_df['q'][range(sentences)] # questions\n",
        "cs = train_df['p'][range(sentences)] # contexts\n",
        "\n",
        "batch_size = len(qs)\n",
        "\n",
        "encoded_inputs = input_tokenizer(\n",
        "    qs.values.tolist(),\n",
        "    cs.values.tolist(),\n",
        "    #train_df['q'].values.tolist(),\n",
        "    #train_df['p'].values.tolist(),\n",
        "    truncation=\"only_second\",\n",
        "    max_length=max_length,\n",
        "    stride=doc_stride,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=\"max_length\",\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "input_ids, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask\n",
        "max_input_length = input_ids.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "emIgycXc1mPk",
        "outputId": "c171e250-6167-456b-deb7-761b923cbcce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_input_length: 512\n",
            "encoded_inputs shape = (20, 512)\n"
          ]
        }
      ],
      "source": [
        "print(\"max_input_length:\", max_input_length)\n",
        "print(\"encoded_inputs shape =\", encoded_inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cpxq9qK1mPl"
      },
      "source": [
        "The 'token_type_ids' encodes wether the encoded id is part of the question (=0) or the context (=1). The Attention Mask indicates if the input is needed (=1) or it's padding (=0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIGwPsUg1mPm"
      },
      "source": [
        "Prepare also the expected outputs, for the training (this code follows the example given by the tutors, but I'm not convinced that this is the proper formatting for a QA Bert model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "APmOG4gv1mPm",
        "outputId": "77cb34f3-579d-4a83-d2d7-065f8cb61331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 30, 5, 31, 10, 5, 32, 33, 3]\n"
          ]
        }
      ],
      "source": [
        "# Output\n",
        "outputs = \"<start> \" + train_df['a'][range(sentences)] + \" <end>\"\n",
        "\n",
        "output_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\n",
        "output_tokenizer.fit_on_texts(outputs)\n",
        "\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
        "\n",
        "encoded_output = output_tokenizer.texts_to_sequences(outputs)\n",
        "print(encoded_output[sample])\n",
        "max_output_length = max([len(item) for item in encoded_output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "t4wdmkDs1mPn",
        "outputId": "17d82207-b850-4b54-f679-e886186ee5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_output_length: 11\n",
            "max_sequence_length: 512\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = max(max_input_length, max_output_length)\n",
        "\n",
        "print(\"max_output_length: {}\".format(max_output_length))\n",
        "print(\"max_sequence_length: {}\".format(max_sequence_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cNunTdpj1mPn",
        "outputId": "341ad36a-a211-4744-818d-8c4785f05a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2 30  5 31 10  5 32 33  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ],
      "source": [
        "encoded_output = tf.keras.preprocessing.sequence.pad_sequences(encoded_output,\n",
        "                                                                        padding='post',\n",
        "                                                                        maxlen=max_sequence_length)\n",
        "print(encoded_output[sample])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-G1IKYO61mPo",
        "outputId": "51413bbc-b166-4320-8cbb-11d6ef001804"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 512, 128) - (20, 16) - (20, 16)\n"
          ]
        }
      ],
      "source": [
        "# Test encoder\n",
        "encoder = Encoder(model_name=model_name,\n",
        "                    decoder_units=16)\n",
        "encoder_output, encoder_h, encoder_s = encoder({'input_ids': input_ids,\n",
        "                                                'attention_mask': attention_mask})\n",
        "print(f'{encoder_output.shape} - {encoder_h.shape} - {encoder_s.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zkKbWjm11mPo",
        "outputId": "b0e3ad8e-c144-4efd-a827-7c97d69e1f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 511, 63)\n"
          ]
        }
      ],
      "source": [
        "# Test decoder\n",
        "decoder = Decoder(vocab_size=output_vocab_size,\n",
        "                    embedding_dim=50,\n",
        "                    decoder_units=16,\n",
        "                    batch_size=batch_size,\n",
        "                    max_sequence_length=max_sequence_length)\n",
        "decoder.attention.setup_memory(encoder_output)\n",
        "initial_state = decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n",
        "\n",
        "decoder_batch = {\n",
        "    'input_ids': tf.convert_to_tensor(encoded_output, tf.int32),\n",
        "    'initial_state': initial_state\n",
        "}\n",
        "decoder_outputs = decoder(decoder_batch).rnn_output\n",
        "print(f'{decoder_outputs.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RueCJjXO1mPp"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "trainer = MyTrainer(encoder=encoder,\n",
        "                    decoder=decoder,\n",
        "                    max_length=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "l5H1c5ZN1mPp",
        "outputId": "d56bee8b-043a-4526-e4d9-efca762bd374"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss - 0.040791284292936325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1/3 [00:13<00:27, 13.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
            "Loss - 0.0352395623922348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2/3 [00:20<00:09,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
            "Loss - 0.035170719027519226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:26<00:00,  8.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    batch = {\n",
        "        'encoder_input_ids': input_ids,\n",
        "        'encoder_attention_mask': attention_mask,\n",
        "        'decoder_target': encoded_output\n",
        "    }\n",
        "    loss = trainer.batch_fit(batch)\n",
        "    print(f'Loss - {loss}')\n",
        "\n",
        "    generated = trainer.generate(input_ids=input_ids,\n",
        "                                    attention_mask=attention_mask)\n",
        "    translated = trainer.translate(generated)\n",
        "    print(f'Translated - {translated}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3vOKrD11mPp"
      },
      "source": [
        "An example of answered question by the pretrained (*original*) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "09T-nztL1mPp",
        "outputId": "a3e7b98e-9407-4984-f02a-b40253b9f6a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model outputs: {'score': 2.5508483304292895e-05, 'start': 343, 'end': 386, 'answer': 'codices from throughout history, as well as'}\n",
            "\n",
            "official results are (from train.json):\n",
            "span_start: 151\n",
            "span_end: 179\n",
            "span_text: Formally established in 1475\n",
            "input_text: It was formally established in 1475\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertForQuestionAnswering, pipeline\n",
        "\n",
        "model = TFBertForQuestionAnswering.from_pretrained(model_name, from_pt=True)\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=model_name)\n",
        "\n",
        "outputs = question_answerer(question=train_df['q'][0], context=train_df['p'][0])\n",
        "\n",
        "print(\"model outputs:\", outputs)\n",
        "print()\n",
        "print(\"official results are (from train.json):\") \n",
        "print(\"span_start: 151\")\n",
        "print(\"span_end: 179\")\n",
        "print(\"span_text: Formally established in 1475\")\n",
        "print(\"input_text: It was formally established in 1475\")\n",
        "#print(\"start scores: {}\".format(start_scores))\n",
        "#print(\"end scores: {}\".format(end_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkhJeOxIVG9U"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfOBhZf1mPp"
      },
      "source": [
        "### Question generation $f_\\theta(P, Q, H)$ with text passage $P$, question $Q$ and dialogue history $H$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9maQVnw61mPp"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpTttkRT1mPq"
      },
      "source": [
        "## Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "j9Mb67HE1mPq"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmviMChy1mPq"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jDn-h3WL1mPq"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BnJYkp91si0"
      },
      "source": [
        "#BERT2BERT Pytorch version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjo5CbruHgPC",
        "outputId": "bbc2f695-0185-43ba-d0c7-2194cad1d142"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contexts = list(train_df['p'])\n",
        "questions = list(train_df['q'])\n",
        "answers = list(train_df['a'])\n",
        "\n",
        "contexts = contexts[:30000]\n",
        "questions = questions[:30000]\n",
        "answers = answers[:30000]\n",
        "len(contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3qnKLzpyuYv",
        "outputId": "390fc963-9d3f-417c-e487-15084db05e39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape: (30000, 400)\n",
            "decoder_input_ids shape: (30000, 141)\n"
          ]
        }
      ],
      "source": [
        "from transformers import EncoderDecoderModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "model_name = 'distilroberta-base'\n",
        "# tie_encoder_decoder to share weights and half the number of parameters\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name, tie_encoder_decoder=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config_eos_token_id = tokenizer.sep_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "\n",
        "input_values = tokenizer(questions, contexts, \n",
        "                          padding=True,\n",
        "                          truncation=True,\n",
        "                          max_length = 400\n",
        "                          )\n",
        "input_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']\n",
        "label_values = tokenizer(answers,\n",
        "                          padding=True,\n",
        "                          truncation=True,\n",
        "                          max_length = 150\n",
        "                          )\n",
        "labels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n",
        "#Tokens with indices set to ``-100`` are ignored (masked) during training, the loss is only computed for the tokens with labels\n",
        "masked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\n",
        "\n",
        "print(f'input_ids shape: {np.shape(input_ids)}')\n",
        "print(f'decoder_input_ids shape: {np.shape(labels)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LePPZM5FRjX6",
        "outputId": "84300463-d0da-4d13-9739-17772a651fa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_values.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko_uyAU1BN2o"
      },
      "source": [
        "The dictionary of encodings returned from the tokenizer will be used to build a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z71ZL00bP9nD",
        "outputId": "c2650d6a-a0b5-4717-dc29-dd48659b09d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encodings = input_values\n",
        "encodings.update({'decoder_input_ids': labels,\n",
        "                 'decoder_attention_mask': labels_mask,\n",
        "                 'labels': masked_labels\n",
        "                 })\n",
        "encodings.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax_O8CegF5wv"
      },
      "source": [
        "  Since the training will be done by using the Trainer API, which need as input a Pytorch dataset, the function defined below build a Pytorch Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dkWdaeb6AfHD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "psBNzKDTstsW"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "kDGBVcNgHcJY"
      },
      "outputs": [],
      "source": [
        "#define some parameters of the model\n",
        "batch_size = 20\n",
        "learning_rate = 1e-4\n",
        "tot_epochs = 2.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oecDSCLH2c0u"
      },
      "source": [
        "The version in which the traning is done by the Trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff2a-2Pl3Tx3"
      },
      "source": [
        "#Training with the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TQCaPaG8NB5C",
        "outputId": "b9bd7f2c-dc96-47ac-a902-d4c77fd10c57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 30000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 20\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3000\n",
            "  Number of trainable parameters = 96944217\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.35 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [59], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m,per_device_train_batch_size \u001b[39m=\u001b[39m batch_size, num_train_epochs \u001b[39m=\u001b[39m tot_epochs, save_steps\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m )\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model,\n\u001b[1;32m      7\u001b[0m     training_args,\n\u001b[1;32m      8\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m      9\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m---> 11\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:1527\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1524\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1526\u001b[0m )\n\u001b[0;32m-> 1527\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1528\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1529\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1530\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1531\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1532\u001b[0m )\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:1775\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1777\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1778\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1779\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1780\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:2523\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2520\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2522\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2523\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2526\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2554\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2555\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2556\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2557\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:590\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m kwargs_decoder \u001b[39m=\u001b[39m {\n\u001b[1;32m    586\u001b[0m     argument[\u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m) :]: value \u001b[39mfor\u001b[39;00m argument, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m argument\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    591\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    592\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    593\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    594\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    595\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    596\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    597\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_encoder,\n\u001b[1;32m    598\u001b[0m     )\n\u001b[1;32m    599\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    600\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\u001b[39m*\u001b[39mencoder_outputs)\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:853\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    844\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    846\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    847\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    848\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    852\u001b[0m )\n\u001b[0;32m--> 853\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    854\u001b[0m     embedding_output,\n\u001b[1;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    857\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    858\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    859\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    860\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    861\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    862\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    864\u001b[0m )\n\u001b[1;32m    865\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    866\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    528\u001b[0m         hidden_states,\n\u001b[1;32m    529\u001b[0m         attention_mask,\n\u001b[1;32m    530\u001b[0m         layer_head_mask,\n\u001b[1;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    533\u001b[0m         past_key_value,\n\u001b[1;32m    534\u001b[0m         output_attentions,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    402\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    410\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    413\u001b[0m         hidden_states,\n\u001b[1;32m    414\u001b[0m         attention_mask,\n\u001b[1;32m    415\u001b[0m         head_mask,\n\u001b[1;32m    416\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    417\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    419\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    421\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    330\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    331\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 339\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    340\u001b[0m         hidden_states,\n\u001b[1;32m    341\u001b[0m         attention_mask,\n\u001b[1;32m    342\u001b[0m         head_mask,\n\u001b[1;32m    343\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    344\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    345\u001b[0m         past_key_value,\n\u001b[1;32m    346\u001b[0m         output_attentions,\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    349\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/.miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:235\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    232\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    234\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.35 GiB already allocated; 0 bytes free; 3.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments('output',per_device_train_batch_size = batch_size, num_train_epochs = tot_epochs, save_steps=5000 )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zyT3ZnN3CjE"
      },
      "source": [
        "#Custom training with Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDNdl1mrNOhq"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, DataCollatorWithPadding, TrainingArguments\n",
        "from transformers import AdamW\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_ld = torch.utils.data.DataLoader(train_dataset,\n",
        "                                     batch_size=batch_size,\n",
        "                                     )\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "optim = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(tot_epochs):\n",
        "    loop = tqdm(train_ld)\n",
        "    for batch in loop:\n",
        "        optim.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "        decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        loss, outputs = model(input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              decoder_input_ids= decoder_input_ids,\n",
        "                              decoder_attention_mask= decoder_attention_mask,\n",
        "                              labels = labels\n",
        "                        )[:2]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS2JruSS4Lte"
      },
      "source": [
        "#Testing the Trainer model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZW-Qs56RV4_"
      },
      "outputs": [],
      "source": [
        "test = train_df.iloc[50000:50250]\n",
        "context_test = list(test['p'])\n",
        "question_test = list(test['q'])\n",
        "answer_test = list(test['a'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdO6Zx8Q8vI"
      },
      "outputs": [],
      "source": [
        "input_values = tokenizer(question_test,context_test, padding=True, truncation=True, max_length = 400)\n",
        "input_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIFMjpiIUroq"
      },
      "outputs": [],
      "source": [
        "#clean some GPU memory\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BwlwGcjR9dA"
      },
      "outputs": [],
      "source": [
        "trainer.model.cuda()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "mylist = []\n",
        "for input in input_ids:\n",
        "  input = np.array(input)\n",
        "  input = np.expand_dims(input, axis=0)\n",
        "  #print(np.shape(input))\n",
        "  generated = trainer.model.generate(input_ids=torch.tensor(input).to(device),\n",
        "                                                 max_length=30,\n",
        "                                                 repetition_penalty=3.,\n",
        "                                                 min_length=1,\n",
        "                                                 no_repeat_ngram_size=2,\n",
        "                                                 early_stopping=True,\n",
        "                                                 num_beams=1\n",
        "                                                 )\n",
        "  generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "  mylist.append(generated)\n",
        "  #print(f'Generated: {generated}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqC8s-Nff8i8"
      },
      "outputs": [],
      "source": [
        "x = pd.DataFrame(mylist, columns = ['generated'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAe35G7NakrQ"
      },
      "outputs": [],
      "source": [
        "x['answers'] = answer_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v8Xd6xVHNIS"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "x.head(250)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e405b0a43b05ed5b511dac57849ab560497f023fc2f8f0bfd2781bf41b5f416c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
