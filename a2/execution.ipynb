{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (1.23.3)\n",
      "Requirement already satisfied: matplotlib in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (3.5.3)\n",
      "Requirement already satisfied: transformers in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (4.37.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2022.6.15.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/daniele/.miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='Dataset', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='Dataset', url_path=test_url, suffix='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from typing import List, Dict, Callable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(model: keras.Model,\n",
    "                x: np.ndarray,\n",
    "                prediction_info: Dict):\n",
    "    \"\"\"\n",
    "    Inference routine of a given input set of examples\n",
    "\n",
    "    :param model: Keras built and possibly trained model\n",
    "    :param x: input set of examples in np.ndarray format\n",
    "    :param prediction_info: dictionary storing model predict() argument information\n",
    "\n",
    "    :return\n",
    "        predictions: predicted labels in np.ndarray format\n",
    "    \"\"\"\n",
    "    print(f'Starting prediction: \\n{prediction_info}')\n",
    "    print(f'Predicting on {x.shape[0]} samples')\n",
    "    predictions = model.predict(x, **prediction_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(model: keras.Model, \n",
    "             x: np.ndarray, \n",
    "             y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute F1_score on the given data with corresponding labels\n",
    "\n",
    "    :param model: Keras built and possibly trained model\n",
    "    :param x: data in np.ndarray format\n",
    "    :param y: ground-truth labels in np.ndarray format\n",
    "\n",
    "    :return\n",
    "        score: f1_macro_score\n",
    "    \"\"\"\n",
    "    #predictions on the x set\n",
    "    prediction_info = {\n",
    "        'batch_size': 64,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    y_pred = predict_data(model=model, x=x, prediction_info=prediction_info)\n",
    "\n",
    "    #compute argmax to take the best class for each sample\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    #compute the f1_macro\n",
    "    score = f1_score(y, y_pred, average ='macro')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question generation $f_\\theta(P, Q)$ with text passage $P$ and question $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper class\n",
    "\n",
    "    train_op -> uses tf.GradientTape to compute the loss\n",
    "    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, max_length):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.max_length = max_length\n",
    "        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, logits, target):\n",
    "        loss = self.ce(y_true=target, y_pred=logits)\n",
    "        mask = tf.logical_not(tf.math.equal(target, 0))\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    @tf.function\n",
    "    def train_op(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n",
    "                                                                 'hidden_state': inputs['encoder_state']})\n",
    "\n",
    "            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n",
    "            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n",
    "\n",
    "            decoder.attention.setup_memory(encoder_output)\n",
    "\n",
    "            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n",
    "            predicted = self.decoder({'input_ids': decoder_input,\n",
    "                                      'initial_state': decoder_initial_state}).rnn_output\n",
    "\n",
    "            loss = self.compute_loss(logits=predicted, target=real_target)\n",
    "\n",
    "        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
    "        return loss, grads\n",
    "\n",
    "    @tf.function\n",
    "    def batch_fit(self, inputs):\n",
    "        loss, grads = self.train_op(inputs=inputs)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def generate(self, input_ids):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        encoder_initial_state = [tf.zeros((batch_size, self.encoder.encoder_units)),\n",
    "                                 tf.zeros((batch_size, self.encoder.encoder_units))]\n",
    "        encoder_output, encoder_h, encoder_s = self.encoder({\n",
    "            'input_ids': input_ids,\n",
    "            'hidden_state': encoder_initial_state\n",
    "        })\n",
    "\n",
    "        start_tokens = tf.fill([batch_size], tokenizer.word_index['<start>'])\n",
    "        end_token = tokenizer.word_index['<end>']\n",
    "\n",
    "        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n",
    "                                                    sampler=greedy_sampler,\n",
    "                                                    output_layer=self.decoder.generation_dense,\n",
    "                                                    maximum_iterations=self.max_length)\n",
    "        self.decoder.attention.setup_memory(encoder_output)\n",
    "\n",
    "        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n",
    "        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n",
    "        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n",
    "                                         start_tokens=start_tokens,\n",
    "                                         end_token=end_token,\n",
    "                                         initial_state=decoder_initial_state)\n",
    "        return outputs\n",
    "\n",
    "    def translate(self, generated):\n",
    "        return tokenizer.sequences_to_texts(generated.sample_id.numpy())\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encoder_units = encoder_units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                   output_dim=embedding_dim)\n",
    "        self.encoder_lstm = tf.keras.layers.LSTM(self.encoder_units,\n",
    "                                                 return_sequences=True,\n",
    "                                                 return_state=True)\n",
    "\n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        input_emb = self.embedding(input_ids)\n",
    "        encoder_output, lstm_hidden, lstm_states = self.encoder_lstm(input_emb, initial_state=inputs['hidden_state'])\n",
    "        return encoder_output, lstm_hidden, lstm_states\n",
    "\n",
    "    def initialize(self, batch_size):\n",
    "        return [tf.zeros((batch_size, self.encoder_units)), tf.zeros((batch_size, self.encoder_units))]\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                   output_dim=embedding_dim)\n",
    "        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n",
    "\n",
    "        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n",
    "                                                       memory=None,\n",
    "                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n",
    "\n",
    "        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n",
    "                                                                 self.attention,\n",
    "                                                                 attention_layer_size=self.decoder_units)\n",
    "\n",
    "        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n",
    "                                                sampler=self.sampler,\n",
    "                                                output_layer=self.generation_dense)\n",
    "\n",
    "    def build_initial_state(self, batch_size, encoder_state):\n",
    "        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        initial_state = initial_state.clone(cell_state=encoder_state)\n",
    "        return initial_state\n",
    "\n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        input_emb = self.embedding(input_ids)\n",
    "        decoder_output, _, _ = self.decoder(input_emb,\n",
    "                                            initial_state=inputs['initial_state'],\n",
    "                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 16) -- (2, 16) -- (2, 16)\n",
      "(2, 5, 16)\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "input_sample = [\n",
    "    \"hello there how is it going\",\n",
    "    \"this assignment is hellish\"\n",
    "]\n",
    "output_sample = [\n",
    "    \"<start> it is going well <end>\",\n",
    "    \"<start> I agree <end>\"\n",
    "]\n",
    "\n",
    "batch_size = len(input_sample)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(input_sample + output_sample)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "encoded_input_sample = tokenizer.texts_to_sequences(input_sample)\n",
    "max_input_length = max([len(item) for item in encoded_input_sample])\n",
    "\n",
    "encoded_output_sample = tokenizer.texts_to_sequences(output_sample)\n",
    "max_output_length = max([len(item) for item in encoded_output_sample])\n",
    "\n",
    "max_sequence_length = max(max_input_length, max_output_length)\n",
    "\n",
    "encoded_input_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_input_sample,\n",
    "                                                                        padding='post',\n",
    "                                                                        maxlen=max_sequence_length)\n",
    "encoded_output_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_output_sample,\n",
    "                                                                        padding='post',\n",
    "                                                                        maxlen=max_sequence_length)\n",
    "\n",
    "# Test encoder\n",
    "encoder = Encoder(vocab_size=vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    encoder_units=16)\n",
    "\n",
    "sample_hidden = encoder.initialize(batch_size=batch_size)\n",
    "encoder_sample_batch = {\n",
    "    'input_ids': tf.convert_to_tensor(encoded_input_sample, dtype=tf.int32),\n",
    "    'hidden_state': sample_hidden\n",
    "}\n",
    "\n",
    "sample_output, sample_h, sample_c = encoder(inputs=encoder_sample_batch)\n",
    "print(f'{sample_output.shape} -- {sample_h.shape} -- {sample_c.shape}')\n",
    "\n",
    "# Test decoder\n",
    "decoder = Decoder(vocab_size=vocab_size,\n",
    "                    embedding_dim=50,\n",
    "                    decoder_units=16,\n",
    "                    batch_size=batch_size,\n",
    "                    max_sequence_length=max_sequence_length)\n",
    "decoder.attention.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(batch_size, [sample_h, sample_c])\n",
    "\n",
    "decoder_sample_batch = {\n",
    "    'input_ids': tf.convert_to_tensor(encoded_output_sample, tf.int32),\n",
    "    'initial_state': initial_state\n",
    "}\n",
    "sample_decoder_outputs = decoder(decoder_sample_batch).rnn_output\n",
    "print(f'{sample_decoder_outputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer = MyTrainer(encoder=encoder,\n",
    "                    decoder=decoder,\n",
    "                    max_length=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 2.2167553901672363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:02<00:27,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated - ['it it how it how <start>', 'i i i i i i']\n",
      "Loss - 2.212101697921753\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.2074058055877686\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.202622652053833\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.197714328765869\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.192643165588379\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1873672008514404\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.181844711303711\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.176034450531006\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1698975563049316\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.163393497467041\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:03<00:07, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 2.1564807891845703\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1491177082061768\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1412577629089355\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1328539848327637\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.123854160308838\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.114203453063965\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.1038424968719482\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.0927093029022217\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.080737590789795\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.0678584575653076\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.0540032386779785\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:03<00:02, 23.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 2.039102554321289\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.0230932235717773\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 2.0059216022491455\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.9875494241714478\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.9679648876190186\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.9471944570541382\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.9253171682357788\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.9024807214736938\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.878915786743164\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.8549442291259766\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.8309694528579712\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.8074371814727783\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.7847537994384766\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:03<00:01, 37.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 1.7631763219833374\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.742719292640686\n",
      "Translated - ['i i i i i i', 'i i i i i i']\n",
      "Loss - 1.7231413125991821\n",
      "Translated - ['i i i i i <end>', 'i i i i <end> <end>']\n",
      "Loss - 1.7040239572525024\n",
      "Translated - ['i i i i <end>', 'i i i <end> <end>']\n",
      "Loss - 1.6849130392074585\n",
      "Translated - ['i i i <end>', 'i i <end> <end>']\n",
      "Loss - 1.6654428243637085\n",
      "Translated - ['i i <end>', 'i i <end>']\n",
      "Loss - 1.6454098224639893\n",
      "Translated - ['i i <end>', 'i i <end>']\n",
      "Loss - 1.624798059463501\n",
      "Translated - ['going i <end>', 'i <end> <end>']\n",
      "Loss - 1.6037698984146118\n",
      "Translated - ['going i <end>', 'i <end> <end>']\n",
      "Loss - 1.5826255083084106\n",
      "Translated - ['going going <end>', 'i <end> <end>']\n",
      "Loss - 1.5617433786392212\n",
      "Translated - ['going going <end>', 'i <end> <end>']\n",
      "Loss - 1.5414999723434448\n",
      "Translated - ['going going <end>', 'i <end> <end>']\n",
      "Loss - 1.5221741199493408\n",
      "Translated - ['going going going <end>', 'i <end> <end> <end>']\n",
      "Loss - 1.5038511753082275\n",
      "Translated - ['going going going <end>', '<end> <end> <end> <end>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:03<00:00, 51.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 1.486371636390686\n",
      "Translated - ['going going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.469369888305664\n",
      "Translated - ['going going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.4524027109146118\n",
      "Translated - ['going going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.4350945949554443\n",
      "Translated - ['going going going going <end>', '<end> <end> <end> <end> <end>']\n",
      "Loss - 1.4172313213348389\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.3987889289855957\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.379907250404358\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.3608219623565674\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.3417596817016602\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.322819709777832\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.3038990497589111\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.284736156463623\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.2650747299194336\n",
      "Translated - ['it going going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.2448428869247437\n",
      "Translated - ['it it going <end>', '<end> <end> <end> <end>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [00:03<00:00, 59.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - 1.2242052555084229\n",
      "Translated - ['it it going <end>', '<end> <end> <end> <end>']\n",
      "Loss - 1.2034533023834229\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.1828190088272095\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.1623501777648926\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.1419302225112915\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.1214234828948975\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.1008204221725464\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.0802757740020752\n",
      "Translated - ['it it going going <end>', 'i <end> <end> <end> <end>']\n",
      "Loss - 1.0600214004516602\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 1.040226697921753\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 1.0209081172943115\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 1.0019690990447998\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 0.9833375215530396\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 0.9650442004203796\n",
      "Translated - ['it it going <end>', 'i <end> <end> <end>']\n",
      "Loss - 0.9471742510795593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:04<00:00, 59.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.9297483563423157\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.9126607179641724\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.8957479596138\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.8789197206497192\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.8622139096260071\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.8457285761833191\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.829515278339386\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.8135622143745422\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.7978771924972534\n",
      "Translated - ['it it going <end>', 'i agree <end> <end>']\n",
      "Loss - 0.7825426459312439\n",
      "Translated - ['it is going going <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.7676534652709961\n",
      "Translated - ['it is going going <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.7532182931900024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated - ['is it going going <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.7391657829284668\n",
      "Translated - ['is it going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.725447416305542\n",
      "Translated - ['is it going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.7120766043663025\n",
      "Translated - ['is it going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6990617513656616\n",
      "Translated - ['is it going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6863529086112976\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6738934516906738\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6616845726966858\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6497541666030884\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6380911469459534\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n",
      "Loss - 0.6266587972640991\n",
      "Translated - ['is is going well <end>', 'i agree <end> <end> <end>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    encoder_hidden_state = encoder.initialize(batch_size=batch_size)\n",
    "    batch = {\n",
    "        'encoder_input_ids': encoded_input_sample,\n",
    "        'encoder_state': encoder_hidden_state,\n",
    "        'decoder_target': encoded_output_sample\n",
    "    }\n",
    "    loss = trainer.batch_fit(batch)\n",
    "    print(f'Loss - {loss}')\n",
    "\n",
    "    generated = trainer.generate(input_ids=encoded_input_sample)\n",
    "    translated = trainer.translate(generated)\n",
    "    print(f'Translated - {translated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question generation $f_\\theta(P, Q, H)$ with text passage $P$, question $Q$ and dialogue history $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e405b0a43b05ed5b511dac57849ab560497f023fc2f8f0bfd2781bf41b5f416c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
