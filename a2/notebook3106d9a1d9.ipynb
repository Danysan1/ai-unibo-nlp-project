{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb)\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb)","metadata":{"id":"XAbMpqPm1mPC"}},{"cell_type":"markdown","source":"# Assignment 2 execution","metadata":{"id":"qtlMx4kv1mPK"}},{"cell_type":"code","source":"%pip install pandas numpy matplotlib transformers==4.25.1  dataset tensorflow_addons","metadata":{"id":"feevAAsT1mPL","outputId":"e7b763fb-cdd4-4491-85bf-bfb749788be8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data loading","metadata":{"id":"kJSsWQg_1mPO"}},{"cell_type":"markdown","source":"### Dataset download","metadata":{"id":"1mXm1Fpq1mPP"}},{"cell_type":"code","source":"import os\nimport urllib.request\nfrom tqdm import tqdm\n\nclass DownloadProgressBar(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n        \ndef download_url(url, output_path):\n    with DownloadProgressBar(unit='B', unit_scale=True,\n                             miniters=1, desc=url.split('/')[-1]) as t:\n        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n\ndef download_data(data_path, url_path, suffix):    \n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n        \n    data_path = os.path.join(data_path, f'{suffix}.json')\n    if not os.path.exists(data_path):\n        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n        download_url(url=url_path, output_path=data_path)\n        print(\"Download completed!\")","metadata":{"id":"oOqlik_a1mPP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_folder = 'Dataset'","metadata":{"id":"sskRUOtB1mPR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train & Validation data\ntrain_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\ndownload_data(data_path=data_folder, url_path=train_url, suffix='train')\n\n# Test data\ntest_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\ndownload_data(data_path=data_folder, url_path=test_url, suffix='test')","metadata":{"id":"CzsJWqI61mPR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset loading","metadata":{"id":"GsM9znVV1mPT"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nfrom os import path\nfrom matplotlib import pyplot as plt","metadata":{"id":"VRsmY5Wn1mPV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loadDataset(filename):\n    with open(path.join(data_folder, filename)) as file_obj:\n        df = json.load(file_obj)[\"data\"]\n    print(f'{len(df)} stories / {len(df[0][\"questions\"])} questions in the first row')\n\n    storyDType = pd.CategoricalDtype(pd.unique([story[\"story\"] for story in df]))\n    print(f\"{storyDType.categories.size} distinct stories\")\n\n    sourceDType = pd.CategoricalDtype(pd.unique([story[\"source\"] for story in df]))\n    print(f\"{sourceDType.categories.size} distinct sources: {sourceDType.categories}\")\n\n    df = np.array([\n        [\n            sourceDType.categories.get_loc(story[\"source\"]), # Sources factorization\n            storyDType.categories.get_loc(story[\"story\"]), # Sources factorization\n            story[\"questions\"][question_index][\"input_text\"],\n            story[\"answers\"][question_index][\"input_text\"],\n            story[\"answers\"][question_index][\"span_text\"],\n        ]\n        for story in df\n        for question_index in range(len(story[\"questions\"]))\n        if story[\"answers\"][question_index][\"input_text\"] != 'unknown'\n    ])\n    print(f'{df.shape} question-answer pairs x columns')\n    print(f'First row: {df[0]}')\n    \n    # https://marcobonzanini.com/2021/09/15/tips-for-saving-memory-with-pandas/\n    # https://pandas.pydata.org/docs/user_guide/categorical.html\n    df = pd.DataFrame({\n        \"source\": pd.Series(pd.Categorical.from_codes(df[:,0].astype(np.int16), dtype=sourceDType)),\n        \"p\": pd.Series(pd.Categorical.from_codes(df[:,1].astype(np.int16), dtype=storyDType)),\n        \"q\": df[:,2],\n        \"a\": df[:,3],\n        \"span\": df[:,4],\n    })\n\n    return df","metadata":{"id":"lI74kMng1mPW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = loadDataset(\"train.json\")\ntrain_df.count()","metadata":{"id":"jsZrgF5d1mPX","outputId":"0da854bb-f9f6-4d48-f1f0-60f8461abf02","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.unique(train_df[\"p\"]).size","metadata":{"id":"lOQSe3Sc1mPY","outputId":"108e74fe-6f6e-4d41-bc31-d88b4b21271a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.unique(train_df[\"span\"]).size","metadata":{"id":"WJaLGY_p1mPZ","outputId":"c2d31830-e668-4f22-e59c-5574a0a561a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.unique(train_df[\"source\"]).size","metadata":{"id":"V_ml0A1b1mPZ","outputId":"88bda1d9-bf6b-4d3c-ab86-e18033ffba97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"G9VjurfZ1mPZ","outputId":"a5f0827d-9f74-459d-f2ad-0a5acdc1c86f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.memory_usage(deep=True)","metadata":{"id":"nrRkHhES1mPa","outputId":"f1736c82-64dc-4b69-ba5f-87e4dc3652b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = loadDataset(\"test.json\")\ntest_df.count()","metadata":{"id":"v5aer1n41mPa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing","metadata":{"id":"XRs7YHKY1mPa"}},{"cell_type":"markdown","source":"### Check unanswerable questions in the Train Dataset","metadata":{"id":"k0GmOXam1mPb"}},{"cell_type":"code","source":"idx = (train_df.a == 'unknown')\nunanswerable = train_df[idx]\nunanswerable.q.count()","metadata":{"id":"6Aq524zd1mPb","outputId":"f6f42ea9-b280-4f34-ae88-e2ea8bdeb02f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All unanswerable questions in the Train Dataset have been already removed.","metadata":{"id":"lTdjLFYI1mPb"}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{"id":"maBslb2n1mPb"}},{"cell_type":"code","source":"train_df[\"p\"][42]","metadata":{"id":"VH5Ep77e1mPb","outputId":"6f4b6b2c-ad95-43d9-e59f-12fe2a4aefc4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"q\"][42]","metadata":{"id":"zYcgBVcG1mPc","outputId":"8540b69c-d459-42df-a5b9-94793de5d8c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"a\"][42]","metadata":{"id":"5zvn9Tv01mPc","outputId":"c8d1810a-428d-4a57-a571-ac727b48a55e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"span\"][42]","metadata":{"id":"NokHfoLm1mPc","outputId":"44e2cc0a-53bc-47f8-efb7-76c45b9e201f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"source\"][42]","metadata":{"id":"JskMPh-Y1mPc","outputId":"674fce52-5190-43e1-8f88-c419b3ef350e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution statistics","metadata":{"id":"IhdF4AR7ef05"}},{"cell_type":"markdown","source":"Sources:","metadata":{"id":"ebBVUlveef05"}},{"cell_type":"code","source":"train_df[\"source\"].hist()","metadata":{"id":"8lIuR9RO1mPd","outputId":"6fba63dd-4283-4b29-e09e-58a28c87263b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Occurrences of 25 most popular stories:","metadata":{"id":"Ax5uSP6Lef06"}},{"cell_type":"code","source":"story_counts = train_df[\"p\"].cat.codes.value_counts(sort=True)\nstory_counts[:25].plot(kind=\"bar\", figsize=(15,5))","metadata":{"id":"wUgsqg4Uef06","outputId":"c72091e4-0ded-464a-e902-3fecc6ccc716","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Occurrences of 25 least popular stories:","metadata":{"id":"qL-m6SuVef06"}},{"cell_type":"code","source":"story_counts[-25:-1].plot(kind=\"bar\", figsize=(15,5))","metadata":{"id":"jkiwEygzef06","outputId":"26ee9a1b-c469-4287-dd92-7dd5afb82ca4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histogram of story popularities:","metadata":{"id":"9-EVRQQVef06"}},{"cell_type":"code","source":"story_counts.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"CJLPGCfcef07","outputId":"903506fa-8eb0-4b2e-d820-6ce6399506b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing rows with outlier story lengths to save memory","metadata":{}},{"cell_type":"code","source":"train_df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LOGARITHMIC histogram of story length:","metadata":{}},{"cell_type":"code","source":"story_lengths = train_df[\"p\"].str.len()\nstory_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_length_limit = story_lengths.quantile(0.999)\np_length_limit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_length_mask = story_lengths < p_length_limit\np_length_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[p_length_mask]\ntrain_df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing rows with outlier question/answer/span lengths to save memory","metadata":{}},{"cell_type":"markdown","source":"LOGARITHMIC histogram of question length:","metadata":{}},{"cell_type":"code","source":"question_lengths = train_df[\"q\"].str.len()\nquestion_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"kFobL8Upef07","outputId":"f2d6a442-5f64-4792-a744-76e213be9e9b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_length_limit = question_lengths.quantile(0.999)\nq_length_limit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LOGARITHMIC histogram of answer length:","metadata":{}},{"cell_type":"code","source":"answer_lengths = train_df[\"a\"].str.len()\nanswer_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"_7phh51Nef07","outputId":"feec6821-80f7-421c-bc31-6bc0103f38f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_length_limit = answer_lengths.quantile(0.999)\na_length_limit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"span_lengths = train_df[\"span\"].str.len()\nspan_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"span_length_limit = span_lengths.quantile(0.999)\nspan_length_limit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bad_length_mask = (question_lengths > q_length_limit) | (answer_lengths > a_length_limit) | (span_lengths > span_length_limit)\nbad_length_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excluded_stories = train_df[\"p\"][bad_length_mask].unique()\nlen(excluded_stories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excluded_mask = ~train_df[\"p\"].isin(excluded_stories)\nexcluded_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[excluded_mask]\ntrain_df.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Validation-Test split","metadata":{"id":"mm2QCJbR1mPd"}},{"cell_type":"code","source":"train_df = train_df.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_rows = len(train_df)\ntotal_rows","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ideal_split_index = int(total_rows * 0.8)\nideal_split_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[ ideal_split_index-3 : ideal_split_index+1 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"before_split_mask = pd.Series(np.linspace(0, total_rows, total_rows)) < ideal_split_index\nbefore_split_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_story = train_df[\"p\"][ideal_split_index - 1]\nsplit_story_mask = train_df[\"p\"] == split_story\nsplit_story_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mask = before_split_mask | split_story_mask\ntrain_mask.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = train_df[~train_mask]\ntrain_df = train_df[train_mask]\nlen(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model definition","metadata":{"id":"SICbZ6vB1mPd"}},{"cell_type":"markdown","source":"### Utilities","metadata":{"id":"6wuh0_ic1mPd"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom typing import List, Dict, Callable\nimport random","metadata":{"id":"4pKuTsYt1mPd","outputId":"28dcc870-e8ed-46b7-ddb0-97709087727d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_data(model: keras.Model,\n                x: np.ndarray,\n                prediction_info: Dict):\n    \"\"\"\n    Inference routine of a given input set of examples\n\n    :param model: Keras built and possibly trained model\n    :param x: input set of examples in np.ndarray format\n    :param prediction_info: dictionary storing model predict() argument information\n\n    :return\n        predictions: predicted labels in np.ndarray format\n    \"\"\"\n    print(f'Starting prediction: \\n{prediction_info}')\n    print(f'Predicting on {x.shape[0]} samples')\n    predictions = model.predict(x, **prediction_info)\n    return predictions","metadata":{"id":"YutAeLih1mPe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_f1(model: keras.Model, \n             x: np.ndarray, \n             y: np.ndarray):\n    \"\"\"\n    Compute F1_score on the given data with corresponding labels\n\n    :param model: Keras built and possibly trained model\n    :param x: data in np.ndarray format\n    :param y: ground-truth labels in np.ndarray format\n\n    :return\n        score: f1_macro_score\n    \"\"\"\n    #predictions on the x set\n    prediction_info = {\n        'batch_size': 64,\n        'verbose': 1\n    }\n    y_pred = predict_data(model=model, x=x, prediction_info=prediction_info)\n\n    #compute argmax to take the best class for each sample\n    y_pred = np.argmax(y_pred, axis=1)\n    #compute the f1_macro\n    score = f1_score(y, y_pred, average ='macro')\n    return score","metadata":{"id":"CS_dd2On1mPe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_reproducibility(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    tf.config.experimental.enable_op_determinism()","metadata":{"id":"1-akLLJo1mPe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tqdm import tqdm\nfrom transformers import TFAutoModel, AutoTokenizer\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"id":"61_GSxvB1mPe","outputId":"917fa56f-5eb2-49a2-dbd3-834f0b79c8db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question generation $f_\\theta(P, Q)$ with text passage $P$ and question $Q$","metadata":{"id":"1aFTVbjH1mPf"}},{"cell_type":"markdown","source":"### Seq2Seq LSTM","metadata":{"id":"XxoRFA2j1mPf"}},{"cell_type":"code","source":"class MyTrainer(object):\n    \"\"\"\n    Simple wrapper class\n\n    train_op -> uses tf.GradientTape to compute the loss\n    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n    \"\"\"\n\n    def __init__(self, encoder, decoder, max_length):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.max_length = max_length\n        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n\n    @tf.function\n    def compute_loss(self, logits, target):\n        loss = self.ce(y_true=target, y_pred=logits)\n        mask = tf.logical_not(tf.math.equal(target, 0))\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    @tf.function\n    def train_op(self, inputs):\n        with tf.GradientTape() as tape:\n            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n                                                                 'hidden_state': inputs['encoder_state']})\n\n            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n\n            decoder.attention.setup_memory(encoder_output)\n\n            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n            predicted = self.decoder({'input_ids': decoder_input,\n                                      'initial_state': decoder_initial_state}).rnn_output\n\n            loss = self.compute_loss(logits=predicted, target=real_target)\n\n        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n        return loss, grads\n\n    @tf.function\n    def batch_fit(self, inputs):\n        loss, grads = self.train_op(inputs=inputs)\n        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n        return loss\n\n    @tf.function\n    def generate(self, input_ids):\n        batch_size = input_ids.shape[0]\n        encoder_initial_state = [tf.zeros((batch_size, self.encoder.encoder_units)),\n                                 tf.zeros((batch_size, self.encoder.encoder_units))]\n        encoder_output, encoder_h, encoder_s = self.encoder({\n            'input_ids': input_ids,\n            'hidden_state': encoder_initial_state\n        })\n\n        start_tokens = tf.fill([batch_size], tokenizer.word_index['<start>'])\n        end_token = tokenizer.word_index['<end>']\n\n        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n                                                    sampler=greedy_sampler,\n                                                    output_layer=self.decoder.generation_dense,\n                                                    maximum_iterations=self.max_length)\n        self.decoder.attention.setup_memory(encoder_output)\n\n        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n                                         start_tokens=start_tokens,\n                                         end_token=end_token,\n                                         initial_state=decoder_initial_state)\n        return outputs\n\n    def translate(self, generated):\n        return tokenizer.sequences_to_texts(generated.sample_id.numpy())\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, embedding_dim, encoder_units):\n        super(Encoder, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.encoder_units = encoder_units\n\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.encoder_lstm = tf.keras.layers.LSTM(self.encoder_units,\n                                                 return_sequences=True,\n                                                 return_state=True)\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        encoder_output, lstm_hidden, lstm_states = self.encoder_lstm(input_emb, initial_state=inputs['hidden_state'])\n        return encoder_output, lstm_hidden, lstm_states\n\n    def initialize(self, batch_size):\n        return [tf.zeros((batch_size, self.encoder_units)), tf.zeros((batch_size, self.encoder_units))]\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n\n        self.max_sequence_length = max_sequence_length\n        self.batch_size = batch_size\n\n        self.decoder_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n\n        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n                                                       memory=None,\n                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n\n        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n                                                                 self.attention,\n                                                                 attention_layer_size=self.decoder_units)\n\n        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n                                                sampler=self.sampler,\n                                                output_layer=self.generation_dense)\n\n    def build_initial_state(self, batch_size, encoder_state):\n        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n        return initial_state\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        decoder_output, _, _ = self.decoder(input_emb,\n                                            initial_state=inputs['initial_state'],\n                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n        return decoder_output","metadata":{"id":"OZecoZhu1mPf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample\ninput_sample = [\n    \"hello there how is it going\",\n    \"this assignment is hellish\"\n]\noutput_sample = [\n    \"<start> it is going well <end>\",\n    \"<start> I agree <end>\"\n]\n\nbatch_size = len(input_sample)\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\ntokenizer.fit_on_texts(input_sample + output_sample)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nencoded_input_sample = tokenizer.texts_to_sequences(input_sample)\nmax_input_length = max([len(item) for item in encoded_input_sample])\n\nencoded_output_sample = tokenizer.texts_to_sequences(output_sample)\nmax_output_length = max([len(item) for item in encoded_output_sample])\n\nmax_sequence_length = max(max_input_length, max_output_length)\n\nencoded_input_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_input_sample,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\nencoded_output_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_output_sample,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\n\n# Test encoder\nencoder = Encoder(vocab_size=vocab_size,\n                    embedding_dim=50,\n                    encoder_units=16)\n\nsample_hidden = encoder.initialize(batch_size=batch_size)\nencoder_sample_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_input_sample, dtype=tf.int32),\n    'hidden_state': sample_hidden\n}\n\nsample_output, sample_h, sample_c = encoder(inputs=encoder_sample_batch)\nprint(f'{sample_output.shape} -- {sample_h.shape} -- {sample_c.shape}')\n\n# Test decoder\ndecoder = Decoder(vocab_size=vocab_size,\n                    embedding_dim=50,\n                    decoder_units=16,\n                    batch_size=batch_size,\n                    max_sequence_length=max_sequence_length)\ndecoder.attention.setup_memory(sample_output)\ninitial_state = decoder.build_initial_state(batch_size, [sample_h, sample_c])\n\ndecoder_sample_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_output_sample, tf.int32),\n    'initial_state': initial_state\n}\nsample_decoder_outputs = decoder(decoder_sample_batch).rnn_output\nprint(f'{sample_decoder_outputs.shape}')","metadata":{"id":"t29CV-S41mPg","outputId":"b15fcf00-b73a-41ca-f7aa-954a058ad5d1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\ntrainer = MyTrainer(encoder=encoder,\n                    decoder=decoder,\n                    max_length=max_sequence_length)","metadata":{"id":"SZhSq6V41mPg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nfor epoch in tqdm(range(epochs)):\n    encoder_hidden_state = encoder.initialize(batch_size=batch_size)\n    batch = {\n        'encoder_input_ids': encoded_input_sample,\n        'encoder_state': encoder_hidden_state,\n        'decoder_target': encoded_output_sample\n    }\n    loss = trainer.batch_fit(batch)\n    print(f'Loss - {loss}')\n\n    generated = trainer.generate(input_ids=encoded_input_sample)\n    translated = trainer.translate(generated)\n    print(f'Translated - {translated}')","metadata":{"id":"d5lLS5pc1mPg","outputId":"d8da62b6-89fe-47fe-ef85-061c004f53c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO","metadata":{"id":"Akd5eb5N1mPh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seq2Seq Bert-Tiny","metadata":{"id":"Ia7JgdiF1mPh"}},{"cell_type":"code","source":"class MyTrainer(object):\n    \"\"\"\n    Simple wrapper class\n\n    train_op -> uses tf.GradientTape to compute the loss\n    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n    \"\"\"\n\n    def __init__(self, encoder, decoder, max_length):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.max_length = max_length\n        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n\n    @tf.function\n    def compute_loss(self, logits, target):\n        loss = self.ce(y_true=target, y_pred=logits)\n        mask = tf.logical_not(tf.math.equal(target, 0))\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    @tf.function\n    def train_op(self, inputs):\n        with tf.GradientTape() as tape:\n            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n                                                                 'attention_mask': inputs['encoder_attention_mask']})\n\n            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n\n            decoder.attention.setup_memory(encoder_output)\n\n            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n            predicted = self.decoder({'input_ids': decoder_input,\n                                      'initial_state': decoder_initial_state}).rnn_output\n\n            loss = self.compute_loss(logits=predicted, target=real_target)\n\n        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n        return loss, grads\n\n    @tf.function\n    def batch_fit(self, inputs):\n        loss, grads = self.train_op(inputs=inputs)\n        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n        return loss\n\n    # @tf.function\n    def generate(self, input_ids, attention_mask=None):\n        batch_size = input_ids.shape[0]\n        encoder_output, encoder_h, encoder_s = self.encoder({\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        })\n\n        start_tokens = tf.fill([batch_size], output_tokenizer.word_index['<start>'])\n        end_token = output_tokenizer.word_index['<end>']\n\n        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n                                                    sampler=greedy_sampler,\n                                                    output_layer=self.decoder.generation_dense,\n                                                    maximum_iterations=self.max_length)\n        self.decoder.attention.setup_memory(encoder_output)\n\n        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n                                         start_tokens=start_tokens,\n                                         end_token=end_token,\n                                         initial_state=decoder_initial_state)\n        return outputs\n\n    def translate(self, generated):\n        return output_tokenizer.sequences_to_texts(generated.sample_id.numpy())\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self, model_name, decoder_units):\n        super(Encoder, self).__init__()\n        self.model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n        self.reducer = tf.keras.layers.Dense(decoder_units)\n\n    def call(self, inputs, training=False, **kwargs):\n        model_output = self.model(inputs)\n        all_outputs = model_output[0]\n        pooled_output = model_output[1]\n        pooled_output = self.reducer(pooled_output)\n        return all_outputs, pooled_output, pooled_output\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n\n        self.max_sequence_length = max_sequence_length\n        self.batch_size = batch_size\n\n        self.decoder_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n\n        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n                                                       memory=None,\n                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n\n        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n                                                                 self.attention,\n                                                                 attention_layer_size=self.decoder_units)\n\n        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n                                                sampler=self.sampler,\n                                                output_layer=self.generation_dense)\n\n    def build_initial_state(self, batch_size, encoder_state):\n        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n        return initial_state\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        decoder_output, _, _ = self.decoder(input_emb,\n                                            initial_state=inputs['initial_state'],\n                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n        return decoder_output\n","metadata":{"id":"tIz7Z9QV1mPh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, AutoTokenizer, AutoConfig\n\nmodel_name = 'prajjwal1/bert-tiny'\n\n#config = AutoConfig.from_pretrained(model_name)\n#model = BertForQuestionAnswering.from_pretrained(model_name, config=config)\ninput_tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"vV1rlQqR1mPi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next block of code is an example of encoding of a question-context pair: in this case, the question is the first part of the encoding, and the context is the second part. There are two special tokens: [CLS] token at the start of the encoding, [SEP] token between the question and the context, and at the end of the encoding.\n\nIn this case the context is the *span*, to provide a better example that explains the encoding.","metadata":{"id":"kswqcr_m1mPi"}},{"cell_type":"code","source":"line = 42\n\nencoded_question = input_tokenizer(train_df['q'][line], return_tensors='tf', padding=True)\nprint(train_df['q'][line])\n\nencoded_span = input_tokenizer(train_df['span'][line], return_tensors='tf', padding=True)\nprint(train_df['span'][line])\n\nencoded_qs = input_tokenizer(train_df['q'][line], train_df['span'][line], return_tensors='tf', padding=True)\n\nprint('= '*40)\nfor idx, tok in zip(encoded_qs.input_ids.numpy()[0], input_tokenizer.convert_ids_to_tokens(encoded_qs.input_ids[0])):\n    print(\"{}\\t{}\".format(idx, tok))","metadata":{"id":"gxqLjPlL1mPi","outputId":"8b7d0cea-e17f-44ac-abb6-290888dc468a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets encode a part of the dataset in sentences of: [CLS] question [SEP] passage [SEP]. Otherwise, the training would be very slow.","metadata":{"id":"UYZk4xQk1mPi"}},{"cell_type":"code","source":"max_length = 512  # The maximum length of a feature (question and context)\ndoc_stride = (\n    128  # The authorized overlap between two part of the context when splitting\n)\nsentences = 20\nsample = 10","metadata":{"id":"xBycfrq51mPj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input\nqs = train_df['q'][range(sentences)] # questions\ncs = train_df['p'][range(sentences)] # contexts\n\nbatch_size = len(qs)\n\nencoded_inputs = input_tokenizer(\n    qs.values.tolist(),\n    cs.values.tolist(),\n    #train_df['q'].values.tolist(),\n    #train_df['p'].values.tolist(),\n    truncation=\"only_second\",\n    max_length=max_length,\n    stride=doc_stride,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n    padding=\"max_length\",\n    return_tensors='tf'\n)\n\ninput_ids, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask\nmax_input_length = input_ids.shape[-1]","metadata":{"id":"ZhjywqdM1mPj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"max_input_length:\", max_input_length)\nprint(\"encoded_inputs shape =\", encoded_inputs['input_ids'].shape)","metadata":{"id":"emIgycXc1mPk","outputId":"c171e250-6167-456b-deb7-761b923cbcce","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 'token_type_ids' encodes wether the encoded id is part of the question (=0) or the context (=1). The Attention Mask indicates if the input is needed (=1) or it's padding (=0).","metadata":{"id":"1cpxq9qK1mPl"}},{"cell_type":"markdown","source":"Prepare also the expected outputs, for the training (this code follows the example given by the tutors, but I'm not convinced that this is the proper formatting for a QA Bert model).","metadata":{"id":"sIGwPsUg1mPm"}},{"cell_type":"code","source":"# Output\noutputs = \"<start> \" + train_df['a'][range(sentences)] + \" <end>\"\n\noutput_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\noutput_tokenizer.fit_on_texts(outputs)\n\noutput_vocab_size = len(output_tokenizer.word_index) + 1\n\nencoded_output = output_tokenizer.texts_to_sequences(outputs)\nprint(encoded_output[sample])\nmax_output_length = max([len(item) for item in encoded_output])","metadata":{"id":"APmOG4gv1mPm","outputId":"77cb34f3-579d-4a83-d2d7-065f8cb61331","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sequence_length = max(max_input_length, max_output_length)\n\nprint(\"max_output_length: {}\".format(max_output_length))\nprint(\"max_sequence_length: {}\".format(max_sequence_length))","metadata":{"id":"t4wdmkDs1mPn","outputId":"17d82207-b850-4b54-f679-e886186ee5c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_output = tf.keras.preprocessing.sequence.pad_sequences(encoded_output,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\nprint(encoded_output[sample])","metadata":{"id":"cNunTdpj1mPn","outputId":"341ad36a-a211-4744-818d-8c4785f05a5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test encoder\nencoder = Encoder(model_name=model_name,\n                    decoder_units=16)\nencoder_output, encoder_h, encoder_s = encoder({'input_ids': input_ids,\n                                                'attention_mask': attention_mask})\nprint(f'{encoder_output.shape} - {encoder_h.shape} - {encoder_s.shape}')","metadata":{"id":"-G1IKYO61mPo","outputId":"51413bbc-b166-4320-8cbb-11d6ef001804","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test decoder\ndecoder = Decoder(vocab_size=output_vocab_size,\n                    embedding_dim=50,\n                    decoder_units=16,\n                    batch_size=batch_size,\n                    max_sequence_length=max_sequence_length)\ndecoder.attention.setup_memory(encoder_output)\ninitial_state = decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n\ndecoder_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_output, tf.int32),\n    'initial_state': initial_state\n}\ndecoder_outputs = decoder(decoder_batch).rnn_output\nprint(f'{decoder_outputs.shape}')","metadata":{"id":"zkKbWjm11mPo","outputId":"b0e3ad8e-c144-4efd-a827-7c97d69e1f85","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\ntrainer = MyTrainer(encoder=encoder,\n                    decoder=decoder,\n                    max_length=max_sequence_length)","metadata":{"id":"RueCJjXO1mPp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3\nfor epoch in tqdm(range(epochs)):\n    batch = {\n        'encoder_input_ids': input_ids,\n        'encoder_attention_mask': attention_mask,\n        'decoder_target': encoded_output\n    }\n    loss = trainer.batch_fit(batch)\n    print(f'Loss - {loss}')\n\n    generated = trainer.generate(input_ids=input_ids,\n                                    attention_mask=attention_mask)\n    translated = trainer.translate(generated)\n    print(f'Translated - {translated}')","metadata":{"id":"l5H1c5ZN1mPp","outputId":"d56bee8b-043a-4526-e4d9-efca762bd374","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An example of answered question by the pretrained (*original*) model.","metadata":{"id":"P3vOKrD11mPp"}},{"cell_type":"code","source":"from transformers import TFBertForQuestionAnswering, pipeline\n\nmodel = TFBertForQuestionAnswering.from_pretrained(model_name, from_pt=True)\n\nquestion_answerer = pipeline(\"question-answering\", model=model_name)\n\noutputs = question_answerer(question=train_df['q'][0], context=train_df['p'][0])\n\nprint(\"model outputs:\", outputs)\nprint()\nprint(\"official results are (from train.json):\") \nprint(\"span_start: 151\")\nprint(\"span_end: 179\")\nprint(\"span_text: Formally established in 1475\")\nprint(\"input_text: It was formally established in 1475\")\n#print(\"start scores: {}\".format(start_scores))\n#print(\"end scores: {}\".format(end_scores))","metadata":{"id":"09T-nztL1mPp","outputId":"a3e7b98e-9407-4984-f02a-b40253b9f6a0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT2BERT Bert-Tiny","metadata":{"id":"pAG9J3GGfF76"}},{"cell_type":"code","source":"#take a subset from the training set\nstart = 0\nend = 60_000\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])\ncontexts = contexts[start:end]\nquestions = questions[start:end]\nanswers = answers[start:end]\nlen(contexts)","metadata":{"id":"oL3BVkcG5KZ8","outputId":"5dd2bf70-2ed5-42a0-b0d4-25b3c43d4561","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel, AutoTokenizer\nfrom tqdm import tqdm\n\n\nmodel_name = 'prajjwal1/bert-tiny'\n\n# tie_encoder_decoder to share weights and half the number of parameters\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name,\n                                                                        #encoder_from_pt=True,\n                                                                        #decoder_from_pt=True,\n                                                                        tie_encoder_decoder=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# set special tokens\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# set decoding params                               \nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.repetition_penalty = 5.0\nmodel.config.num_beams = 2\nmodel.config.vocab_size = model.config.encoder.vocab_size\n","metadata":{"outputId":"a75cb503-137a-4b43-8cf0-0567a309fe71","id":"25WjYqJRfsZu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings = tokenizer(questions, contexts, \n                          padding=True,\n                          truncation= 'only_second',\n                          max_length = 499,\n                          )\ninput_ids, input_attention_mask = encodings['input_ids'], encodings['attention_mask']\nlabel_values = tokenizer(answers,\n                          padding=True,\n                          truncation=True,\n                          max_length = 25,\n                          )\nlabels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n\n\n\n#Tokens with indices set to ``-100`` are ignored (masked) during training, the loss is only computed for the tokens with labels\nmasked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\nprint(f'length of input_ids: {np.shape(input_ids)}')","metadata":{"id":"_J9fZwmgqJZo","outputId":"d6b584f6-209c-408a-8a83-e13a17756085","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.keys()","metadata":{"outputId":"f0919455-47eb-47f3-cf13-9a9b27449329","id":"TXROfeJ0fsZu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.pop('token_type_ids')\nencodings.update({#'decoder_input_ids': labels,\n                 #'decoder_attention_mask': labels_mask,\n                 'labels': masked_labels\n                 })\nencodings.keys()","metadata":{"outputId":"15a092d1-a926-4d0a-9dcc-6c0350013947","id":"AEAZjiqdfsZv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomTextDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)","metadata":{"id":"6zawXNuzfsZv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parameters\nbatch_size = 16\nnum_epochs = 3\nlr = 4e-4","metadata":{"id":"Jqtjk7qM3r1r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer\n#create training dataset\ntrain_dataset = CustomTextDataset(encodings)\n#create training dataloader\ntrain_ld = torch.utils.data.DataLoader(train_dataset,\n                                     batch_size=batch_size,\n                                     )\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\noptim = torch.optim.AdamW(model.parameters(), lr=lr)\nloop_start = timer()\nfor epoch in range(num_epochs):\n    model.train()\n    loss_score = []\n    loop = tqdm(train_ld)\n    for batch in loop:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        loss, outputs = model(input_ids,\n                              attention_mask=attention_mask,\n                              labels = labels\n                        )[:2]\n        loss_score.append(loss.item())\n        #loss = outputs[0]\n        loss.backward()\n        optim.step()\n\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n    average_loss = np.mean(loss_score)\n    print(f\"\\nEpoch: {epoch}, average Loss: {average_loss}\")\nloop_end = timer()\ntime_loop = loop_end - loop_start\nprint(f'\\nTime for {num_epochs} epochs (s): {(time_loop):.3f}')","metadata":{"outputId":"f5d4f349-9488-4833-80d4-f76df98efda9","id":"AvaNFc8WfsZv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Free some memory\nimport gc\ndel encodings,input_ids,input_attention_mask,labels\ntorch.cuda.empty_cache()\ntorch.cuda.reset_accumulated_memory_stats()\ngc.collect()","metadata":{"id":"PS4LyEvk1y88","outputId":"4224b9df-5f72-419d-b753-7e415545734f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generation","metadata":{"id":"1ezjXyEag7rc"}},{"cell_type":"markdown","source":"Load test dataset.","metadata":{"id":"ZpudKS7ZhCN6"}},{"cell_type":"code","source":"input_values = tokenizer(list(test_df['q']),list(test_df['p']), padding=True, truncation=True, max_length = 499)\ninput_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']","metadata":{"id":"XeQC8zqgghs6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nl = []\nmodel.to(device)\nmodel.eval()\nfor input, mask in zip(input_ids,input_attention_mask):\n  input = np.expand_dims(np.array(input), axis=0)\n  mask = np.expand_dims(np.array(mask), axis=0)\n  generated = model.generate(input_ids=torch.tensor(input).to(device),\n                             #attention_mask=torch.tensor(mask).to(device), \n                                                 max_length=20,\n                                                 repetition_penalty=5.,\n                                                 min_length=1,\n                                                 no_repeat_ngram_size=3,\n                                                 early_stopping=True,\n                                                decoder_start_token_id = model.config.decoder_start_token_id,\n                                                 num_beams=2,\n                                                 )\n  generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n  l.append(generated)","metadata":{"id":"Af_Oia8-GeN0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.DataFrame(l, columns = ['generated'])\nx['questions'] = test_df['q']\nx['answers'] = test_df['a']\n#pd.set_option('display.max_rows', None)\n#x.head(300)","metadata":{"id":"gWei4De6ghs6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#utility functions taken from the allennlp library for computing the F1-score\nimport collections\nimport re\nimport string\nfrom typing import Callable, Sequence, TypeVar, Tuple\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(a_pred: str, a_gold: str) -> float:\n    pred_toks = get_tokens(a_pred)\n    gold_toks = get_tokens(a_gold)\n    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n    num_same = sum(common.values())\n    if len(pred_toks) == 0 or len(gold_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return float(pred_toks == gold_toks)\n    if num_same == 0:\n        return 0.0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"id":"svvWfoUD89sR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\npredictions = x['generated']\ntrue_answers = x['answers']\nfor a_pred, a_gold in zip(predictions, true_answers):\n  score.append(compute_f1(a_pred, a_gold))\n\naverage_score = np.mean(score)\nprint(f'average_score: {average_score}')\nx['score'] = score\ntotal = len(x[x['score'] != 0])\nprint(f'length: {total} / {len(x)}')\n","metadata":{"outputId":"247cee84-a44c-444f-9902-3d44a5a0785d","id":"-A27frM4BF37","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ncorrect = x[x['score'] != 0].reset_index(drop=True)\ncorrect.head(200)","metadata":{"id":"dGdF3olw61_b","outputId":"93298116-94e3-437a-ef3d-6890abd50846","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT2BERT Distilroberta-base","metadata":{"id":"G2-Rwv5gM8fS"}},{"cell_type":"code","source":"#entire dataset\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])","metadata":{"id":"7fSwmQReM8fS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a subset from the training set\nstart = 0\nend = 60_000\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])\ncontexts = contexts[start:end]\nquestions = questions[start:end]\nanswers = answers[start:end]\nlen(contexts)","metadata":{"id":"x_gmV73fM8fS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel, AutoTokenizer\nfrom tqdm import tqdm\n\n\nmodel_name = 'distilroberta-base'\n\n# tie_encoder_decoder to share weights and half the number of parameters\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name, tie_encoder_decoder=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# set special tokens\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# set decoding params                               \nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.repetition_penalty = 5.0\nmodel.config.num_beams = 2\nmodel.config.vocab_size = model.config.encoder.vocab_size\n","metadata":{"id":"5SwrherKM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings = tokenizer(questions, contexts, \n                          padding=True,\n                          truncation= True,\n                          max_length = 512,\n                          )\ninput_ids, input_attention_mask = encodings['input_ids'], encodings['attention_mask']\nlabel_values = tokenizer(answers,\n                          padding=True,\n                          truncation=True,\n                          max_length = 25,\n                          )\nlabels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n\n\n\n#Tokens with indices set to ``-100`` are ignored (masked) during training, the loss is only computed for the tokens with labels\nmasked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\nprint(f'length of input_ids: {np.shape(input_ids)}')","metadata":{"id":"OxXjWb25M8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.keys()","metadata":{"id":"XfyOa50nM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.update({'labels': masked_labels})\nencodings.keys()","metadata":{"outputId":"15a092d1-a926-4d0a-9dcc-6c0350013947","id":"2TQ6POAgM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CreateDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)","metadata":{"id":"dxTWgEfWM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parameters\nbatch_size = 12\nnum_epochs = 3\n#also try with lr = 4e-4\nlr = 4e-5","metadata":{"id":"b5qfLBOJM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer\n#create training dataset\ntrain_dataset = CreateDataset(encodings)\n#create training dataloader\ntrain_ld = torch.utils.data.DataLoader(train_dataset,\n                                     batch_size=batch_size,\n                                     )\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\noptim = torch.optim.AdamW(model.parameters(), lr=lr)\nloop_start = timer()\nfor epoch in range(num_epochs):\n    model.train()\n    loss_score = []\n    loop = tqdm(train_ld)\n    for batch in loop:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        loss, outputs = model(input_ids,\n                              attention_mask=attention_mask,\n                              labels = labels\n                        )[:2]\n        loss_score.append(loss.item())\n        loss.backward()\n        optim.step()\n\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n    average_loss = np.mean(loss_score)\n    print(f\"Epoch: {epoch}, average Loss: {average_loss}\")\nloop_end = timer()\ntime_loop = loop_end - loop_start\nprint(f'\\nTime for {num_epochs} epochs (s): {(time_loop):.3f}')","metadata":{"id":"xKdl21v2M8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Free some memory\nimport gc\ndel encodings,input_ids,input_attention_mask,labels\ntorch.cuda.empty_cache()\ntorch.cuda.reset_accumulated_memory_stats()\ngc.collect()","metadata":{"outputId":"4224b9df-5f72-419d-b753-7e415545734f","id":"Z9UrY8RhM8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generation","metadata":{"id":"Xqq0pd7RM8fU"}},{"cell_type":"markdown","source":"Load test dataset.","metadata":{"id":"IBgEOzIDM8fU"}},{"cell_type":"code","source":"input_values = tokenizer(list(test_df['q']),list(test_df['p']), padding=True, truncation=True, max_length = 512)\ninput_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']","metadata":{"id":"LYdbBdsgM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#utility functions taken from the allennlp library for computing the F1-score\nimport collections\nimport re\nimport string\nfrom typing import Callable, Sequence, TypeVar, Tuple\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(a_pred: str, a_gold: str) -> float:\n    pred_toks = get_tokens(a_pred)\n    gold_toks = get_tokens(a_gold)\n    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n    num_same = sum(common.values())\n    if len(pred_toks) == 0 or len(gold_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return float(pred_toks == gold_toks)\n    if num_same == 0:\n        return 0.0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"id":"WcL-JtVqM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nl = []\nmodel.to(device)\n#set the model in evaluation mode\nmodel.eval()\nfor input, mask in zip(input_ids,input_attention_mask):\n  input = np.expand_dims(np.array(input), axis=0)\n  mask = np.expand_dims(np.array(mask), axis=0)\n  generated = model.generate(input_ids=torch.tensor(input).to(device),\n                                                 max_length=20,\n                                                 repetition_penalty=5.,\n                                                 min_length=1,\n                                                 no_repeat_ngram_size=3,\n                                                 early_stopping=True,\n                                                decoder_start_token_id = model.config.decoder_start_token_id,\n                                                 num_beams=2,\n                                                 )\n  generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n  l.append(generated)","metadata":{"id":"H1h4HFwTM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.DataFrame(l, columns = ['generated'])\nx['answers'] = test_df['a']\n#pd.set_option('display.max_rows', None)\n#x.head(300)","metadata":{"id":"ESKHnK6uM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\npredictions = x['generated']\ntrue_answers = x['answers']\nfor a_pred, a_gold in zip(predictions, true_answers):\n  score.append(compute_f1(a_pred, a_gold))\naverage_score = np.mean(score)\nprint(f'average_score: {average_score}')\nx['score'] = score\ntotal = len(x[x['score'] != 0])\nprint(f'length: {total} / {len(x)}')\n","metadata":{"id":"uUnYQmXGM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ncorrect = x[x['score'] != 0]\ncorrect = correct.reset_index(drop=True)\ncorrect.head(500)","metadata":{"id":"rH5l9jFTM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question generation $f_\\theta(P, Q, H)$ with text passage $P$, question $Q$ and dialogue history $H$","metadata":{"id":"scfOBhZf1mPp"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"9maQVnw61mPp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$","metadata":{"id":"WpTttkRT1mPq"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"j9Mb67HE1mPq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions","metadata":{"id":"JmviMChy1mPq"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"jDn-h3WL1mPq","trusted":true},"execution_count":null,"outputs":[]}]}