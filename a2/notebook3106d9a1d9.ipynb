{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"XAbMpqPm1mPC"}},{"cell_type":"markdown","source":"# Assignment 2 execution","metadata":{"id":"qtlMx4kv1mPK"}},{"cell_type":"code","source":"%pip install pandas numpy matplotlib transformers==4.25.1  dataset tensorflow_addons","metadata":{"id":"feevAAsT1mPL","outputId":"e7b763fb-cdd4-4491-85bf-bfb749788be8","execution":{"iopub.status.busy":"2023-01-11T09:54:10.441812Z","iopub.execute_input":"2023-01-11T09:54:10.442269Z","iopub.status.idle":"2023-01-11T09:54:21.531421Z","shell.execute_reply.started":"2023-01-11T09:54:10.442176Z","shell.execute_reply":"2023-01-11T09:54:21.529932Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\nRequirement already satisfied: transformers==4.25.1 in /opt/conda/lib/python3.7/site-packages (4.25.1)\nRequirement already satisfied: dataset in /opt/conda/lib/python3.7/site-packages (1.5.2)\nRequirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.14.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (4.64.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (22.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (3.7.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.25.1) (4.13.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (9.1.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.33.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.3)\nRequirement already satisfied: sqlalchemy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from dataset) (1.4.39)\nRequirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.7/site-packages (from dataset) (1.9.1)\nRequirement already satisfied: banal>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from dataset) (1.0.6)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.13.3)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=0.6.2->dataset) (1.2.4)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=0.6.2->dataset) (5.10.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.2->dataset) (1.1.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.25.1) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.25.1) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.25.1) (1.26.13)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.25.1) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.25.1) (2.1.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data loading","metadata":{"id":"kJSsWQg_1mPO"}},{"cell_type":"markdown","source":"### Dataset download","metadata":{"id":"1mXm1Fpq1mPP"}},{"cell_type":"code","source":"import os\nimport urllib.request\nfrom tqdm import tqdm\n\nclass DownloadProgressBar(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n        \ndef download_url(url, output_path):\n    with DownloadProgressBar(unit='B', unit_scale=True,\n                             miniters=1, desc=url.split('/')[-1]) as t:\n        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n\ndef download_data(data_path, url_path, suffix):    \n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n        \n    data_path = os.path.join(data_path, f'{suffix}.json')\n    if not os.path.exists(data_path):\n        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n        download_url(url=url_path, output_path=data_path)\n        print(\"Download completed!\")","metadata":{"id":"oOqlik_a1mPP","execution":{"iopub.status.busy":"2023-01-11T09:54:21.534683Z","iopub.execute_input":"2023-01-11T09:54:21.535124Z","iopub.status.idle":"2023-01-11T09:54:21.545917Z","shell.execute_reply.started":"2023-01-11T09:54:21.535080Z","shell.execute_reply":"2023-01-11T09:54:21.544651Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_folder = 'Dataset'","metadata":{"id":"sskRUOtB1mPR","execution":{"iopub.status.busy":"2023-01-11T09:54:21.548282Z","iopub.execute_input":"2023-01-11T09:54:21.549293Z","iopub.status.idle":"2023-01-11T09:54:21.558907Z","shell.execute_reply.started":"2023-01-11T09:54:21.549254Z","shell.execute_reply":"2023-01-11T09:54:21.557947Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Train & Validation data\ntrain_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\ndownload_data(data_path=data_folder, url_path=train_url, suffix='train')\n\n# Test data\ntest_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\ndownload_data(data_path=data_folder, url_path=test_url, suffix='test')","metadata":{"id":"CzsJWqI61mPR","execution":{"iopub.status.busy":"2023-01-11T09:54:21.562286Z","iopub.execute_input":"2023-01-11T09:54:21.562687Z","iopub.status.idle":"2023-01-11T09:54:21.569408Z","shell.execute_reply.started":"2023-01-11T09:54:21.562652Z","shell.execute_reply":"2023-01-11T09:54:21.568462Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Dataset loading","metadata":{"id":"GsM9znVV1mPT"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nfrom os import path\nfrom matplotlib import pyplot as plt","metadata":{"id":"VRsmY5Wn1mPV","execution":{"iopub.status.busy":"2023-01-11T09:54:21.571086Z","iopub.execute_input":"2023-01-11T09:54:21.571510Z","iopub.status.idle":"2023-01-11T09:54:21.578985Z","shell.execute_reply.started":"2023-01-11T09:54:21.571452Z","shell.execute_reply":"2023-01-11T09:54:21.578082Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def loadDataset(filename):\n    with open(path.join(data_folder, filename)) as file_obj:\n        df = json.load(file_obj)[\"data\"]\n    print(f'{len(df)} stories / {len(df[0][\"questions\"])} questions in the first row')\n\n    storyDType = pd.CategoricalDtype(pd.unique([story[\"story\"] for story in df]))\n    print(f\"{storyDType.categories.size} distinct stories\")\n\n    sourceDType = pd.CategoricalDtype(pd.unique([story[\"source\"] for story in df]))\n    print(f\"{sourceDType.categories.size} distinct sources: {sourceDType.categories}\")\n\n    df = np.array([\n        [\n            sourceDType.categories.get_loc(story[\"source\"]), # Sources factorization\n            storyDType.categories.get_loc(story[\"story\"]), # Sources factorization\n            story[\"questions\"][question_index][\"input_text\"],\n            story[\"answers\"][question_index][\"input_text\"],\n            story[\"answers\"][question_index][\"span_text\"],\n        ]\n        for story in df\n        for question_index in range(len(story[\"questions\"]))\n        if story[\"answers\"][question_index][\"input_text\"] != 'unknown'\n    ])\n    print(f'{df.shape} question-answer pairs x columns')\n    print(f'First row: {df[0]}')\n    \n    # https://marcobonzanini.com/2021/09/15/tips-for-saving-memory-with-pandas/\n    # https://pandas.pydata.org/docs/user_guide/categorical.html\n    df = pd.DataFrame({\n        \"source\": pd.Series(pd.Categorical.from_codes(df[:,0].astype(np.int16), dtype=sourceDType)),\n        \"p\": pd.Series(pd.Categorical.from_codes(df[:,1].astype(np.int16), dtype=storyDType)),\n        \"q\": df[:,2],\n        \"a\": df[:,3],\n        \"span\": df[:,4],\n    })\n\n    return df","metadata":{"id":"lI74kMng1mPW","execution":{"iopub.status.busy":"2023-01-11T09:54:21.580170Z","iopub.execute_input":"2023-01-11T09:54:21.580476Z","iopub.status.idle":"2023-01-11T09:54:21.593390Z","shell.execute_reply.started":"2023-01-11T09:54:21.580435Z","shell.execute_reply":"2023-01-11T09:54:21.592250Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df = loadDataset(\"train.json\")\ntrain_df.count()","metadata":{"id":"jsZrgF5d1mPX","outputId":"0da854bb-f9f6-4d48-f1f0-60f8461abf02","execution":{"iopub.status.busy":"2023-01-11T09:54:21.594879Z","iopub.execute_input":"2023-01-11T09:54:21.595612Z","iopub.status.idle":"2023-01-11T09:54:28.241444Z","shell.execute_reply.started":"2023-01-11T09:54:21.595576Z","shell.execute_reply":"2023-01-11T09:54:28.240382Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"7199 stories / 20 questions in the first row\n6605 distinct stories\n5 distinct sources: Index(['wikipedia', 'cnn', 'gutenberg', 'race', 'mctest'], dtype='object')\n(107276, 5) question-answer pairs x columns\nFirst row: ['0' '0' 'When was the Vat formally opened?'\n 'It was formally established in 1475' 'Formally established in 1475']\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"source    107276\np         107276\nq         107276\na         107276\nspan      107276\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"pd.unique(train_df[\"p\"]).size","metadata":{"id":"lOQSe3Sc1mPY","outputId":"108e74fe-6f6e-4d41-bc31-d88b4b21271a","execution":{"iopub.status.busy":"2023-01-11T09:54:28.242942Z","iopub.execute_input":"2023-01-11T09:54:28.243303Z","iopub.status.idle":"2023-01-11T09:54:28.254409Z","shell.execute_reply.started":"2023-01-11T09:54:28.243270Z","shell.execute_reply":"2023-01-11T09:54:28.253152Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"6605"},"metadata":{}}]},{"cell_type":"code","source":"pd.unique(train_df[\"span\"]).size","metadata":{"id":"WJaLGY_p1mPZ","outputId":"c2d31830-e668-4f22-e59c-5574a0a561a4","execution":{"iopub.status.busy":"2023-01-11T09:54:28.256117Z","iopub.execute_input":"2023-01-11T09:54:28.257406Z","iopub.status.idle":"2023-01-11T09:54:28.333317Z","shell.execute_reply.started":"2023-01-11T09:54:28.257349Z","shell.execute_reply":"2023-01-11T09:54:28.332131Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"99470"},"metadata":{}}]},{"cell_type":"code","source":"pd.unique(train_df[\"source\"]).size","metadata":{"id":"V_ml0A1b1mPZ","outputId":"88bda1d9-bf6b-4d3c-ab86-e18033ffba97","execution":{"iopub.status.busy":"2023-01-11T09:54:28.342354Z","iopub.execute_input":"2023-01-11T09:54:28.345473Z","iopub.status.idle":"2023-01-11T09:54:28.362136Z","shell.execute_reply.started":"2023-01-11T09:54:28.345405Z","shell.execute_reply":"2023-01-11T09:54:28.358805Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"G9VjurfZ1mPZ","outputId":"a5f0827d-9f74-459d-f2ad-0a5acdc1c86f","execution":{"iopub.status.busy":"2023-01-11T09:54:28.363609Z","iopub.execute_input":"2023-01-11T09:54:28.366099Z","iopub.status.idle":"2023-01-11T09:54:28.404023Z","shell.execute_reply.started":"2023-01-11T09:54:28.366062Z","shell.execute_reply":"2023-01-11T09:54:28.403045Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      source                                                  p  \\\n0  wikipedia  The Vatican Apostolic Library (), more commonl...   \n1  wikipedia  The Vatican Apostolic Library (), more commonl...   \n2  wikipedia  The Vatican Apostolic Library (), more commonl...   \n3  wikipedia  The Vatican Apostolic Library (), more commonl...   \n4  wikipedia  The Vatican Apostolic Library (), more commonl...   \n\n                                   q                                    a  \\\n0  When was the Vat formally opened?  It was formally established in 1475   \n1           what is the library for?                             research   \n2                 for what subjects?                     history, and law   \n3                               and?     philosophy, science and theology   \n4          what was started in 2014?                           a  project   \n\n                                                span  \n0                       Formally established in 1475  \n1           he Vatican Library is a research library  \n2  Vatican Library is a research library for hist...  \n3  Vatican Library is a research library for hist...  \n4  March 2014, the Vatican Library began an initi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>p</th>\n      <th>q</th>\n      <th>a</th>\n      <th>span</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wikipedia</td>\n      <td>The Vatican Apostolic Library (), more commonl...</td>\n      <td>When was the Vat formally opened?</td>\n      <td>It was formally established in 1475</td>\n      <td>Formally established in 1475</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wikipedia</td>\n      <td>The Vatican Apostolic Library (), more commonl...</td>\n      <td>what is the library for?</td>\n      <td>research</td>\n      <td>he Vatican Library is a research library</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wikipedia</td>\n      <td>The Vatican Apostolic Library (), more commonl...</td>\n      <td>for what subjects?</td>\n      <td>history, and law</td>\n      <td>Vatican Library is a research library for hist...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wikipedia</td>\n      <td>The Vatican Apostolic Library (), more commonl...</td>\n      <td>and?</td>\n      <td>philosophy, science and theology</td>\n      <td>Vatican Library is a research library for hist...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wikipedia</td>\n      <td>The Vatican Apostolic Library (), more commonl...</td>\n      <td>what was started in 2014?</td>\n      <td>a  project</td>\n      <td>March 2014, the Vatican Library began an initi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.memory_usage(deep=True)","metadata":{"id":"nrRkHhES1mPa","outputId":"f1736c82-64dc-4b69-ba5f-87e4dc3652b2","execution":{"iopub.status.busy":"2023-01-11T09:54:28.405318Z","iopub.execute_input":"2023-01-11T09:54:28.405938Z","iopub.status.idle":"2023-01-11T09:54:28.474964Z","shell.execute_reply.started":"2023-01-11T09:54:28.405901Z","shell.execute_reply":"2023-01-11T09:54:28.474034Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Index          128\nsource      107764\np         14241201\nq          9110271\na          7714559\nspan      12090637\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"#test_df = loadDataset(\"test.json\")\n#test_df.count()","metadata":{"id":"v5aer1n41mPa","execution":{"iopub.status.busy":"2023-01-11T09:54:28.479224Z","iopub.execute_input":"2023-01-11T09:54:28.481459Z","iopub.status.idle":"2023-01-11T09:54:28.487214Z","shell.execute_reply.started":"2023-01-11T09:54:28.481422Z","shell.execute_reply":"2023-01-11T09:54:28.485937Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-Processing","metadata":{"id":"XRs7YHKY1mPa"}},{"cell_type":"markdown","source":"### Check unanswerable questions in the Train Dataset","metadata":{"id":"k0GmOXam1mPb"}},{"cell_type":"code","source":"idx = (train_df.a == 'unknown')\nunanswerable = train_df[idx]\nunanswerable.q.count()","metadata":{"id":"6Aq524zd1mPb","outputId":"f6f42ea9-b280-4f34-ae88-e2ea8bdeb02f","execution":{"iopub.status.busy":"2023-01-11T09:54:28.488984Z","iopub.execute_input":"2023-01-11T09:54:28.490073Z","iopub.status.idle":"2023-01-11T09:54:28.517080Z","shell.execute_reply.started":"2023-01-11T09:54:28.490036Z","shell.execute_reply":"2023-01-11T09:54:28.516243Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"All unanswerable questions in the Train Dataset have been already removed.","metadata":{"id":"lTdjLFYI1mPb"}},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{"id":"maBslb2n1mPb"}},{"cell_type":"code","source":"train_df[\"p\"][42]","metadata":{"id":"VH5Ep77e1mPb","outputId":"6f4b6b2c-ad95-43d9-e59f-12fe2a4aefc4","execution":{"iopub.status.busy":"2023-01-11T09:54:28.521499Z","iopub.execute_input":"2023-01-11T09:54:28.523909Z","iopub.status.idle":"2023-01-11T09:54:28.535348Z","shell.execute_reply.started":"2023-01-11T09:54:28.523875Z","shell.execute_reply":"2023-01-11T09:54:28.534322Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"Lassiter, will you be my rider?\" Jane had asked him. \\n\\n\"I reckon so,\" he had replied. \\n\\nFew as the words were, Jane knew how infinitely much they implied. She wanted him to take charge of her cattle and horse and ranges, and save them if that were possible. Yet, though she could not have spoken aloud all she meant, she was perfectly honest with herself. Whatever the price to be paid, she must keep Lassiter close to her; she must shield from him the man who had led Milly Erne to Cottonwoods. In her fear she so controlled her mind that she did not whisper this Mormon\\'s name to her own soul, she did not even think it. Besides, beyond this thing she regarded as a sacred obligation thrust upon her, was the need of a helper, of a friend, of a champion in this critical time. If she could rule this gun-man, as Venters had called him, if she could even keep him from shedding blood, what strategy to play his flame and his presence against the game of oppression her churchmen were waging against her? Never would she forget the effect on Tull and his men when Venters shouted Lassiter\\'s name. If she could not wholly control Lassiter, then what she could do might put off the fatal day. \\n\\nOne of her safe racers was a dark bay, and she called him Bells because of the way he struck his iron shoes on the stones. When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes. A rider\\'s love of a thoroughbred shone in them. Round and round Bells he walked, plainly weakening all the time in his determination not to take one of Jane\\'s favorite racers. '"},"metadata":{}}]},{"cell_type":"code","source":"train_df[\"q\"][42]","metadata":{"id":"zYcgBVcG1mPc","outputId":"8540b69c-d459-42df-a5b9-94793de5d8c2","execution":{"iopub.status.busy":"2023-01-11T09:54:28.539925Z","iopub.execute_input":"2023-01-11T09:54:28.541594Z","iopub.status.idle":"2023-01-11T09:54:28.550716Z","shell.execute_reply.started":"2023-01-11T09:54:28.541560Z","shell.execute_reply":"2023-01-11T09:54:28.549853Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'Was Lassiter impressed with the horse?'"},"metadata":{}}]},{"cell_type":"code","source":"train_df[\"a\"][42]","metadata":{"id":"5zvn9Tv01mPc","outputId":"c8d1810a-428d-4a57-a571-ac727b48a55e","execution":{"iopub.status.busy":"2023-01-11T09:54:28.552404Z","iopub.execute_input":"2023-01-11T09:54:28.553981Z","iopub.status.idle":"2023-01-11T09:54:28.564969Z","shell.execute_reply.started":"2023-01-11T09:54:28.553947Z","shell.execute_reply":"2023-01-11T09:54:28.564088Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'Yes'"},"metadata":{}}]},{"cell_type":"code","source":"train_df[\"span\"][42]","metadata":{"id":"NokHfoLm1mPc","outputId":"44e2cc0a-53bc-47f8-efb7-76c45b9e201f","execution":{"iopub.status.busy":"2023-01-11T09:54:28.567275Z","iopub.execute_input":"2023-01-11T09:54:28.569944Z","iopub.status.idle":"2023-01-11T09:54:28.582174Z","shell.execute_reply.started":"2023-01-11T09:54:28.569910Z","shell.execute_reply":"2023-01-11T09:54:28.581258Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes.'"},"metadata":{}}]},{"cell_type":"code","source":"train_df[\"source\"][42]","metadata":{"id":"JskMPh-Y1mPc","outputId":"674fce52-5190-43e1-8f88-c419b3ef350e","execution":{"iopub.status.busy":"2023-01-11T09:54:28.583323Z","iopub.execute_input":"2023-01-11T09:54:28.583800Z","iopub.status.idle":"2023-01-11T09:54:28.592106Z","shell.execute_reply.started":"2023-01-11T09:54:28.583760Z","shell.execute_reply":"2023-01-11T09:54:28.591132Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'gutenberg'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Distribution statistics","metadata":{"id":"IhdF4AR7ef05"}},{"cell_type":"markdown","source":"Sources:","metadata":{"id":"ebBVUlveef05"}},{"cell_type":"code","source":"train_df[\"source\"].hist()","metadata":{"id":"8lIuR9RO1mPd","outputId":"6fba63dd-4283-4b29-e09e-58a28c87263b","execution":{"iopub.status.busy":"2023-01-11T09:54:28.593727Z","iopub.execute_input":"2023-01-11T09:54:28.594558Z","iopub.status.idle":"2023-01-11T09:54:28.950248Z","shell.execute_reply.started":"2023-01-11T09:54:28.594521Z","shell.execute_reply":"2023-01-11T09:54:28.949289Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Occurrences of 25 most popular stories:","metadata":{"id":"Ax5uSP6Lef06"}},{"cell_type":"code","source":"story_counts = train_df[\"p\"].cat.codes.value_counts(sort=True)\nstory_counts[:25].plot(kind=\"bar\", figsize=(15,5))","metadata":{"id":"wUgsqg4Uef06","outputId":"c72091e4-0ded-464a-e902-3fecc6ccc716","execution":{"iopub.status.busy":"2023-01-11T09:54:28.952456Z","iopub.execute_input":"2023-01-11T09:54:28.953556Z","iopub.status.idle":"2023-01-11T09:54:29.289472Z","shell.execute_reply.started":"2023-01-11T09:54:28.953510Z","shell.execute_reply":"2023-01-11T09:54:29.288376Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Occurrences of 25 least popular stories:","metadata":{"id":"qL-m6SuVef06"}},{"cell_type":"code","source":"story_counts[-25:-1].plot(kind=\"bar\", figsize=(15,5))","metadata":{"id":"jkiwEygzef06","outputId":"26ee9a1b-c469-4287-dd92-7dd5afb82ca4","execution":{"iopub.status.busy":"2023-01-11T09:54:29.291200Z","iopub.execute_input":"2023-01-11T09:54:29.291838Z","iopub.status.idle":"2023-01-11T09:54:29.635570Z","shell.execute_reply.started":"2023-01-11T09:54:29.291798Z","shell.execute_reply":"2023-01-11T09:54:29.634322Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA3AAAAE+CAYAAADMJ/LiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtjklEQVR4nO3debglZXXv8e+PQaPigKEdLtC0Ayo4oXZQrySCA+IQINEkYKJoVHJVRE004nAl6o0XTdQEo0FULhoVNI6toohz1KDdzDMiDnRHpRXFAaMC6/7x1mk3x3M4BzjUrurz/TzPfrp21d6nVu8aV9X7rkpVIUmSJEkavi2mHYAkSZIkaXFM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkktpp2AHPZbrvtatWqVdMOQ5IkSZKm4pRTTvlBVa2YPX6QCdyqVatYt27dtMOQJEmSpKlI8u25xtuEUpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGYsEELsmOST6X5Nwk5yR57hyfSZIjk1yU5Mwk95+YdlCSr3evg5b6PyBJkiRJy8ViHuR9JfA3VXVqklsCpyQ5qarOnfjMo4Gdu9cDgX8FHpjktsDhwGqguu+uqaofLen/QpIkSZKWgQXvwFXVd6vq1G74p8B5wPazPrYf8M5qTgZuk+SOwKOAk6rqsi5pOwnYZ0n/B5IkSZK0TCzmDtwmSVYB9wO+OmvS9sAlE+/Xd+PmGz/X3z4YOBhg5cqV1xrHqsM+fh2int+3jnjsDf4bm1ssSxEHGMt8hhSLJEmSxmfRRUySbAN8AHheVf1kqQOpqqOranVVrV6xYsVS/3lJkiRJGr1FJXBJtqYlb++uqg/O8ZENwI4T73foxs03XpIkSZJ0HS2mCmWAtwPnVdXr5/nYGuDJXTXKBwGXV9V3gROBvZNsm2RbYO9unCRJkiTpOlpMH7iHAE8CzkpyejfuJcBKgKo6CjgBeAxwEXAF8NRu2mVJXgWs7b73yqq6bMmilyRJkqRlZMEErqq+BGSBzxTw7HmmHQMcc72ikyRJkiRtsugiJpIkSZKk6TKBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSR2GqhDyQ5BngccGlV3WuO6S8E/nzi7+0CrKiqy5J8C/gpcBVwZVWtXqrAJUmSJGm5WcwduGOBfeabWFX/UFW7VdVuwIuBL1TVZRMf2aubbvImSZIkSTfAgglcVX0RuGyhz3UOBI67QRFJkiRJkua0ZH3gktycdqfuAxOjC/hUklOSHLxU85IkSZKk5WjBPnDXwR8CX57VfHKPqtqQ5HbASUnO7+7o/ZYuwTsYYOXKlUsYliRJkiRtHpayCuUBzGo+WVUbun8vBT4E7D7fl6vq6KpaXVWrV6xYsYRhSZIkSdLmYUkSuCS3Bh4KfGRi3C2S3HJmGNgbOHsp5idJkiRJy9FiHiNwHLAnsF2S9cDhwNYAVXVU97E/Aj5VVT+f+OrtgQ8lmZnPe6rqk0sXuiRJkiQtLwsmcFV14CI+cyztcQOT4y4G7nt9A5MkSZIkXdNS9oGTJEmSJN2ITOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJBZM4JIck+TSJGfPM33PJJcnOb17vXxi2j5JLkhyUZLDljJwSZIkSVpuFnMH7lhgnwU+8x9VtVv3eiVAki2BNwGPBnYFDkyy6w0JVpIkSZKWswUTuKr6InDZ9fjbuwMXVdXFVfUr4Hhgv+vxdyRJkiRJLF0fuAcnOSPJJ5Lcsxu3PXDJxGfWd+PmlOTgJOuSrNu4ceMShSVJkiRJm4+lSOBOBXaqqvsCbwQ+fH3+SFUdXVWrq2r1ihUrliAsSZIkSdq83OAErqp+UlU/64ZPALZOsh2wAdhx4qM7dOMkSZIkSdfDDU7gktwhSbrh3bu/+UNgLbBzkjsluQlwALDmhs5PkiRJkparrRb6QJLjgD2B7ZKsBw4HtgaoqqOAJwDPTHIl8AvggKoq4MokhwAnAlsCx1TVOTfK/0KSJEmSloEFE7iqOnCB6f8C/Ms8004ATrh+oUmSJEmSJi1VFUpJkiRJ0o3MBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRsIETpIkSZJGwgROkiRJkkbCBE6SJEmSRmLBBC7JMUkuTXL2PNP/PMmZSc5K8pUk952Y9q1u/OlJ1i1l4JIkSZK03CzmDtyxwD7XMv2bwEOr6t7Aq4CjZ03fq6p2q6rV1y9ESZIkSRLAVgt9oKq+mGTVtUz/ysTbk4EdliAuSZIkSdIsS90H7mnAJybeF/CpJKckOXiJ5yVJkiRJy8qCd+AWK8letARuj4nRe1TVhiS3A05Kcn5VfXGe7x8MHAywcuXKpQpLkiRJkjYbS3IHLsl9gLcB+1XVD2fGV9WG7t9LgQ8Bu8/3N6rq6KpaXVWrV6xYsRRhSZIkSdJm5QYncElWAh8EnlRVF06Mv0WSW84MA3sDc1aylCRJkiQtbMEmlEmOA/YEtkuyHjgc2Bqgqo4CXg78LvDmJABXdhUnbw98qBu3FfCeqvrkjfB/kCRJkqRlYTFVKA9cYPrTgafPMf5i4L6//Q1JkiRJ0vWx1FUoJUmSJEk3EhM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRoJEzhJkiRJGolFJXBJjklyaZKz55meJEcmuSjJmUnuPzHtoCRf714HLVXgkiRJkrTcLPYO3LHAPtcy/dHAzt3rYOBfAZLcFjgceCCwO3B4km2vb7CSJEmStJwtKoGrqi8Cl13LR/YD3lnNycBtktwReBRwUlVdVlU/Ak7i2hNBSZIkSdI8tlqiv7M9cMnE+/XduPnG/5YkB9Pu3rFy5colCkvSfFYd9vEb/De+dcRjBxEHGMt8hhLLUsQBxjIfY7lx4gBjmc9QYtnc1lswlvkMJZYhbEODKWJSVUdX1eqqWr1ixYpphyNJkiRJg7NUCdwGYMeJ9zt04+YbL0mSJEm6jpYqgVsDPLmrRvkg4PKq+i5wIrB3km274iV7d+MkSZIkSdfRovrAJTkO2BPYLsl6WmXJrQGq6ijgBOAxwEXAFcBTu2mXJXkVsLb7U6+sqmsrhiJJkiRJmseiEriqOnCB6QU8e55pxwDHXPfQJEmSJEmTBlPERJIkSZJ07UzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJEzgJEmSJGkkFpXAJdknyQVJLkpy2BzT35Dk9O51YZIfT0y7amLamiWMXZIkSZKWla0W+kCSLYE3AY8E1gNrk6ypqnNnPlNVz5/4/HOA+038iV9U1W5LFrEkSZIkLVOLuQO3O3BRVV1cVb8Cjgf2u5bPHwgctxTBSZIkSZJ+YzEJ3PbAJRPv13fjfkuSnYA7AZ+dGP07SdYlOTnJ/vPNJMnB3efWbdy4cRFhSZIkSdLystRFTA4A3l9VV02M26mqVgNPBP4pyV3m+mJVHV1Vq6tq9YoVK5Y4LEmSJEkav8UkcBuAHSfe79CNm8sBzGo+WVUbun8vBj7PNfvHSZIkSZIWaTEJ3Fpg5yR3SnITWpL2W9Ukk9wD2Bb4z4lx2ya5aTe8HfAQ4NzZ35UkSZIkLWzBKpRVdWWSQ4ATgS2BY6rqnCSvBNZV1UwydwBwfFXVxNd3Ad6S5GpasnjEZPVKSZIkSdLiLZjAAVTVCcAJs8a9fNb7v5vje18B7n0D4pMkSZIkdZa6iIkkSZIk6UZiAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI2ECJ0mSJEkjYQInSZIkSSNhAidJkiRJI7GoBC7JPkkuSHJRksPmmP6UJBuTnN69nj4x7aAkX+9eBy1l8JIkSZK0nGy10AeSbAm8CXgksB5Ym2RNVZ0766PvrapDZn33tsDhwGqggFO67/5oSaKXJEmSpGVkMXfgdgcuqqqLq+pXwPHAfov8+48CTqqqy7qk7SRgn+sXqiRJkiQtb4tJ4LYHLpl4v74bN9vjk5yZ5P1JdryO35UkSZIkLWCpiph8FFhVVfeh3WV7x3X9A0kOTrIuybqNGzcuUViSJEmStPlYTAK3Adhx4v0O3bhNquqHVfXL7u3bgAcs9rsTf+PoqlpdVatXrFixmNglSZIkaVlZTAK3Ftg5yZ2S3AQ4AFgz+YEkd5x4uy9wXjd8IrB3km2TbAvs3Y2TJEmSJF1HC1ahrKorkxxCS7y2BI6pqnOSvBJYV1VrgEOT7AtcCVwGPKX77mVJXkVLAgFeWVWX3Qj/D0mSJEna7C2YwAFU1QnACbPGvXxi+MXAi+f57jHAMTcgRkmSJEkSS1fERJIkSZJ0IzOBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJFYVAKXZJ8kFyS5KMlhc0z/6yTnJjkzyWeS7DQx7aokp3evNUsZvCRJkiQtJ1st9IEkWwJvAh4JrAfWJllTVedOfOw0YHVVXZHkmcBrgT/rpv2iqnZb2rAlSZIkaflZzB243YGLquriqvoVcDyw3+QHqupzVXVF9/ZkYIelDVOSJEmStJgEbnvgkon367tx83ka8ImJ97+TZF2Sk5PsP9+XkhzcfW7dxo0bFxGWJEmSJC0vCzahvC6S/AWwGnjoxOidqmpDkjsDn01yVlV9Y/Z3q+po4GiA1atX11LGJUmSJEmbg8XcgdsA7Djxfodu3DUkeQTwUmDfqvrlzPiq2tD9ezHweeB+NyBeSZIkSVq2FpPArQV2TnKnJDcBDgCuUU0yyf2At9CSt0snxm+b5Kbd8HbAQ4DJ4ieSJEmSpEVasAllVV2Z5BDgRGBL4JiqOifJK4F1VbUG+AdgG+DfkwB8p6r2BXYB3pLkalqyeMSs6pWSJEmSpEVaVB+4qjoBOGHWuJdPDD9inu99Bbj3DQlQkiRJktQs6kHekiRJkqTpM4GTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJEwgZMkSZKkkTCBkyRJkqSRMIGTJEmSpJFYVAKXZJ8kFyS5KMlhc0y/aZL3dtO/mmTVxLQXd+MvSPKoJYxdkiRJkpaVBRO4JFsCbwIeDewKHJhk11kfexrwo6q6K/AG4DXdd3cFDgDuCewDvLn7e5IkSZKk62gxd+B2By6qqour6lfA8cB+sz6zH/CObvj9wMOTpBt/fFX9sqq+CVzU/T1JkiRJ0nWUqrr2DyRPAPapqqd3758EPLCqDpn4zNndZ9Z3778BPBD4O+DkqnpXN/7twCeq6v1zzOdg4ODu7d2BC27Yf43tgB/cwL+xVIxlbsby24YSBxjLfIxlbsYyt6HEMpQ4wFjmYyxzM5a5DSWWocQBm2csO1XVitkjt1qCP7wkqupo4Oil+ntJ1lXV6qX6ezeEsczNWIYbBxjLfIxlbsYyt6HEMpQ4wFjmYyxzM5a5DSWWocQByyuWxTSh3ADsOPF+h27cnJ9JshVwa+CHi/yuJEmSJGkRFpPArQV2TnKnJDehFSVZM+sza4CDuuEnAJ+t1jZzDXBAV6XyTsDOwNeWJnRJkiRJWl4WbEJZVVcmOQQ4EdgSOKaqzknySmBdVa0B3g78W5KLgMtoSR7d594HnAtcCTy7qq66kf4vsy1Zc8wlYCxzM5bfNpQ4wFjmYyxzM5a5DSWWocQBxjIfY5mbscxtKLEMJQ5YRrEsWMREkiRJkjQMi3qQtyRJkiRp+kzgJEmSJGkkTOAkSZIkaSRM4CRJkiRpJDaLBC7JoUl2XPiT05PkkVOa7z2SvCjJkd3rRUl26TmGlUl+pxtOkqcmeWOSZ3bPDVy2ktwhyR264RVJ/jjJPacd1zQl2XdmfdHwDG1/m2SLJFt0wzdJcv8kt51CHK63i5BkjyR/nWTvKc1/ZZLbdMOrkjwhyb2mEctQJLlVkrvMMf4+04hnCIa2n9PckvxBkrt3ww9J8oIkj51CHL2vL5tFFcoklwM/B74BHAf8e1VtnG5U15TkO1W1sud5vgg4EDgeWN+N3oH2mIfjq+qInuI4G9i9qq5I8hrgLsCHgYcBVNVf9hFHF8t2VfWDifd/AewOnA28tXrcIJL8FXAYEOA1wFO6OPYAXltVb+8xltcDH6iqL/c1z2uJ5Re07fkTtO35xB4fPzJXPPcA9gO270ZtANZU1XnLMZYh7W+T7A+8Bbga+F/AS4CfAXcHnllVH+0xlqGtt7cFDgH+i/aon5cADwbOA15dVT/qKY6vVdXu3fAzgGcDHwL2Bj7a13Gom/9hwF8BvwT+EXgB8GXgQcDbq+r1fcUylySvrqqX9DzPPwX+CbgU2Bp4SlWt7aadWlX37zmeOwCH07bplwPPAR5PW2+fW1Xf7SmOweznunhWApdW1X8nCe184f60x3S9taqu7Dme3YGqqrVJdgX2Ac6vqhN6jOGfaOdvW9EedfZw2v73ocBpVfXCHmPpfX3ZXBK404AHAI8A/gzYFziF9iN+sKp+2lMcsx9wvmkS8LCqukUfcUzEcyFwz6r69azxNwHOqaqde4rj3KratRs+Bfi9qrq6e39GVd23jzi6+W06ICV5GfD7wHuAxwHrq+r5PcZyFvBA4GbAt4G7VtX3kmwLfK6qdusxlo1dDCuA9wLHVdVpfc1/Viyn0ZL7J9AuNtyLdsJ3XFV9oedYBnERZEixDGV/OxHLo2nb0Bm0fcsFSXaiXZBY3XMsg1hvu3hOAM4CbgXs0g2/D3gkcN+q2q+nOE6rqvt1w2uBx1TVxiS3AE6uqnv3EUc3/3OA1cDNgW8Bd56I5atV1duduCRHzh4FPAl4J0BVHdpTHKcDj66q73Yn5e8EXlxVH5pcdn1J8kng48AtgCcC76Ydo/cHHtHnestA9nNdPEO6EH44bb+7FXAS7Tzmc7R9y4lV9fc9xXEObT97M9rFzO2732drWgLX5/bc//pSVaN/AafOer919+MdB2zsMY4fAY+lZf+Trz2B70/hdzkf2GmO8TsBF/QYx4m0BBbgAzMxAb8LnNHzb3La5HoD3GJinTmr51hOnRg+Y9a00/qKY3J+wN2A/w2c060/hwN3m9bv0r2/A3Ao8J/AJT3HciGw9RzjbwJ8fTnGMpT9bTfv0yaGz762OKfwu0xtve3mf3r3b4ANc03rKY4zgG27/f26+ZZfT7Gc2f27Je2O0xbzrT89xHIJ8C7gycBB3WvjzHCPcZw16/0daSeeh/a9Dc1eJ4DvzJp2eo9xDGY/183/3InhU2atu32fR53VbUM3B34C3Kobf7OZbaynOM7u/v0d2vn3zbr3W07+Xpvr+rK59D/K5Jtqd5zWAGuS3LzHOE4Grqg5rrYmuaDHOGY8D/hMkq/TDhYAK4G70prW9OXpwDuT/B1wOXB6d9XvNsBf9xgHwM2S3I/W/3PLqvo5tHUmSd/NnSrJ1t36uqnNdtePpu/+qQVQVRcCrwJe1fV/OBA4gbbO9GX29vw94EjgyO7OSp+uBv4H7e7kpDt205ZjLEPZ37Zgki2q3dH/y4lxW9IS215DmXwz5fUWYIvubv4tgW2SrKqqbyX5Xfr9bW5NO+EMbZ93x2p3e7Zh1m/Wg1OTvId2d+czwDu6Oz4PozVF69OutH3tPsALquq/khxeVe/oOY6fJrlLVX0DoFs2e9HuHk+jP/bkse+d1zLtxjao/RxwSZKHVdVnaXePdwS+3W3PfbuyWvPwK5J8o6p+AlBVv0jS57Ho40n+g5bAvQ14X5KTaTdOvthjHDCF9WVzaUJ5t+7EU7Okde7fnWv2mVlbU+ibkVY85W602+7ruzh6PQlO8nm6ZKXzxO6A9bu0W/99NrlaCfxXzWq7nmR7YJeq+nSPsZxWPTeVmU+SPavq89OOAyDJPsC/AHNeBKmqTy63WIa0v03ye7Q7CP89a/wqYI+qelePsQxmvQVIciCtbxPAs4BndsO7AK+oqqOnEdeM7qTm9lX1zR7nuRXwJ7RjwPtpx8YnAt8B3jRzQa9PSR5A64/3cdp2vKrn+d+XduH567PGbw38aVW9u+d4XknrA/6zWePvChxRVU/oKY7B7OcAugIZ76TdXbqc1lf+dNqF8BdU1Wd6jOWrwF7VmivOXEAjya1p3T966zeZ5MG0vngnpxXi+SPa9vz+Ps8vp7G+bBYJHGxKVKiqq7s+XvcCvlVVl005rttOO4a5JNlm9g5yOeuu2N+0qq6Ychz7VtV8fSlvzPkObn1IcnsmLjxU1fenFMeQLoJMPZYkt6mqH/c1vzGb1vY8Mf8tacf5K7vkZTfattRLIYiJOAZ5fB6KrijFs4AHV9VfDCCeqa63k5LsQVdorKo+1fO8VwI/qaofdxeFVtMKdZzdZxyzYhrChfCbVtUv5xi/HXDHqjqrx1i2mrkI3t3Vvwdw8TT2LX3v5zaXxwjsD3wX2JBkP+A/gH8Azkzyhz3G8ZAk5yU5J8kDk5wErE1ySXeVYEh6ay6SZLJp0/ZJPpPkR0m+kuRufcXRzX/OsshVdVXfyVvaIwMmX48Hjp5532cswK+6k4iZ2PZK8jdJHt1zHCTZrWsG8Xngtd3rC0lOTtJrRTRoO2NaZamLaXe/zp5G8jYTS1WdXFUfqKoPAPefQiw/SPLpJE9LV459WtIek/KJJB9Pcpckxyb5cZKvpf/HpQxpe54pVnX1xB3+36c1Ldqt5zj2ZwDH5y6WWyX5v0n+LckTZ017c5+xTMz39sD9aH0l/2YK8x/aevu1ieFn0Fod3BI4PK2KaF9xHAZ8ATg5ydOBT9IKd7w3Sd9dPzapqvOq6iPd/v/2fSdvXQzXSN6S3LVbb27Xc/L2FOD7SS7szlXOpFX0PqNrgdCbaeznNos7cBlIJbJux/M0YBvgo8D+VfWl7qTzjVX1kD7imIhnvp1MgJdWVS/PSso1Kz++D/g0rb3yfrQmIw/vI45u/lfRTsSPp1WH67vfw2Qsv6YVeLmU37SffgKtaU9Vv1WlzgD2rKofJXkhrRnCCbQTvnVV9eIeYzkd+Kuq+uqs8Q8C3lL9Vi3dldaPaRWtueJpwO1oB/bnVtXlPcYy1/b8EuDVANVTCfS06qkvpvWP3Af4Eq2j9keq6hd9xDARyxdpB8ltgCOAF9GqqD4OeF7P+5bBbM9dPIPYpodyfO5i+QDtIszJtD6Tv6Y1o/9lei6Zn2Q34ChaH8EN3egdgB8Dz6qqU3uKY2jr7abm/Jli1dIMqGJpF89cyfSbaXduqaoP9hjL54A/qaofJHkSrfDZF2nVKI+uqjf2FMdZwF60BP8M4H5V9Y3uoshJVdXbcwynsZ/bLO7AQesw3rWl/05VXdCN+zb9/h+3rqqzquo/aVVnvtTFcSptofbt1bTqX7ec9dqG6S37u1XV0d2dhA8BfT9w90zaicwWtM6lZyQ5LK15RN/+J229WFtVT62qpwI/6IZ7PWjSCrrMPBfqz4CHV9X/oe2Q+n4o5i1mJ28AVXUyrfhAn44Bnl1Vd6X1OTi/qu5Ee3ZUb8/p67yCdoDcht9sy1tODPfl11X1sar6c9oJ57uBPwXWpxWI6NMtq+qjVXVcF9fx1XyUtu/r05C2ZxjQNj2Q4zPAXarqsKr6cFXtS6tE/NlMpxDEsbSLQLtU1SO61z1oxcf+X49xDG293SLJtt0ySXXP0qrWP7HPZ51d1V2Q+jHwC+CHE3FMw3tpFx0eB/xh97pF9+/jeo5lRf3mebqH0pr+Pp12fHpGj3FcVVU/6PYtP6vfFOKZSneLvvdzm0sVyqFUIptcSLOvbvZdEQ3awenDVXXK7Aldk4C+7JD2zJsAK/KbyovQSq32qaq1X38p8NK0594cAHwp7WHr/7PHQNYmeSTwnO6K1ou4ZoGVPv0kyb263+YHtKpOv6DtI/o+yfpEko/TOmzPFOvYkVZuu7eiIZ2bTeyIv5bkqG74rVNoRnNP4HW0g/YrqnUgP6iqXtFzHJua2nYnOO+jVf+6Ne1ZTX3acmJ49h3IXve5A9ueYUDb9ECOzwA3nYiFqvr7JBtodw+26TmWeS9UdXd5ejHA9XYoVUuHVLEUWqJ9BC3R/lfYVDjpqVOI5ddJtq+qDcDPaA+wBvgl19wn39i+k+T/0i5gnp/kdcAHac9i67WfL/S/n9tcmlAOohJZkn2BT9esvlRplXEeX1Wv7SOOifneHbis5ngafJLb93WVIslBs0at6Zr13AE4tKpe0kccXSybmmfMGh/gD2oKD9zt5v8/aBXjVlfVnacw//sA/0a79Q/wENpJzb2B11dVr3dWuvbs+3HNYh1rquqEnuP4IK3Z5GeBPwa2raq/TKvQdnZV3b3PeLqY9gP+FngDrVpbr+tLkhdU1T/2Oc/5JPkr4N01d8W6Q6rqeVOKa6rbcxfDILbpoRyfu3m+FvhUzarwm1bh9Y1VtXOPsRxJexjzXBeqvllVfT7qZyam7Wn7lamtt/NJz1VLM8yKpVsAz6FdKHsRcPyUzhf2BN5Ee67vbYH705rh7kGr5t3L8SHJrYBn05bRvwCPAp5KW0avqh6LNU1jP7dZJHDSYiV5Yt/JyFh0V4r25poVrk6sZVxxMK1Ix0toz2w6g1bG+qfd3aZdumad04hrG9pD1h9YVX8wjRi0sD4vlM0zf7fpWZLcmXYxZkfgKuBC4D3VPcuq51gGcaFK4zKERLs7Bj6Ra+5bPlJV508jnuVos0jgkpxKu2163Ewb2CnFcQjtisgPuivAxwD3AS4AnlY9l57t7nAdTnvI78tpV24eD5xHa3vfy9WJ7oD5MuC/aE0A3gA8uIvjhVX1rT7iGJp5ls8fA+fT4/IZmu4q40G0dXXyJOuoGtBztparLil4Oq3/2yer6ssT017W9bOamiQXVlWv1W27+c7Vn/dUWoXB1DItmd9dbPhb2va8A/ArWlXXo6rq2J5jOZTWX+iLwGNod9d/TOsX/azluH8Z+vY8LUM5rxyLJLerqkt7nucdaOdOxYDPoZJ8oqqWvKL35lLEZFvawww/l1Y++vldE5a+PXOiY+c/A2+oqtvQbnW/ZQrxHEtrq30J8Dla/4fH0MqbHtVzHGtpbaVPpm1cj6b1ZzqmxzhmmsrMDN86yduTnJnkPV3loj4dy28vn8fS//K5Vkk+0fMs3w7sREv2Pwd8rBv3siTP6TOQJKuTfC7Ju5LsmOSkJJcnWZuk1wefJ9kmySvTHlNyeZKNaY9WeEqfcdD2ZQ+ldeo/Mslk37Ney44n+WmSn3Svnyb5KXCXmfF9xkLrZ3bKrNf2tCRuXc+xzJTMPyKtZP6Bs6b1WTL/3bTKv4+iFeI5EngSsFeSV/cYB7QCC4/ukpJHAPesqpfSqqm+oc9AuuPPEWmPHrosyQ+74SPS7+M5BrM9D8xQziuBa6wv5095fSHJbWe/gK+lFZ/pszDdsbQbAVM/h0py/3leD+BGenTLZnMHrn5Tpv73aeWt/5i2YI+rqqN7iuOCmT4xSdZW1e9NTDuzeixp2s1zU3+vtAIdKyemnV5Vuw0gjjn7pN2IsUyuK28Dvge8lba+PLSq9u8xlkEsn25+85XPDvCxqrpjj7FcY1tJcnJVPSjJTYHTq6q353ulPRrkcNqB/LXA86vq/UkeDvyfqurt+Y5JPgJ8iPYYjj+lda4/nnZ3e0NffUknl09aP5E3A9vR9rsn97w9H0lbNi+caaqY5JvVKoX2KsnfAI/sYjlrmrF08x5EyfwkZ9TEoz9mjo3dnfZzq1Ve7EVa2fHV3W+wLa3U+Opu2tnVY2n4JCfS+ta+o6q+1427A/AU4GFVtXdPcQxmex6SoZxXTsQz3/pyEK3CbC/rSzffq4Fvzxq9A60ZZfXVrHNg51BX0R4vNFehnQdV1ZJXot9c7sBtUlX/UVXPol35fA2tqV5f3p/2INk7Ax9K8rwkOyWZ6VTZt8nl+85rmXZjuzrJ3dI6ed48ycwB8670W7FottVV9bKq+nZVvYH2rK8+DWX5QLtD+o+0KoeTr3+knSD36ddphX9mEstfwaaHh/Z9xWnrqvpEtTL1VVXv72L5DK2qX59WVdWxVbW+2jPf9q2qr9M6bfd5pXxTRa2qurKqDqb1D/wsPVfyq6pDaa0djktyaJcUTOWqZFW9jtYU7eVJXp/kltOKpTOUkvk/T7IHbCr0dRm0h9Iz98nOjeltwNokb6U9OPtNXVwrZuLq0aqqes3MyThsKkN+BK0FQl8Gsz0P1ZTPK2fMt768hn7XF4AX0roG7VtVd+ouUq3vhvvskzekc6jzaM+v3Wv2i9Y6Y8ltLo8RuHD2iKq6itZEr7ey41X10i5ZO45WXeqmwMHAh4E/7yuOCR9Jsk1V/ayqXjYzskucfus3uxH9Le3B5lfTqie9OK1C2q1pv0+fbpdW/j3ArZKkfnMbuu8NfijLB36z8/n67AlJLpnj8zemF9KarfySto86oItjBa05ZZ/+O8netHW1kuxfVR9O8lBa37w+/TzJHlX1pdknwkn6PBFel2Sfqtq0b62qV6SVY//XHuOYmfcpSR4BHEK7Atp3Yj0Zy3rgT7rlcxLtIcDTMpSS+c8E3ppkZ+Ac4GmwaXt+U49xUFX/nOTTwC7A66oruFCtUnPfxYC+neRvaXdUZu4e3552B67Pfe6gtucBGcR55YShrC9U1euSvBd4Q3d+cDjTuVg1pHOov2P+c8gbpevHZtGEcrbuat/utDLfn5pyLP9WVU+a0rwfCJxXVT9JcjPgMFq513OBV1fV5dOIq4vtY7SrN1f3PN/DZ416c1Vt7JoivLaqntxnPJOmud4meQKtBO4Fc0zbv6o+3HM8DwaurPaMol1pfVTOr/4fI3BfWtPJq4Hn005GD6JVi3tGVX2lx1juQ7uDMHMi/JdVdWF3InxgVR3ZYyy70+5ITnX5zBHL7wN7AeumEcusuH6f1rfoa9M4DmVYJfN3od29OLkmHvkwO3FYTromnIfRqlDerhv9fWANrdrtj+b7bg+xvXOax8IhmvZ55VDXl+5i1UtodwjvMI0YJmKZ9jK6B20/99U+9nObRQKX5GtVtXs3/AzacyE+RCuf/NGuSUIfcayZY/TDaE0R6Jqx9CbJOcB9q+rKJEcDV9CeZ/Lwbnwvza4G+Lv0upFdSxyDWG+7+c9O9l9Mq57Xe7LfJdmPpt19O4m2Q/48rX/RiVX1933Fcm2SPLWq/t+044B+Y5lj+TyQ1oG89+UzpHVlju35WbTWF71vzwvpeX05lPZbnE/rzP/cqvpIN623vnhj0vPymX18Du0iyFSOz0MxpOPzQqZ9LOrOGe5SVWf3vO4OZhl1+7ln01oz7UYf+7mqGv0LOG1ieC2wohu+Be2uQl9xnAq8C9iTduV1T9rT4B9KK5DR9+9y3mRss6ad3ufyGcrvQruVfQHtxOpbwH7z/UZ9/C4Tw1Nbb7t5ngNs1Q0fTXsI8R60phEf7DmWs2h9I28O/AS4VTf+ZsCZfcayQJzfmXYM04hlSMtnYLGcNjE81e15gOvLNt3wKlpFzufO/s18TW35DOb4PKSX2/PwYxnSMprGfm5z6QO3RXd7eQvaXcWNAFX18yRX9hjHauC5wEtplchOT/KLqvpCjzFMmrwackaS1VW1LsndaBXJ+vIAhvO7HAw8oKp+lmQVrfDMqqr6Z/rvUD+U9RZgi6qamefq+s3Voi8lOb3nWK6s1tfgiiTfqO4Bu1X1i7TqV71JcuZ8k4BeHzsxoFgGs3wGFsuQtuchrS9bVNfSoaq+lWRP2n53J/rf5w7GgJbPkI7PQ+L2PPxYhrSMet/PbS4J3K1pz9wJrdDAHavqu2kPEO3tAFGtP9cbkvx79+/3me5v/HTgn5O8jFYF5z+7DqeXdNN6MbDfZUgnE4NYbztDSfYBfpXk5lV1Be3kAoAkt6b1RevT7WnPr5rdvyBAb/3fBhbLkJbPkGIZ0vYMw1lfvp9kt6o6HaC7ePY42jNA791jHEMziOUzsOPzkLg9Dz+WIS2j3vdzm8VGWlWr5pl0NfBHPYYCXKMS2WNpzXqmolq/packuRVwJ9ryXl9dBaMpxDOE32UwJxMDW28Hkex3/qDaIwNmTi5mbE0rINKnj9GaRZw+e0KSzy/TWIa0fAYTy8C2ZxjO+vJk4BpXw7u7/U9O8pYe4xiaoSwfYDDH58Fwex5+LANbRr3v5zaLIibSYiXZgdbs6ntzTHtIVX15CmENxlCSfUmSJM3NBE6SJEmSRqLvBxdLkiRJkq4nEzhJkiRJGgkTOEmSJEkaCRM4SZIkSRqJ/w8Qb81vyTCRygAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Histogram of story popularities:","metadata":{"id":"9-EVRQQVef06"}},{"cell_type":"code","source":"story_counts.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"CJLPGCfcef07","outputId":"903506fa-8eb0-4b2e-d820-6ce6399506b2","execution":{"iopub.status.busy":"2023-01-11T09:54:29.637459Z","iopub.execute_input":"2023-01-11T09:54:29.638582Z","iopub.status.idle":"2023-01-11T09:54:30.796423Z","shell.execute_reply.started":"2023-01-11T09:54:29.638533Z","shell.execute_reply":"2023-01-11T09:54:30.795514Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA20AAAEvCAYAAADW/SmEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWCklEQVR4nO3df4xl53kX8O+Dl0SVl06LHK2KbViXdSMsW2qbUQwqrWYFadd1Jy5RVLyyQlySLJZq1IpIsAGkRJUQbiFIjQitFmKllVJvQyDg9bok5ceSfxJwHKI6jpvWNRvVq2CTBk3ZNCI4ffhjrs1kO7O9u3Nnzzszn49kee+5Z8995rnv3nu/c97z3uruAAAAMKY/NnUBAAAAbE1oAwAAGJjQBgAAMDChDQAAYGBCGwAAwMCENgAAgIEdmLqAJLnhhhv68OHDU5eRr371q7n++uunLmPf0v9p6f+09H96noNp6f+09H9a+j+tUfr/5JNPfrm7X7PZfZOGtqpaTbJ65MiRfPrTn56ylCTJuXPnsrKyMnUZ+5b+T0v/p6X/0/McTEv/p6X/09L/aY3S/6r64lb3TTo9srvPdPeJpaWlKcsAAAAYlmvaAAAABia0AQAADExoAwAAGJjQBgAAMDChDQAAYGBCGwAAwMCENgAAgIEJbQAAAAMT2gAAAAZ2YOoC2J0Onzw7137nH7p7hysBAIC9zZk2AACAgQltAAAAA5s0tFXValWdWltbm7IMAACAYU0a2rr7THefWFpamrIMAACAYZkeCQAAMDChDQAAYGBCGwAAwMCENgAAgIEJbQAAAAMT2gAAAAYmtAEAAAzswNQFsLnDJ8/Otd/5h+7e4UoAAIApOdMGAAAwMKENAABgYEIbAADAwIQ2AACAgQltAAAAAxPaAAAABia0AQAADExoAwAAGNjCv1y7qv5ckp9MckOS/9DdP7/ox2DnzPul3gAAwLUx15m2qnq4ql6sqs9dsv1YVX2hqp6tqpNJ0t3PdPcDSX4syfctvmQAAID9Y97pkR9Mcmzjhqq6Lsn7k9yV5LYkx6vqttl9b0xyNsnjC6sUAABgH5ortHX3J5J85ZLNr0/ybHc/191fT3I6yT2z/R/t7ruS3LfIYgEAAPab6u75dqw6nOSx7r59dvvNSY5199tnt9+S5M4kH0nypiSvTvLr3f3+LY53IsmJJDl06NDrTp8+vb2fZAEuXryYgwcPTl1GkuSpC2sLPd4dNy4N/bjJWP3fj/R/Wvo/Pc/BtPR/Wvo/Lf2f1ij9P3r06JPdvbzZfQtfiKS7zyU5N8d+p5KcSpLl5eVeWVlZdClX7Ny5cxmhjiS5f8ELgpy/b2Xox03G6v9+pP/T0v/peQ6mpf/T0v9p6f+0dkP/t7Pk/4UkN2+4fdNsGwAAAAuyndD2RJJbq+qWqnpVknuTPHolB6iq1ao6tba22Cl5AAAAe8W8S/4/kuSTSV5bVc9X1du6+6UkDyb5WJJnkny4u5++kgfv7jPdfWJpaf7rngAAAPaTua5p6+7jW2x/PJb1BwAA2DHbmR4JAADADps0tLmmDQAA4PImDW2uaQMAALg80yMBAAAGJrQBAAAMzDVtAAAAA3NNGwAAwMBMjwQAABiY0AYAADAwoQ0AAGBgFiIBAAAYmIVIAAAABmZ6JAAAwMCENgAAgIEJbQAAAAM7MHUBXBuHT56dugQAAOAqWD0SAABgYFaPBAAAGJhr2gAAAAYmtAEAAAxMaAMAABiY0AYAADAwoQ0AAGBglvwHAAAYmCX/AQAABmZ6JAAAwMCENgAAgIEJbQAAAAMT2gAAAAYmtAEAAAxMaAMAABiY0AYAADAwoQ0AAGBgk4a2qlqtqlNra2tTlgEAADCsSUNbd5/p7hNLS0tTlgEAADAs0yMBAAAGJrQBAAAMTGgDAAAYmNAGAAAwMKENAABgYEIbAADAwIQ2AACAgQltAAAAAxPaAAAABia0AQAADExoAwAAGNikoa2qVqvq1Nra2pRlAAAADGvS0NbdZ7r7xNLS0pRlAAAADMv0SAAAgIEJbQAAAAMT2gAAAAYmtAEAAAxMaAMAABiY0AYAADAwoQ0AAGBgQhsAAMDAhDYAAICBCW0AAAADE9oAAAAGJrQBAAAMTGgDAAAYmNAGAAAwMKENAABgYEIbAADAwA7sxEGr6keT3J3kW5N8oLs/vhOPAwAAsNfNfaatqh6uqher6nOXbD9WVV+oqmer6mSSdPe/6e53JHkgyV9dbMkAAAD7x5VMj/xgkmMbN1TVdUnen+SuJLclOV5Vt23Y5e/P7gcAAOAqzB3auvsTSb5yyebXJ3m2u5/r7q8nOZ3knlr3M0l+tbs/s7hyAQAA9pfq7vl3rjqc5LHuvn12+81JjnX322e335LkziS/meStSZ5I8tnu/oVNjnUiyYkkOXTo0OtOnz69vZ9kAS5evJiDBw9OXUaS5KkLa1OXsBB33Lg0974j9X8/0v9p6f/0PAfT0v9p6f+09H9ao/T/6NGjT3b38mb37chCJN39viTv+yP2OZXkVJIsLy/3ysrKTpRyRc6dO5cR6kiS+0+enbqEhTh/38rc+47U//1I/6el/9PzHExL/6el/9PS/2nthv5vd8n/C0lu3nD7ptk2AAAAFmC7oe2JJLdW1S1V9aok9yZ5dN6/XFWrVXVqbW1vTAUEAABYtCtZ8v+RJJ9M8tqqer6q3tbdLyV5MMnHkjyT5MPd/fS8x+zuM919Ymlp/uueAAAA9pO5r2nr7uNbbH88yeMLqwgAAIBXbHd6JAAAADto0tDmmjYAAIDLmzS0uaYNAADg8kyPBAAAGJjQBgAAMDChDQAAYGBzL/m/E6pqNcnqkSNHpiyDARw+eTbvvOOl3H/y7B+57/mH7r4GFQEAwBgsRAIAADAw0yMBAAAGJrQBAAAMTGgDAAAY2KShrapWq+rU2tralGUAAAAMy0IkAAAAAzM9EgAAYGBCGwAAwMAm/XLt/ebwHF8cDQAAsJGFSAAAAAZmIRIAAICBmR7JjjIlFAAAtsdCJAAAAAMT2gAAAAYmtAEAAAxMaAMAABiY0AYAADCwSVePrKrVJKtHjhyZsowtzbvy4fmH7t7hSgAAgP3K97QBAAAMzPRIAACAgQltAAAAA5v0mja4Gq41BABgP3GmDQAAYGDOtC3AvGd+AAAArpQzbQAAAAMT2gAAAAY2aWirqtWqOrW2tjZlGQAAAMPy5doAAAADMz0SAABgYEIbAADAwIQ2AACAgQltAAAAAxPaAAAABia0AQAADExoAwAAGJjQBgAAMDChDQAAYGBCGwAAwMCENgAAgIFNGtqqarWqTq2trU1ZBgAAwLAmDW3dfaa7TywtLU1ZBgAAwLBMjwQAABiY0AYAADAwoQ0AAGBgQhsAAMDAhDYAAICBCW0AAAADE9oAAAAGJrQBAAAMTGgDAAAYmNAGAAAwMKENAABgYEIbAADAwIQ2AACAgQltAAAAAxPaAAAABia0AQAADGzhoa2qvrOqPlBVH1n0sQEAAPabA/PsVFUPJ/mRJC929+0bth9L8nNJrkvyL7r7oe5+LsnbhDZ2i8Mnz8613/mH7t7hSgAA4A+b90zbB5Mc27ihqq5L8v4kdyW5LcnxqrptodUBAADsc3OFtu7+RJKvXLL59Ume7e7nuvvrSU4nuWfB9QEAAOxr1d3z7Vh1OMljL0+PrKo3JznW3W+f3X5LkjuTvDvJP0jyhqxPmfyHWxzvRJITSXLo0KHXnT59ens/yQJcvHgxBw8efOX2UxfWJqxm/zn0LckLX1vc8e64cWmu/eZ9nuc93m516fjn2tL/6XkOpqX/09L/aen/tEbp/9GjR5/s7uXN7pvrmrYr0d2/m+SBOfY7leRUkiwvL/fKysqiS7li586dy8Y67p/zWicW4513vJT3PrW4IXn+vpW59pv3eZ73eLvVpeOfa0v/p+c5mJb+T0v/p6X/09oN/d/O6pEXkty84fZNs20AAAAsyHZC2xNJbq2qW6rqVUnuTfLoYsoCAAAgmX/J/0eSrCS5oaqeT/Lu7v5AVT2Y5GNZX/L/4e5++koevKpWk6weOXLkyqqGgfkKAQAAFmmu0Nbdx7fY/niSx6/2wbv7TJIzy8vL77jaYwAAAOxl25keCQAAwA5b+OqRV8L0SHbSvNMUAQBgZJOeaevuM919Ymlpb3//FQAAwNUyPRIAAGBgQhsAAMDAhDYAAICBTRraqmq1qk6tra1NWQYAAMCwLEQCAAAwMNMjAQAABia0AQAADExoAwAAGJiFSAAAAAZmIRIAAICBmR4JAAAwMKENAABgYEIbAADAwIQ2AACAgVk9EgAAYGBWjwQAABiY6ZEAAAADE9oAAAAGJrQBAAAMTGgDAAAYmNAGAAAwsANTPnhVrSZZPXLkyJRlwFwOnzw7dQkAAOxDlvwHAAAYmOmRAAAAAxPaAAAABia0AQAADExoAwAAGJjQBgAAMDChDQAAYGBCGwAAwMB8uTYAc5n3C+bPP3T3DlcCAPuLL9cGAAAYmOmRAAAAAxPaAAAABia0AQAADExoAwAAGJjQBgAAMDChDQAAYGBCGwAAwMCENgAAgIEJbQAAAAMT2gAAAAYmtAEAAAzswJQPXlWrSVaPHDkyZRnANh0+eXau/c4/dPckj7sTjz2VqXq9l2zVw3fe8VLuv+Q+fQRgBJOeaevuM919YmlpacoyAAAAhmV6JAAAwMCENgAAgIEJbQAAAAMT2gAAAAYmtAEAAAxMaAMAABiY0AYAADAwoQ0AAGBgQhsAAMDAhDYAAICBCW0AAAADE9oAAAAGJrQBAAAMTGgDAAAYmNAGAAAwMKENAABgYEIbAADAwA4s+oBVdX2Sf5bk60nOdfeHFv0YAAAA+8VcZ9qq6uGqerGqPnfJ9mNV9YWqeraqTs42vynJR7r7HUneuOB6AQAA9pV5p0d+MMmxjRuq6rok709yV5LbkhyvqtuS3JTkd2a7fWMxZQIAAOxP1d3z7Vh1OMlj3X377PZfSPKe7v6h2e13zXZ9Psn/6u7Hqup0d9+7xfFOJDmRJIcOHXrd6dOnt/WDLMLFixdz8ODBV24/dWFtwmr2n0PfkrzwtamrGM8dNy7Nve+8Y3azY146/q/keLvBvH3cTg+3Y7P+b2Yn6lv087zo3sxruz/HZq9BU/0s85pqvO6Eef8NsDO2+x6wG8bYyIz/xbja18RR+n/06NEnu3t5s/u2c03bjfn/Z9SS9bB2Z5L3JfmnVXV3kjNb/eXuPpXkVJIsLy/3ysrKNkpZjHPnzmVjHfefPDtdMfvQO+94Ke99auGXWe565+9bmXvfecfsZse8dPxfyfF2g3n7uJ0ebsdm/d/MTtS36Od50b2Z13Z/js1eg6b6WeY11XjdCfP+G2BnbPc9YDeMsZEZ/4txta+Ju6H/C/+E3N1fTfLjiz4uAADAfrSdJf8vJLl5w+2bZtsAAABYkO2EtieS3FpVt1TVq5Lcm+TRKzlAVa1W1am1tb1z3QwAAMAizbvk/yNJPpnktVX1fFW9rbtfSvJgko8leSbJh7v76St58O4+090nlpZcvAoAALCZua5p6+7jW2x/PMnjC60IAACAV2xneiQAAAA7bNLQ5po2AACAy5s0tLmmDQAA4PJMjwQAABiY0AYAADAw17QBAAAMrLp76hpSVf8zyRenriPJDUm+PHUR+5j+T0v/p6X/0/McTEv/p6X/09L/aY3S/z/T3a/Z7I4hQtsoqurT3b08dR37lf5PS/+npf/T8xxMS/+npf/T0v9p7Yb+u6YNAABgYEIbAADAwIS2b3Zq6gL2Of2flv5PS/+n5zmYlv5PS/+npf/TGr7/rmkDAAAYmDNtAAAAAxPaZqrqWFV9oaqeraqTU9ez11XVzVX1n6rq81X1dFX95Gz7e6rqQlV9dvbfD09d615VVeer6qlZnz892/Ynq+rXquq3Zv//9qnr3Iuq6rUbxvhnq+r3quqnjP+dU1UPV9WLVfW5Dds2He+17n2z94Nfr6rvna7yvWGL/v+jqvqNWY8/WlXfNtt+uKq+tuHfwS9MVvgesUX/t3y9qap3zcb/F6rqh6apeu/Yov+/sqH356vqs7Ptxv+CXeYz5656DzA9MklVXZfkN5O8IcnzSZ5Icry7Pz9pYXtYVX1Hku/o7s9U1Z9I8mSSH03yY0kudvc/nrK+/aCqzidZ7u4vb9j2s0m+0t0PzX558e3d/XemqnE/mL3+XEhyZ5Ifj/G/I6rqB5JcTPJL3X37bNum43324fVvJvnhrD8vP9fdd05V+16wRf9/MMl/7O6XqupnkmTW/8NJHnt5P7Zvi/6/J5u83lTVbUkeSfL6JH8qyb9P8l3d/Y1rWvQesln/L7n/vUnWuvunjf/Fu8xnzvuzi94DnGlb9/okz3b3c9399SSnk9wzcU17Wnd/qbs/M/vz/07yTJIbp62KrI/7X5z9+Rez/qLGzvpLSX67u784dSF7WXd/IslXLtm81Xi/J+sfrrq7P5Xk22Zv+lylzfrf3R/v7pdmNz+V5KZrXtg+scX438o9SU539//p7v+e5Nmsf07iKl2u/1VVWf+F9SPXtKh95DKfOXfVe4DQtu7GJL+z4fbzESCumdlvlb4nyX+ZbXpwdjr6YdPzdlQn+XhVPVlVJ2bbDnX3l2Z//h9JDk1T2r5yb775zdr4v3a2Gu/eE669v57kVzfcvqWq/ltV/eeq+v6pitoHNnu9Mf6vre9P8kJ3/9aGbcb/DrnkM+eueg8Q2phUVR1M8q+S/FR3/16Sn0/yZ5N8d5IvJXnvdNXteX+xu783yV1JfmI2feMVvT532vzpHVRVr0ryxiT/crbJ+J+I8T6dqvp7SV5K8qHZpi8l+dPd/T1J/laSX66qb52qvj3M680Yjuebf3Fn/O+QTT5zvmI3vAcIbesuJLl5w+2bZtvYQVX1x7P+j+dD3f2vk6S7X+jub3T3HyT55zElY8d094XZ/19M8tGs9/qFl6cAzP7/4nQV7gt3JflMd7+QGP8T2Gq8e0+4Rqrq/iQ/kuS+2YemzKbl/e7sz08m+e0k3zVZkXvUZV5vjP9rpKoOJHlTkl95eZvxvzM2+8yZXfYeILSteyLJrVV1y+w33/cmeXTimva02RzuDyR5prv/yYbtG+cM/5Ukn7v077J9VXX97GLcVNX1SX4w671+NMlbZ7u9Ncm/nabCfeObfsNq/F9zW433R5P8tdkKYn8+6wsEfGmzA3D1qupYkr+d5I3d/fsbtr9mtkBPquo7k9ya5Llpqty7LvN682iSe6vq1VV1S9b7/1+vdX37xF9O8hvd/fzLG4z/xdvqM2d22XvAgakLGMFs5aoHk3wsyXVJHu7upycua6/7viRvSfLUy8vcJvm7SY5X1Xdn/RT1+SR/Y4ri9oFDST66/jqWA0l+ubv/XVU9keTDVfW2JF/M+sXR7IBZWH5DvnmM/6zxvzOq6pEkK0luqKrnk7w7yUPZfLw/nvVVw55N8vtZX9WTbdii/+9K8uokvzZ7LfpUdz+Q5AeS/HRV/d8kf5Dkge6edxENNrFF/1c2e73p7qer6sNJPp/1aas/YeXI7dms/939gfzha5oT438nbPWZc1e9B1jyHwAAYGCmRwIAAAxMaAMAABiY0AYAADAwoQ0AAGBgQhsAAMDAhDYAAICBCW0AAAADE9oAAAAG9v8AOy4PzXD/zOUAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### Removing rows with outlier story lengths to save memory","metadata":{}},{"cell_type":"code","source":"train_df.count()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:30.800458Z","iopub.execute_input":"2023-01-11T09:54:30.801282Z","iopub.status.idle":"2023-01-11T09:54:30.832675Z","shell.execute_reply.started":"2023-01-11T09:54:30.801245Z","shell.execute_reply":"2023-01-11T09:54:30.831580Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"source    107276\np         107276\nq         107276\na         107276\nspan      107276\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"LOGARITHMIC histogram of story length:","metadata":{}},{"cell_type":"code","source":"story_lengths = train_df[\"p\"].str.len()\nstory_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:30.835733Z","iopub.execute_input":"2023-01-11T09:54:30.838869Z","iopub.status.idle":"2023-01-11T09:54:31.555674Z","shell.execute_reply.started":"2023-01-11T09:54:30.838830Z","shell.execute_reply":"2023-01-11T09:54:31.554346Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"p_length_limit = story_lengths.quantile(0.999)\np_length_limit","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:31.557583Z","iopub.execute_input":"2023-01-11T09:54:31.558214Z","iopub.status.idle":"2023-01-11T09:54:31.569513Z","shell.execute_reply.started":"2023-01-11T09:54:31.558175Z","shell.execute_reply":"2023-01-11T09:54:31.568428Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"4317.0"},"metadata":{}}]},{"cell_type":"code","source":"p_length_mask = story_lengths < p_length_limit\np_length_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:31.571500Z","iopub.execute_input":"2023-01-11T09:54:31.572091Z","iopub.status.idle":"2023-01-11T09:54:31.583146Z","shell.execute_reply.started":"2023-01-11T09:54:31.572054Z","shell.execute_reply":"2023-01-11T09:54:31.582241Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"True     107166\nFalse       110\nName: p, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_df = train_df[p_length_mask]\ntrain_df.count()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:31.592257Z","iopub.execute_input":"2023-01-11T09:54:31.593053Z","iopub.status.idle":"2023-01-11T09:54:31.623764Z","shell.execute_reply.started":"2023-01-11T09:54:31.593015Z","shell.execute_reply":"2023-01-11T09:54:31.622634Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"source    107166\np         107166\nq         107166\na         107166\nspan      107166\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### Removing rows with outlier question/answer/span lengths to save memory","metadata":{}},{"cell_type":"markdown","source":"LOGARITHMIC histogram of question length:","metadata":{}},{"cell_type":"code","source":"question_lengths = train_df[\"q\"].str.len()\nquestion_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"kFobL8Upef07","outputId":"f2d6a442-5f64-4792-a744-76e213be9e9b","execution":{"iopub.status.busy":"2023-01-11T09:54:31.625385Z","iopub.execute_input":"2023-01-11T09:54:31.626081Z","iopub.status.idle":"2023-01-11T09:54:32.494764Z","shell.execute_reply.started":"2023-01-11T09:54:31.626043Z","shell.execute_reply":"2023-01-11T09:54:32.493610Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"q_length_limit = question_lengths.quantile(0.999)\nq_length_limit","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:32.499462Z","iopub.execute_input":"2023-01-11T09:54:32.502405Z","iopub.status.idle":"2023-01-11T09:54:32.517878Z","shell.execute_reply.started":"2023-01-11T09:54:32.502344Z","shell.execute_reply":"2023-01-11T09:54:32.516783Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"89.0"},"metadata":{}}]},{"cell_type":"markdown","source":"LOGARITHMIC histogram of answer length:","metadata":{}},{"cell_type":"code","source":"answer_lengths = train_df[\"a\"].str.len()\nanswer_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"id":"_7phh51Nef07","outputId":"feec6821-80f7-421c-bc31-6bc0103f38f6","execution":{"iopub.status.busy":"2023-01-11T09:54:32.522779Z","iopub.execute_input":"2023-01-11T09:54:32.525818Z","iopub.status.idle":"2023-01-11T09:54:33.644282Z","shell.execute_reply.started":"2023-01-11T09:54:32.525775Z","shell.execute_reply":"2023-01-11T09:54:33.643185Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"a_length_limit = answer_lengths.quantile(0.999)\na_length_limit","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:33.645704Z","iopub.execute_input":"2023-01-11T09:54:33.646719Z","iopub.status.idle":"2023-01-11T09:54:33.658260Z","shell.execute_reply.started":"2023-01-11T09:54:33.646678Z","shell.execute_reply":"2023-01-11T09:54:33.657037Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"142.8350000000064"},"metadata":{}}]},{"cell_type":"code","source":"span_lengths = train_df[\"span\"].str.len()\nspan_lengths.hist(log=True,bins=75,figsize=(15,5))","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:33.660437Z","iopub.execute_input":"2023-01-11T09:54:33.660929Z","iopub.status.idle":"2023-01-11T09:54:34.388386Z","shell.execute_reply.started":"2023-01-11T09:54:33.660889Z","shell.execute_reply":"2023-01-11T09:54:34.387317Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"span_length_limit = span_lengths.quantile(0.999)\nspan_length_limit","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.389762Z","iopub.execute_input":"2023-01-11T09:54:34.390837Z","iopub.status.idle":"2023-01-11T09:54:34.403719Z","shell.execute_reply.started":"2023-01-11T09:54:34.390799Z","shell.execute_reply":"2023-01-11T09:54:34.402355Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"502.5050000000192"},"metadata":{}}]},{"cell_type":"code","source":"bad_length_mask = (question_lengths > q_length_limit) | (answer_lengths > a_length_limit) | (span_lengths > span_length_limit)\nbad_length_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.405681Z","iopub.execute_input":"2023-01-11T09:54:34.406080Z","iopub.status.idle":"2023-01-11T09:54:34.419777Z","shell.execute_reply.started":"2023-01-11T09:54:34.406043Z","shell.execute_reply":"2023-01-11T09:54:34.418350Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"False    106859\nTrue        307\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"excluded_stories = train_df[\"p\"][bad_length_mask].unique()\nlen(excluded_stories)","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.421697Z","iopub.execute_input":"2023-01-11T09:54:34.422153Z","iopub.status.idle":"2023-01-11T09:54:34.430758Z","shell.execute_reply.started":"2023-01-11T09:54:34.422116Z","shell.execute_reply":"2023-01-11T09:54:34.429655Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"272"},"metadata":{}}]},{"cell_type":"code","source":"excluded_mask = ~train_df[\"p\"].isin(excluded_stories)\nexcluded_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.432498Z","iopub.execute_input":"2023-01-11T09:54:34.433243Z","iopub.status.idle":"2023-01-11T09:54:34.445791Z","shell.execute_reply.started":"2023-01-11T09:54:34.433205Z","shell.execute_reply":"2023-01-11T09:54:34.444600Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True     102798\nFalse      4368\nName: p, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_df = train_df[excluded_mask]\ntrain_df.count()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.447477Z","iopub.execute_input":"2023-01-11T09:54:34.448096Z","iopub.status.idle":"2023-01-11T09:54:34.476904Z","shell.execute_reply.started":"2023-01-11T09:54:34.448062Z","shell.execute_reply":"2023-01-11T09:54:34.475863Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"source    102798\np         102798\nq         102798\na         102798\nspan      102798\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train-Validation-Test split","metadata":{"id":"mm2QCJbR1mPd"}},{"cell_type":"code","source":"train_df = train_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.478415Z","iopub.execute_input":"2023-01-11T09:54:34.478769Z","iopub.status.idle":"2023-01-11T09:54:34.492100Z","shell.execute_reply.started":"2023-01-11T09:54:34.478733Z","shell.execute_reply":"2023-01-11T09:54:34.491228Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"total_rows = len(train_df)\ntotal_rows","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.493222Z","iopub.execute_input":"2023-01-11T09:54:34.493515Z","iopub.status.idle":"2023-01-11T09:54:34.501906Z","shell.execute_reply.started":"2023-01-11T09:54:34.493488Z","shell.execute_reply":"2023-01-11T09:54:34.500797Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"102798"},"metadata":{}}]},{"cell_type":"code","source":"ideal_split_index = int(total_rows * 0.8)\nideal_split_index","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.503687Z","iopub.execute_input":"2023-01-11T09:54:34.504548Z","iopub.status.idle":"2023-01-11T09:54:34.511112Z","shell.execute_reply.started":"2023-01-11T09:54:34.504513Z","shell.execute_reply":"2023-01-11T09:54:34.510024Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"82238"},"metadata":{}}]},{"cell_type":"code","source":"train_df[ ideal_split_index-3 : ideal_split_index+1 ]","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.512910Z","iopub.execute_input":"2023-01-11T09:54:34.513692Z","iopub.status.idle":"2023-01-11T09:54:34.528334Z","shell.execute_reply.started":"2023-01-11T09:54:34.513655Z","shell.execute_reply":"2023-01-11T09:54:34.527353Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"       index     source                                                  p  \\\n82235  85698  gutenberg  CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...   \n82236  85699  gutenberg  CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...   \n82237  85700  gutenberg  CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...   \n82238  85701  gutenberg  CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...   \n\n                            q                           a  \\\n82235        Is she an adult?                         yes   \n82236  Who does she work for?  Gruensberg Leather Company   \n82237        Who is her boss?              Mr. Gruensberg   \n82238        What is her job?                   secretary   \n\n                                                    span  \n82235  She had become secretary to Mr. Gruensberg of ...  \n82236  She had become secretary to Mr. Gruensberg of ...  \n82237        She had become secretary to Mr. Gruensberg   \n82238         She had become secretary to Mr. Gruensberg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>source</th>\n      <th>p</th>\n      <th>q</th>\n      <th>a</th>\n      <th>span</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>82235</th>\n      <td>85698</td>\n      <td>gutenberg</td>\n      <td>CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...</td>\n      <td>Is she an adult?</td>\n      <td>yes</td>\n      <td>She had become secretary to Mr. Gruensberg of ...</td>\n    </tr>\n    <tr>\n      <th>82236</th>\n      <td>85699</td>\n      <td>gutenberg</td>\n      <td>CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...</td>\n      <td>Who does she work for?</td>\n      <td>Gruensberg Leather Company</td>\n      <td>She had become secretary to Mr. Gruensberg of ...</td>\n    </tr>\n    <tr>\n      <th>82237</th>\n      <td>85700</td>\n      <td>gutenberg</td>\n      <td>CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...</td>\n      <td>Who is her boss?</td>\n      <td>Mr. Gruensberg</td>\n      <td>She had become secretary to Mr. Gruensberg</td>\n    </tr>\n    <tr>\n      <th>82238</th>\n      <td>85701</td>\n      <td>gutenberg</td>\n      <td>CHAPTER XVIII \\n\\nI \\n\\nTHOUGH he saw them twi...</td>\n      <td>What is her job?</td>\n      <td>secretary</td>\n      <td>She had become secretary to Mr. Gruensberg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"before_split_mask = pd.Series(np.linspace(0, total_rows, total_rows)) < ideal_split_index\nbefore_split_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.529924Z","iopub.execute_input":"2023-01-11T09:54:34.530557Z","iopub.status.idle":"2023-01-11T09:54:34.541001Z","shell.execute_reply.started":"2023-01-11T09:54:34.530521Z","shell.execute_reply":"2023-01-11T09:54:34.540267Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"True     82238\nFalse    20560\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"split_story = train_df[\"p\"][ideal_split_index - 1]\nsplit_story_mask = train_df[\"p\"] == split_story\nsplit_story_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.542640Z","iopub.execute_input":"2023-01-11T09:54:34.543487Z","iopub.status.idle":"2023-01-11T09:54:34.552762Z","shell.execute_reply.started":"2023-01-11T09:54:34.543448Z","shell.execute_reply":"2023-01-11T09:54:34.551769Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"False    102780\nTrue         18\nName: p, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_mask = before_split_mask | split_story_mask\ntrain_mask.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.554508Z","iopub.execute_input":"2023-01-11T09:54:34.555207Z","iopub.status.idle":"2023-01-11T09:54:34.564935Z","shell.execute_reply.started":"2023-01-11T09:54:34.555168Z","shell.execute_reply":"2023-01-11T09:54:34.564161Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"True     82240\nFalse    20558\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"val_df = train_df[~train_mask]\ntrain_df = train_df[train_mask]\nlen(val_df)","metadata":{"execution":{"iopub.status.busy":"2023-01-11T09:54:34.566068Z","iopub.execute_input":"2023-01-11T09:54:34.567485Z","iopub.status.idle":"2023-01-11T09:54:34.585002Z","shell.execute_reply.started":"2023-01-11T09:54:34.567447Z","shell.execute_reply":"2023-01-11T09:54:34.584347Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"20558"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model definition","metadata":{"id":"SICbZ6vB1mPd"}},{"cell_type":"markdown","source":"### Utilities","metadata":{"id":"6wuh0_ic1mPd"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom typing import List, Dict, Callable\nimport random","metadata":{"id":"4pKuTsYt1mPd","outputId":"28dcc870-e8ed-46b7-ddb0-97709087727d","execution":{"iopub.status.busy":"2023-01-11T09:54:34.586345Z","iopub.execute_input":"2023-01-11T09:54:34.586974Z","iopub.status.idle":"2023-01-11T09:54:40.243740Z","shell.execute_reply.started":"2023-01-11T09:54:34.586940Z","shell.execute_reply":"2023-01-11T09:54:40.242669Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def predict_data(model: keras.Model,\n                x: np.ndarray,\n                prediction_info: Dict):\n    \"\"\"\n    Inference routine of a given input set of examples\n\n    :param model: Keras built and possibly trained model\n    :param x: input set of examples in np.ndarray format\n    :param prediction_info: dictionary storing model predict() argument information\n\n    :return\n        predictions: predicted labels in np.ndarray format\n    \"\"\"\n    print(f'Starting prediction: \\n{prediction_info}')\n    print(f'Predicting on {x.shape[0]} samples')\n    predictions = model.predict(x, **prediction_info)\n    return predictions","metadata":{"id":"YutAeLih1mPe","execution":{"iopub.status.busy":"2023-01-11T09:54:40.245046Z","iopub.execute_input":"2023-01-11T09:54:40.245798Z","iopub.status.idle":"2023-01-11T09:54:40.252268Z","shell.execute_reply.started":"2023-01-11T09:54:40.245753Z","shell.execute_reply":"2023-01-11T09:54:40.250943Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def compute_f1(model: keras.Model, \n             x: np.ndarray, \n             y: np.ndarray):\n    \"\"\"\n    Compute F1_score on the given data with corresponding labels\n\n    :param model: Keras built and possibly trained model\n    :param x: data in np.ndarray format\n    :param y: ground-truth labels in np.ndarray format\n\n    :return\n        score: f1_macro_score\n    \"\"\"\n    #predictions on the x set\n    prediction_info = {\n        'batch_size': 64,\n        'verbose': 1\n    }\n    y_pred = predict_data(model=model, x=x, prediction_info=prediction_info)\n\n    #compute argmax to take the best class for each sample\n    y_pred = np.argmax(y_pred, axis=1)\n    #compute the f1_macro\n    score = f1_score(y, y_pred, average ='macro')\n    return score","metadata":{"id":"CS_dd2On1mPe","execution":{"iopub.status.busy":"2023-01-11T09:54:40.253536Z","iopub.execute_input":"2023-01-11T09:54:40.254440Z","iopub.status.idle":"2023-01-11T09:54:40.267332Z","shell.execute_reply.started":"2023-01-11T09:54:40.254403Z","shell.execute_reply":"2023-01-11T09:54:40.266447Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def set_reproducibility(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'","metadata":{"id":"1-akLLJo1mPe","execution":{"iopub.status.busy":"2023-01-11T09:54:40.270670Z","iopub.execute_input":"2023-01-11T09:54:40.270935Z","iopub.status.idle":"2023-01-11T09:54:40.278651Z","shell.execute_reply.started":"2023-01-11T09:54:40.270910Z","shell.execute_reply":"2023-01-11T09:54:40.277717Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tqdm import tqdm\nfrom transformers import TFAutoModel, AutoTokenizer\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"id":"61_GSxvB1mPe","outputId":"917fa56f-5eb2-49a2-dbd3-834f0b79c8db","execution":{"iopub.status.busy":"2023-01-11T09:54:40.280141Z","iopub.execute_input":"2023-01-11T09:54:40.281598Z","iopub.status.idle":"2023-01-11T09:54:42.110378Z","shell.execute_reply.started":"2023-01-11T09:54:40.281556Z","shell.execute_reply":"2023-01-11T09:54:42.109234Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Question generation $f_\\theta(P, Q)$ with text passage $P$ and question $Q$","metadata":{"id":"1aFTVbjH1mPf"}},{"cell_type":"markdown","source":"### Seq2Seq LSTM","metadata":{"id":"XxoRFA2j1mPf"}},{"cell_type":"code","source":"class MyTrainer(object):\n    \"\"\"\n    Simple wrapper class\n\n    train_op -> uses tf.GradientTape to compute the loss\n    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n    \"\"\"\n\n    def __init__(self, encoder, decoder, max_length):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.max_length = max_length\n        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n\n    @tf.function\n    def compute_loss(self, logits, target):\n        loss = self.ce(y_true=target, y_pred=logits)\n        mask = tf.logical_not(tf.math.equal(target, 0))\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    @tf.function\n    def train_op(self, inputs):\n        with tf.GradientTape() as tape:\n            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n                                                                 'hidden_state': inputs['encoder_state']})\n\n            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n\n            decoder.attention.setup_memory(encoder_output)\n\n            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n            predicted = self.decoder({'input_ids': decoder_input,\n                                      'initial_state': decoder_initial_state}).rnn_output\n\n            loss = self.compute_loss(logits=predicted, target=real_target)\n\n        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n        return loss, grads\n\n    @tf.function\n    def batch_fit(self, inputs):\n        loss, grads = self.train_op(inputs=inputs)\n        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n        return loss\n\n    @tf.function\n    def generate(self, input_ids):\n        batch_size = input_ids.shape[0]\n        encoder_initial_state = [tf.zeros((batch_size, self.encoder.encoder_units)),\n                                 tf.zeros((batch_size, self.encoder.encoder_units))]\n        encoder_output, encoder_h, encoder_s = self.encoder({\n            'input_ids': input_ids,\n            'hidden_state': encoder_initial_state\n        })\n\n        start_tokens = tf.fill([batch_size], tokenizer.word_index['<start>'])\n        end_token = tokenizer.word_index['<end>']\n\n        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n                                                    sampler=greedy_sampler,\n                                                    output_layer=self.decoder.generation_dense,\n                                                    maximum_iterations=self.max_length)\n        self.decoder.attention.setup_memory(encoder_output)\n\n        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n                                         start_tokens=start_tokens,\n                                         end_token=end_token,\n                                         initial_state=decoder_initial_state)\n        return outputs\n\n    def translate(self, generated):\n        return tokenizer.sequences_to_texts(generated.sample_id.numpy())\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, embedding_dim, encoder_units):\n        super(Encoder, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.encoder_units = encoder_units\n\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.encoder_lstm = tf.keras.layers.LSTM(self.encoder_units,\n                                                 return_sequences=True,\n                                                 return_state=True)\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        encoder_output, lstm_hidden, lstm_states = self.encoder_lstm(input_emb, initial_state=inputs['hidden_state'])\n        return encoder_output, lstm_hidden, lstm_states\n\n    def initialize(self, batch_size):\n        return [tf.zeros((batch_size, self.encoder_units)), tf.zeros((batch_size, self.encoder_units))]\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n\n        self.max_sequence_length = max_sequence_length\n        self.batch_size = batch_size\n\n        self.decoder_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n\n        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n                                                       memory=None,\n                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n\n        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n                                                                 self.attention,\n                                                                 attention_layer_size=self.decoder_units)\n\n        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n                                                sampler=self.sampler,\n                                                output_layer=self.generation_dense)\n\n    def build_initial_state(self, batch_size, encoder_state):\n        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n        return initial_state\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        decoder_output, _, _ = self.decoder(input_emb,\n                                            initial_state=inputs['initial_state'],\n                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n        return decoder_output","metadata":{"id":"OZecoZhu1mPf","execution":{"iopub.status.busy":"2023-01-11T09:54:42.114339Z","iopub.execute_input":"2023-01-11T09:54:42.114952Z","iopub.status.idle":"2023-01-11T09:54:42.143435Z","shell.execute_reply.started":"2023-01-11T09:54:42.114921Z","shell.execute_reply":"2023-01-11T09:54:42.142572Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Sample\ninput_sample = [\n    \"hello there how is it going\",\n    \"this assignment is hellish\"\n]\noutput_sample = [\n    \"<start> it is going well <end>\",\n    \"<start> I agree <end>\"\n]\n\nbatch_size = len(input_sample)\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\ntokenizer.fit_on_texts(input_sample + output_sample)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nencoded_input_sample = tokenizer.texts_to_sequences(input_sample)\nmax_input_length = max([len(item) for item in encoded_input_sample])\n\nencoded_output_sample = tokenizer.texts_to_sequences(output_sample)\nmax_output_length = max([len(item) for item in encoded_output_sample])\n\nmax_sequence_length = max(max_input_length, max_output_length)\n\nencoded_input_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_input_sample,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\nencoded_output_sample = tf.keras.preprocessing.sequence.pad_sequences(encoded_output_sample,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\n\n# Test encoder\nencoder = Encoder(vocab_size=vocab_size,\n                    embedding_dim=50,\n                    encoder_units=16)\n\nsample_hidden = encoder.initialize(batch_size=batch_size)\nencoder_sample_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_input_sample, dtype=tf.int32),\n    'hidden_state': sample_hidden\n}\n\nsample_output, sample_h, sample_c = encoder(inputs=encoder_sample_batch)\nprint(f'{sample_output.shape} -- {sample_h.shape} -- {sample_c.shape}')\n\n# Test decoder\ndecoder = Decoder(vocab_size=vocab_size,\n                    embedding_dim=50,\n                    decoder_units=16,\n                    batch_size=batch_size,\n                    max_sequence_length=max_sequence_length)\ndecoder.attention.setup_memory(sample_output)\ninitial_state = decoder.build_initial_state(batch_size, [sample_h, sample_c])\n\ndecoder_sample_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_output_sample, tf.int32),\n    'initial_state': initial_state\n}\nsample_decoder_outputs = decoder(decoder_sample_batch).rnn_output\nprint(f'{sample_decoder_outputs.shape}')","metadata":{"id":"t29CV-S41mPg","outputId":"b15fcf00-b73a-41ca-f7aa-954a058ad5d1","execution":{"iopub.status.busy":"2023-01-11T09:54:42.144907Z","iopub.execute_input":"2023-01-11T09:54:42.145503Z","iopub.status.idle":"2023-01-11T09:54:46.455336Z","shell.execute_reply.started":"2023-01-11T09:54:42.145465Z","shell.execute_reply":"2023-01-11T09:54:46.454075Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"(2, 6, 16) -- (2, 16) -- (2, 16)\n(2, 5, 16)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training\ntrainer = MyTrainer(encoder=encoder,\n                    decoder=decoder,\n                    max_length=max_sequence_length)","metadata":{"id":"SZhSq6V41mPg","execution":{"iopub.status.busy":"2023-01-11T09:54:46.456813Z","iopub.execute_input":"2023-01-11T09:54:46.457536Z","iopub.status.idle":"2023-01-11T09:54:46.462894Z","shell.execute_reply.started":"2023-01-11T09:54:46.457494Z","shell.execute_reply":"2023-01-11T09:54:46.461862Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nfor epoch in tqdm(range(epochs)):\n    encoder_hidden_state = encoder.initialize(batch_size=batch_size)\n    batch = {\n        'encoder_input_ids': encoded_input_sample,\n        'encoder_state': encoder_hidden_state,\n        'decoder_target': encoded_output_sample\n    }\n    loss = trainer.batch_fit(batch)\n    print(f'Loss - {loss}')\n\n    generated = trainer.generate(input_ids=encoded_input_sample)\n    translated = trainer.translate(generated)\n    print(f'Translated - {translated}')","metadata":{"id":"d5lLS5pc1mPg","outputId":"d8da62b6-89fe-47fe-ef85-061c004f53c7","execution":{"iopub.status.busy":"2023-01-11T09:54:46.464591Z","iopub.execute_input":"2023-01-11T09:54:46.465300Z","iopub.status.idle":"2023-01-11T09:54:59.049107Z","shell.execute_reply.started":"2023-01-11T09:54:46.465260Z","shell.execute_reply":"2023-01-11T09:54:59.048046Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"  0%|          | 0/100 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 2.220155715942383\n","output_type":"stream"},{"name":"stderr","text":"  5%|         | 5/100 [00:10<02:27,  1.55s/it]","output_type":"stream"},{"name":"stdout","text":"Translated - ['agree is going is going <end>', 'hello hello hello hello hello hello']\nLoss - 2.2144815921783447\nTranslated - ['is is going going well well', 'agree well well well well well']\nLoss - 2.208871364593506\nTranslated - ['is is going well well well', 'agree well well well well well']\nLoss - 2.203260898590088\nTranslated - ['is is going well well well', 'agree agree well well well well']\nLoss - 2.1975815296173096\nTranslated - ['is is going well well well', 'agree agree well well well well']\nLoss - 2.191772222518921\nTranslated - ['is is going well well well', 'agree <end> well well well well']\nLoss - 2.1857800483703613\nTranslated - ['well well well well well well', 'agree <end> <end> well well well']\nLoss - 2.1795573234558105\nTranslated - ['well well well well well well', 'agree <end> <end> well well <end>']\nLoss - 2.1730587482452393\n","output_type":"stream"},{"name":"stderr","text":" 14%|        | 14/100 [00:10<00:32,  2.69it/s]","output_type":"stream"},{"name":"stdout","text":"Translated - ['well well well well well well', 'agree <end> <end> <end> well <end>']\nLoss - 2.1662395000457764\nTranslated - ['well well well well well well', 'agree <end> <end> <end> well <end>']\nLoss - 2.15905499458313\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> well']\nLoss - 2.151458263397217\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.143401622772217\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.1348342895507812\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.125701904296875\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.1159465312957764\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.105506181716919\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":" 24%|       | 24/100 [00:10<00:11,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 2.0943121910095215\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.0822911262512207\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.0693631172180176\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.0554404258728027\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.0404295921325684\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.024230480194092\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 2.0067362785339355\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.9878374338150024\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.9674221277236938\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":" 34%|      | 34/100 [00:11<00:05, 12.50it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 1.9453814029693604\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.9216142892837524\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.8960365056991577\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.8685919046401978\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.8392646312713623\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.8080971240997314\nTranslated - ['well well well well well well', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.775205373764038\nTranslated - ['well well well well well <end>', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.740796685218811\nTranslated - ['it well well well well <end>', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.7051769495010376\n","output_type":"stream"},{"name":"stderr","text":" 44%|     | 44/100 [00:11<00:02, 20.47it/s]","output_type":"stream"},{"name":"stdout","text":"Translated - ['it well well well well <end>', 'agree <end> <end> <end> <end> <end>']\nLoss - 1.6687523126602173\nTranslated - ['it well well well <end>', 'agree <end> <end> <end> <end>']\nLoss - 1.6320117712020874\nTranslated - ['it well well well <end>', 'agree <end> <end> <end> <end>']\nLoss - 1.5954949855804443\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.5597426891326904\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.5252338647842407\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.4923290014266968\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.4612271785736084\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.431951880455017\nTranslated - ['it <end>', 'agree <end>']\nLoss - 1.4043571949005127\nTranslated - ['it <end>', 'agree <end>']\n","output_type":"stream"},{"name":"stderr","text":" 54%|    | 54/100 [00:11<00:01, 29.18it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 1.3781652450561523\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.353021264076233\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.3285577297210693\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.3044493198394775\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.280451774597168\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.2564260959625244\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.2323439121246338\nTranslated - ['agree <end>', 'agree <end>']\nLoss - 1.208270788192749\nTranslated - ['agree <end> <end>', 'agree agree <end>']\nLoss - 1.1843342781066895\nTranslated - ['agree <end> <end>', 'agree agree <end>']\nLoss - 1.1606756448745728\nTranslated - ['agree <end> <end>', 'agree agree <end>']\n","output_type":"stream"},{"name":"stderr","text":" 64%|   | 64/100 [00:11<00:01, 35.87it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 1.1373993158340454\nTranslated - ['agree is <end>', 'agree agree <end>']\nLoss - 1.1145389080047607\nTranslated - ['agree is <end>', 'agree agree <end>']\nLoss - 1.0920475721359253\nTranslated - ['agree is <end>', 'agree agree <end>']\nLoss - 1.069818139076233\nTranslated - ['agree is <end>', 'agree agree <end>']\nLoss - 1.0477182865142822\nTranslated - ['agree is <end>', 'agree agree <end>']\nLoss - 1.0256311893463135\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 1.003488540649414\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.9812868237495422\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.9590826034545898\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.936976432800293\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":" 69%|   | 69/100 [00:11<00:00, 38.07it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.9150853157043457\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.8935174942016602\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.8723518252372742\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.8516286611557007\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.831351101398468\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.8114961385726929\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.792035698890686\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.7729558944702148\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.7542745471000671\nTranslated - ['it is is <end>', 'agree agree <end> <end>']\nLoss - 0.736041247844696\n","output_type":"stream"},{"name":"stderr","text":" 79%|  | 79/100 [00:12<00:00, 41.22it/s]","output_type":"stream"},{"name":"stdout","text":"Translated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.7183204889297485\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.7011578679084778\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6845463514328003\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6684153079986572\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6526552438735962\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6371706128120422\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6219223737716675\nTranslated - ['it is is well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.6069365739822388\nTranslated - ['it is going well <end>', 'agree agree <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":" 89%| | 89/100 [00:12<00:00, 40.86it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.5922739505767822\nTranslated - ['it is going well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.577989399433136\nTranslated - ['it is going well <end>', 'agree agree <end> <end> <end>']\nLoss - 0.564110279083252\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.5506466031074524\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.5376087427139282\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.5250113606452942\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.5128495097160339\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.5010802745819092\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.489637553691864\n","output_type":"stream"},{"name":"stderr","text":" 99%|| 99/100 [00:12<00:00, 40.46it/s]","output_type":"stream"},{"name":"stdout","text":"Translated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.47847262024879456\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4675752520561218\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4569552540779114\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4466119706630707\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4365282952785492\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4266854226589203\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.41707736253738403\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\nLoss - 0.4077049791812897\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":"100%|| 100/100 [00:12<00:00,  7.96it/s]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.39856186509132385\nTranslated - ['it is going well <end>', 'i agree <end> <end> <end>']\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#TODO","metadata":{"id":"Akd5eb5N1mPh","execution":{"iopub.status.busy":"2023-01-11T09:54:59.050704Z","iopub.execute_input":"2023-01-11T09:54:59.051426Z","iopub.status.idle":"2023-01-11T09:54:59.056243Z","shell.execute_reply.started":"2023-01-11T09:54:59.051387Z","shell.execute_reply":"2023-01-11T09:54:59.054792Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Seq2Seq Bert-Tiny","metadata":{"id":"Ia7JgdiF1mPh"}},{"cell_type":"code","source":"class MyTrainer(object):\n    \"\"\"\n    Simple wrapper class\n\n    train_op -> uses tf.GradientTape to compute the loss\n    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n    \"\"\"\n\n    def __init__(self, encoder, decoder, max_length):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.max_length = max_length\n        self.ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-03)\n\n    @tf.function\n    def compute_loss(self, logits, target):\n        loss = self.ce(y_true=target, y_pred=logits)\n        mask = tf.logical_not(tf.math.equal(target, 0))\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_mean(loss)\n\n    @tf.function\n    def train_op(self, inputs):\n        with tf.GradientTape() as tape:\n            encoder_output, encoder_h, encoder_s = self.encoder({'input_ids': inputs['encoder_input_ids'],\n                                                                 'attention_mask': inputs['encoder_attention_mask']})\n\n            decoder_input = inputs['decoder_target'][:, :-1]  # ignore <end>\n            real_target = inputs['decoder_target'][:, 1:]  # ignore <start>\n\n            decoder.attention.setup_memory(encoder_output)\n\n            decoder_initial_state = self.decoder.build_initial_state(decoder.batch_size, [encoder_h, encoder_s])\n            predicted = self.decoder({'input_ids': decoder_input,\n                                      'initial_state': decoder_initial_state}).rnn_output\n\n            loss = self.compute_loss(logits=predicted, target=real_target)\n\n        grads = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n        return loss, grads\n\n    @tf.function\n    def batch_fit(self, inputs):\n        loss, grads = self.train_op(inputs=inputs)\n        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables + self.decoder.trainable_variables))\n        return loss\n\n    # @tf.function\n    def generate(self, input_ids, attention_mask=None):\n        batch_size = input_ids.shape[0]\n        encoder_output, encoder_h, encoder_s = self.encoder({\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        })\n\n        start_tokens = tf.fill([batch_size], output_tokenizer.word_index['<start>'])\n        end_token = output_tokenizer.word_index['<end>']\n\n        greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n        decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.wrapped_decoder_cell,\n                                                    sampler=greedy_sampler,\n                                                    output_layer=self.decoder.generation_dense,\n                                                    maximum_iterations=self.max_length)\n        self.decoder.attention.setup_memory(encoder_output)\n\n        decoder_initial_state = self.decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n        decoder_embedding_matrix = self.decoder.embedding.variables[0]\n        outputs, _, _ = decoder_instance(decoder_embedding_matrix,\n                                         start_tokens=start_tokens,\n                                         end_token=end_token,\n                                         initial_state=decoder_initial_state)\n        return outputs\n\n    def translate(self, generated):\n        return output_tokenizer.sequences_to_texts(generated.sample_id.numpy())\n\n\nclass Encoder(tf.keras.Model):\n\n    def __init__(self, model_name, decoder_units):\n        super(Encoder, self).__init__()\n        self.model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n        self.reducer = tf.keras.layers.Dense(decoder_units)\n\n    def call(self, inputs, training=False, **kwargs):\n        model_output = self.model(inputs)\n        all_outputs = model_output[0]\n        pooled_output = model_output[1]\n        pooled_output = self.reducer(pooled_output)\n        return all_outputs, pooled_output, pooled_output\n\n\nclass Decoder(tf.keras.Model):\n\n    def __init__(self, vocab_size, max_sequence_length, embedding_dim, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n\n        self.max_sequence_length = max_sequence_length\n        self.batch_size = batch_size\n\n        self.decoder_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n                                                   output_dim=embedding_dim)\n        self.decoder_lstm_cell = tf.keras.layers.LSTMCell(self.decoder_units)\n\n        self.attention = tfa.seq2seq.BahdanauAttention(units=self.decoder_units,\n                                                       memory=None,\n                                                       memory_sequence_length=self.batch_size * [max_sequence_length])\n\n        self.wrapped_decoder_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm_cell,\n                                                                 self.attention,\n                                                                 attention_layer_size=self.decoder_units)\n\n        self.generation_dense = tf.keras.layers.Dense(vocab_size)\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        self.decoder = tfa.seq2seq.BasicDecoder(self.wrapped_decoder_cell,\n                                                sampler=self.sampler,\n                                                output_layer=self.generation_dense)\n\n    def build_initial_state(self, batch_size, encoder_state):\n        initial_state = self.wrapped_decoder_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n        return initial_state\n\n    def call(self, inputs, training=False, **kwargs):\n        input_ids = inputs['input_ids']\n        input_emb = self.embedding(input_ids)\n        decoder_output, _, _ = self.decoder(input_emb,\n                                            initial_state=inputs['initial_state'],\n                                            sequence_length=self.batch_size * [self.max_sequence_length - 1])\n        return decoder_output\n","metadata":{"id":"tIz7Z9QV1mPh","execution":{"iopub.status.busy":"2023-01-11T09:54:59.058083Z","iopub.execute_input":"2023-01-11T09:54:59.058889Z","iopub.status.idle":"2023-01-11T09:54:59.086261Z","shell.execute_reply.started":"2023-01-11T09:54:59.058850Z","shell.execute_reply":"2023-01-11T09:54:59.085236Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, AutoTokenizer, AutoConfig\n\nmodel_name = 'prajjwal1/bert-tiny'\n\n#config = AutoConfig.from_pretrained(model_name)\n#model = BertForQuestionAnswering.from_pretrained(model_name, config=config)\ninput_tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"vV1rlQqR1mPi","execution":{"iopub.status.busy":"2023-01-11T09:54:59.087893Z","iopub.execute_input":"2023-01-11T09:54:59.088663Z","iopub.status.idle":"2023-01-11T09:55:03.613938Z","shell.execute_reply.started":"2023-01-11T09:54:59.088626Z","shell.execute_reply":"2023-01-11T09:55:03.612676Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"The next block of code is an example of encoding of a question-context pair: in this case, the question is the first part of the encoding, and the context is the second part. There are two special tokens: [CLS] token at the start of the encoding, [SEP] token between the question and the context, and at the end of the encoding.\n\nIn this case the context is the *span*, to provide a better example that explains the encoding.","metadata":{"id":"kswqcr_m1mPi"}},{"cell_type":"code","source":"line = 42\n\nencoded_question = input_tokenizer(train_df['q'][line], return_tensors='tf', padding=True)\nprint(train_df['q'][line])\n\nencoded_span = input_tokenizer(train_df['span'][line], return_tensors='tf', padding=True)\nprint(train_df['span'][line])\n\nencoded_qs = input_tokenizer(train_df['q'][line], train_df['span'][line], return_tensors='tf', padding=True)\n\nprint('= '*40)\nfor idx, tok in zip(encoded_qs.input_ids.numpy()[0], input_tokenizer.convert_ids_to_tokens(encoded_qs.input_ids[0])):\n    print(\"{}\\t{}\".format(idx, tok))","metadata":{"id":"gxqLjPlL1mPi","outputId":"8b7d0cea-e17f-44ac-abb6-290888dc468a","execution":{"iopub.status.busy":"2023-01-11T09:55:03.615817Z","iopub.execute_input":"2023-01-11T09:55:03.616838Z","iopub.status.idle":"2023-01-11T09:55:03.645574Z","shell.execute_reply.started":"2023-01-11T09:55:03.616792Z","shell.execute_reply":"2023-01-11T09:55:03.644436Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Was Lassiter impressed with the horse?\nWhen Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes.\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n101\t[CLS]\n2001\twas\n27333\tlass\n21646\t##iter\n7622\timpressed\n2007\twith\n1996\tthe\n3586\thorse\n1029\t?\n102\t[SEP]\n2043\twhen\n15333\tje\n4103\t##rd\n2419\tled\n2041\tout\n2023\tthis\n10944\tslender\n1010\t,\n17950\tbeautifully\n2328\tbuilt\n3586\thorse\n27333\tlass\n21646\t##iter\n3402\tsuddenly\n2150\tbecame\n2035\tall\n2159\teyes\n1012\t.\n102\t[SEP]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Lets encode a part of the dataset in sentences of: [CLS] question [SEP] passage [SEP]. Otherwise, the training would be very slow.","metadata":{"id":"UYZk4xQk1mPi"}},{"cell_type":"code","source":"max_length = 512  # The maximum length of a feature (question and context)\ndoc_stride = (\n    128  # The authorized overlap between two part of the context when splitting\n)\nsentences = 20\nsample = 10","metadata":{"id":"xBycfrq51mPj","execution":{"iopub.status.busy":"2023-01-11T09:55:03.647279Z","iopub.execute_input":"2023-01-11T09:55:03.647964Z","iopub.status.idle":"2023-01-11T09:55:03.653433Z","shell.execute_reply.started":"2023-01-11T09:55:03.647925Z","shell.execute_reply":"2023-01-11T09:55:03.652389Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Input\nqs = train_df['q'][range(sentences)] # questions\ncs = train_df['p'][range(sentences)] # contexts\n\nbatch_size = len(qs)\n\nencoded_inputs = input_tokenizer(\n    qs.values.tolist(),\n    cs.values.tolist(),\n    #train_df['q'].values.tolist(),\n    #train_df['p'].values.tolist(),\n    truncation=\"only_second\",\n    max_length=max_length,\n    stride=doc_stride,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n    padding=\"max_length\",\n    return_tensors='tf'\n)\n\ninput_ids, attention_mask = encoded_inputs.input_ids, encoded_inputs.attention_mask\nmax_input_length = input_ids.shape[-1]","metadata":{"id":"ZhjywqdM1mPj","execution":{"iopub.status.busy":"2023-01-11T09:55:03.654997Z","iopub.execute_input":"2023-01-11T09:55:03.655585Z","iopub.status.idle":"2023-01-11T09:55:03.685777Z","shell.execute_reply.started":"2023-01-11T09:55:03.655547Z","shell.execute_reply":"2023-01-11T09:55:03.684499Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(\"max_input_length:\", max_input_length)\nprint(\"encoded_inputs shape =\", encoded_inputs['input_ids'].shape)","metadata":{"id":"emIgycXc1mPk","outputId":"c171e250-6167-456b-deb7-761b923cbcce","execution":{"iopub.status.busy":"2023-01-11T09:55:03.687224Z","iopub.execute_input":"2023-01-11T09:55:03.687884Z","iopub.status.idle":"2023-01-11T09:55:03.694891Z","shell.execute_reply.started":"2023-01-11T09:55:03.687834Z","shell.execute_reply":"2023-01-11T09:55:03.693766Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"max_input_length: 512\nencoded_inputs shape = (20, 512)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The 'token_type_ids' encodes wether the encoded id is part of the question (=0) or the context (=1). The Attention Mask indicates if the input is needed (=1) or it's padding (=0).","metadata":{"id":"1cpxq9qK1mPl"}},{"cell_type":"markdown","source":"Prepare also the expected outputs, for the training (this code follows the example given by the tutors, but I'm not convinced that this is the proper formatting for a QA Bert model).","metadata":{"id":"sIGwPsUg1mPm"}},{"cell_type":"code","source":"# Output\noutputs = \"<start> \" + train_df['a'][range(sentences)] + \" <end>\"\n\noutput_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\noutput_tokenizer.fit_on_texts(outputs)\n\noutput_vocab_size = len(output_tokenizer.word_index) + 1\n\nencoded_output = output_tokenizer.texts_to_sequences(outputs)\nprint(encoded_output[sample])\nmax_output_length = max([len(item) for item in encoded_output])","metadata":{"id":"APmOG4gv1mPm","outputId":"77cb34f3-579d-4a83-d2d7-065f8cb61331","execution":{"iopub.status.busy":"2023-01-11T09:55:03.696473Z","iopub.execute_input":"2023-01-11T09:55:03.697124Z","iopub.status.idle":"2023-01-11T09:55:03.707810Z","shell.execute_reply.started":"2023-01-11T09:55:03.697085Z","shell.execute_reply":"2023-01-11T09:55:03.706830Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"[2, 30, 5, 31, 10, 5, 32, 33, 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"max_sequence_length = max(max_input_length, max_output_length)\n\nprint(\"max_output_length: {}\".format(max_output_length))\nprint(\"max_sequence_length: {}\".format(max_sequence_length))","metadata":{"id":"t4wdmkDs1mPn","outputId":"17d82207-b850-4b54-f679-e886186ee5c3","execution":{"iopub.status.busy":"2023-01-11T09:55:03.709297Z","iopub.execute_input":"2023-01-11T09:55:03.710066Z","iopub.status.idle":"2023-01-11T09:55:03.718970Z","shell.execute_reply.started":"2023-01-11T09:55:03.710029Z","shell.execute_reply":"2023-01-11T09:55:03.717925Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"max_output_length: 11\nmax_sequence_length: 512\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_output = tf.keras.preprocessing.sequence.pad_sequences(encoded_output,\n                                                                        padding='post',\n                                                                        maxlen=max_sequence_length)\nprint(encoded_output[sample])","metadata":{"id":"cNunTdpj1mPn","outputId":"341ad36a-a211-4744-818d-8c4785f05a5f","execution":{"iopub.status.busy":"2023-01-11T09:55:03.720911Z","iopub.execute_input":"2023-01-11T09:55:03.721218Z","iopub.status.idle":"2023-01-11T09:55:03.731626Z","shell.execute_reply.started":"2023-01-11T09:55:03.721192Z","shell.execute_reply":"2023-01-11T09:55:03.730261Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"[ 2 30  5 31 10  5 32 33  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test encoder\nencoder = Encoder(model_name=model_name,\n                    decoder_units=16)\nencoder_output, encoder_h, encoder_s = encoder({'input_ids': input_ids,\n                                                'attention_mask': attention_mask})\nprint(f'{encoder_output.shape} - {encoder_h.shape} - {encoder_s.shape}')","metadata":{"id":"-G1IKYO61mPo","outputId":"51413bbc-b166-4320-8cbb-11d6ef001804","execution":{"iopub.status.busy":"2023-01-11T09:55:03.733482Z","iopub.execute_input":"2023-01-11T09:55:03.734096Z","iopub.status.idle":"2023-01-11T09:55:07.030133Z","shell.execute_reply.started":"2023-01-11T09:55:03.734059Z","shell.execute_reply":"2023-01-11T09:55:07.029073Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'bert.embeddings.position_ids', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"(20, 512, 128) - (20, 16) - (20, 16)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test decoder\ndecoder = Decoder(vocab_size=output_vocab_size,\n                    embedding_dim=50,\n                    decoder_units=16,\n                    batch_size=batch_size,\n                    max_sequence_length=max_sequence_length)\ndecoder.attention.setup_memory(encoder_output)\ninitial_state = decoder.build_initial_state(batch_size, [encoder_h, encoder_s])\n\ndecoder_batch = {\n    'input_ids': tf.convert_to_tensor(encoded_output, tf.int32),\n    'initial_state': initial_state\n}\ndecoder_outputs = decoder(decoder_batch).rnn_output\nprint(f'{decoder_outputs.shape}')","metadata":{"id":"zkKbWjm11mPo","outputId":"b0e3ad8e-c144-4efd-a827-7c97d69e1f85","execution":{"iopub.status.busy":"2023-01-11T09:55:07.031448Z","iopub.execute_input":"2023-01-11T09:55:07.031828Z","iopub.status.idle":"2023-01-11T09:55:09.163214Z","shell.execute_reply.started":"2023-01-11T09:55:07.031775Z","shell.execute_reply":"2023-01-11T09:55:09.162076Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"(20, 511, 63)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training\ntrainer = MyTrainer(encoder=encoder,\n                    decoder=decoder,\n                    max_length=max_sequence_length)","metadata":{"id":"RueCJjXO1mPp","execution":{"iopub.status.busy":"2023-01-11T09:55:09.164844Z","iopub.execute_input":"2023-01-11T09:55:09.168098Z","iopub.status.idle":"2023-01-11T09:55:09.255458Z","shell.execute_reply.started":"2023-01-11T09:55:09.168066Z","shell.execute_reply":"2023-01-11T09:55:09.254303Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"epochs = 3\nfor epoch in tqdm(range(epochs)):\n    batch = {\n        'encoder_input_ids': input_ids,\n        'encoder_attention_mask': attention_mask,\n        'decoder_target': encoded_output\n    }\n    loss = trainer.batch_fit(batch)\n    print(f'Loss - {loss}')\n\n    generated = trainer.generate(input_ids=input_ids,\n                                    attention_mask=attention_mask)\n    translated = trainer.translate(generated)\n    print(f'Translated - {translated}')","metadata":{"id":"l5H1c5ZN1mPp","outputId":"d56bee8b-043a-4526-e4d9-efca762bd374","execution":{"iopub.status.busy":"2023-01-11T09:55:09.271557Z","iopub.execute_input":"2023-01-11T09:55:09.272122Z","iopub.status.idle":"2023-01-11T09:55:19.840621Z","shell.execute_reply.started":"2023-01-11T09:55:09.272070Z","shell.execute_reply":"2023-01-11T09:55:19.839423Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":" 33%|      | 1/3 [00:08<00:16,  8.26s/it]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.03878410905599594\nTranslated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n","output_type":"stream"},{"name":"stderr","text":" 67%|   | 2/3 [00:09<00:04,  4.08s/it]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.03520103543996811\nTranslated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n","output_type":"stream"},{"name":"stderr","text":"100%|| 3/3 [00:10<00:00,  3.52s/it]","output_type":"stream"},{"name":"stdout","text":"Loss - 0.03424525633454323\nTranslated - ['<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"An example of answered question by the pretrained (*original*) model.","metadata":{"id":"P3vOKrD11mPp"}},{"cell_type":"code","source":"from transformers import TFBertForQuestionAnswering, pipeline\n\nmodel = TFBertForQuestionAnswering.from_pretrained(model_name, from_pt=True)\n\nquestion_answerer = pipeline(\"question-answering\", model=model_name)\n\noutputs = question_answerer(question=train_df['q'][0], context=train_df['p'][0])\n\nprint(\"model outputs:\", outputs)\nprint()\nprint(\"official results are (from train.json):\") \nprint(\"span_start: 151\")\nprint(\"span_end: 179\")\nprint(\"span_text: Formally established in 1475\")\nprint(\"input_text: It was formally established in 1475\")\n#print(\"start scores: {}\".format(start_scores))\n#print(\"end scores: {}\".format(end_scores))","metadata":{"id":"09T-nztL1mPp","outputId":"a3e7b98e-9407-4984-f02a-b40253b9f6a0","execution":{"iopub.status.busy":"2023-01-11T09:55:19.842118Z","iopub.execute_input":"2023-01-11T09:55:19.842848Z","iopub.status.idle":"2023-01-11T09:55:22.053756Z","shell.execute_reply.started":"2023-01-11T09:55:19.842808Z","shell.execute_reply":"2023-01-11T09:55:22.051717Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"model outputs: {'score': 3.4231896279379725e-05, 'start': 810, 'end': 849, 'answer': 'initial four-year project of digitising'}\n\nofficial results are (from train.json):\nspan_start: 151\nspan_end: 179\nspan_text: Formally established in 1475\ninput_text: It was formally established in 1475\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### BERT2BERT Bert-Tiny","metadata":{"id":"pAG9J3GGfF76"}},{"cell_type":"code","source":"#entire dataset\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])","metadata":{"id":"bBtg7iiwp6DJ","execution":{"iopub.status.busy":"2023-01-11T09:55:22.055554Z","iopub.execute_input":"2023-01-11T09:55:22.055977Z","iopub.status.idle":"2023-01-11T09:55:22.090747Z","shell.execute_reply.started":"2023-01-11T09:55:22.055936Z","shell.execute_reply":"2023-01-11T09:55:22.089695Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"#take a subset from the training set\nstart = 0\nend = 95000\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])\ncontexts = contexts[start:end]\nquestions = questions[start:end]\nanswers = answers[start:end]\nlen(contexts)","metadata":{"id":"oL3BVkcG5KZ8","outputId":"5dd2bf70-2ed5-42a0-b0d4-25b3c43d4561","execution":{"iopub.status.busy":"2023-01-11T09:55:22.093505Z","iopub.execute_input":"2023-01-11T09:55:22.094296Z","iopub.status.idle":"2023-01-11T09:55:22.126165Z","shell.execute_reply.started":"2023-01-11T09:55:22.094257Z","shell.execute_reply":"2023-01-11T09:55:22.125029Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"82240"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel, AutoTokenizer\nfrom tqdm import tqdm\n\n\nmodel_name = 'prajjwal1/bert-tiny'\n\n# tie_encoder_decoder to share weights and half the number of parameters\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name,\n                                                                        #encoder_from_pt=True,\n                                                                        #decoder_from_pt=True,\n                                                                        tie_encoder_decoder=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# set special tokens\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# set decoding params                               \nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.repetition_penalty = 5.0\nmodel.config.num_beams = 2\nmodel.config.vocab_size = model.config.encoder.vocab_size\n","metadata":{"outputId":"a75cb503-137a-4b43-8cf0-0567a309fe71","id":"25WjYqJRfsZu","execution":{"iopub.status.busy":"2023-01-11T09:55:22.127899Z","iopub.execute_input":"2023-01-11T09:55:22.128295Z","iopub.status.idle":"2023-01-11T09:55:25.233427Z","shell.execute_reply.started":"2023-01-11T09:55:22.128257Z","shell.execute_reply":"2023-01-11T09:55:25.232330Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe following encoder weights were not tied to the decoder ['bert/pooler']\n","output_type":"stream"}]},{"cell_type":"code","source":"encodings = tokenizer(questions, contexts, \n                          padding=True,\n                          truncation= 'only_second',\n                          max_length = 499,\n                          )\ninput_ids, input_attention_mask = encodings['input_ids'], encodings['attention_mask']\nlabel_values = tokenizer(answers,\n                          padding=True,\n                          truncation=True,\n                          max_length = 25,\n                          )\nlabels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n\n\n\n#Tokens with indices set to ``-100`` are ignored (masked) during training, the loss is only computed for the tokens with labels\nmasked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\nprint(f'length of input_ids: {np.shape(input_ids)}')","metadata":{"id":"_J9fZwmgqJZo","outputId":"d6b584f6-209c-408a-8a83-e13a17756085","execution":{"iopub.status.busy":"2023-01-11T09:55:25.235129Z","iopub.execute_input":"2023-01-11T09:55:25.235554Z","iopub.status.idle":"2023-01-11T09:56:48.507164Z","shell.execute_reply.started":"2023-01-11T09:55:25.235514Z","shell.execute_reply":"2023-01-11T09:56:48.506023Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"length of input_ids: (82240, 499)\n","output_type":"stream"}]},{"cell_type":"code","source":"encodings.keys()","metadata":{"outputId":"f0919455-47eb-47f3-cf13-9a9b27449329","id":"TXROfeJ0fsZu","execution":{"iopub.status.busy":"2023-01-11T09:56:48.508722Z","iopub.execute_input":"2023-01-11T09:56:48.509345Z","iopub.status.idle":"2023-01-11T09:56:48.517430Z","shell.execute_reply.started":"2023-01-11T09:56:48.509303Z","shell.execute_reply":"2023-01-11T09:56:48.516402Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"},"metadata":{}}]},{"cell_type":"code","source":"encodings.pop('token_type_ids')\nencodings.update({#'decoder_input_ids': labels,\n                 #'decoder_attention_mask': labels_mask,\n                 'labels': masked_labels\n                 })\nencodings.keys()","metadata":{"outputId":"15a092d1-a926-4d0a-9dcc-6c0350013947","id":"AEAZjiqdfsZv","execution":{"iopub.status.busy":"2023-01-11T09:56:48.519272Z","iopub.execute_input":"2023-01-11T09:56:48.520077Z","iopub.status.idle":"2023-01-11T09:56:48.702435Z","shell.execute_reply.started":"2023-01-11T09:56:48.520039Z","shell.execute_reply":"2023-01-11T09:56:48.701230Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask', 'labels'])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomTextDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)","metadata":{"id":"6zawXNuzfsZv","execution":{"iopub.status.busy":"2023-01-11T09:56:48.706228Z","iopub.execute_input":"2023-01-11T09:56:48.706567Z","iopub.status.idle":"2023-01-11T09:56:48.714233Z","shell.execute_reply.started":"2023-01-11T09:56:48.706539Z","shell.execute_reply":"2023-01-11T09:56:48.713084Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#parameters\nbatch_size = 16\nnum_epochs = 3\nlr = 4e-4","metadata":{"id":"Jqtjk7qM3r1r","execution":{"iopub.status.busy":"2023-01-11T09:56:48.715827Z","iopub.execute_input":"2023-01-11T09:56:48.716222Z","iopub.status.idle":"2023-01-11T09:56:48.727712Z","shell.execute_reply.started":"2023-01-11T09:56:48.716183Z","shell.execute_reply":"2023-01-11T09:56:48.726518Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer\n#create training dataset\ntrain_dataset = CustomTextDataset(encodings)\n#create training dataloader\ntrain_ld = torch.utils.data.DataLoader(train_dataset,\n                                     batch_size=batch_size,\n                                     )\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\noptim = torch.optim.AdamW(model.parameters(), lr=lr)\nloop_start = timer()\nfor epoch in range(num_epochs):\n    model.train()\n    loss_score = []\n    loop = tqdm(train_ld)\n    for batch in loop:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        loss, outputs = model(input_ids,\n                              attention_mask=attention_mask,\n                              labels = labels\n                        )[:2]\n        loss_score.append(loss.item())\n        #loss = outputs[0]\n        loss.backward()\n        optim.step()\n\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n    average_loss = np.mean(loss_score)\n    print(f\"\\nEpoch: {epoch}, average Loss: {average_loss}\")\nloop_end = timer()\ntime_loop = loop_end - loop_start\nprint(f'\\nTime for {num_epochs} epochs (s): {(time_loop):.3f}')","metadata":{"outputId":"f5d4f349-9488-4833-80d4-f76df98efda9","id":"AvaNFc8WfsZv","execution":{"iopub.status.busy":"2023-01-11T09:56:48.729233Z","iopub.execute_input":"2023-01-11T09:56:48.729988Z","iopub.status.idle":"2023-01-11T10:06:03.251530Z","shell.execute_reply.started":"2023-01-11T09:56:48.729923Z","shell.execute_reply":"2023-01-11T10:06:03.250440Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"  0%|          | 0/5140 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\nEpoch 0: 100%|| 5140/5140 [03:05<00:00, 27.70it/s, loss=3.06]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 0, average Loss: 4.06643428584481\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|| 5140/5140 [03:04<00:00, 27.90it/s, loss=2.64]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 1, average Loss: 3.3645927429663067\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|| 5140/5140 [03:04<00:00, 27.83it/s, loss=2.21]","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 2, average Loss: 3.071040308707419\n\nTime for 3 epochs (s): 554.492\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"#Free some memory\nimport gc\ndel encodings,input_ids,input_attention_mask,labels\ntorch.cuda.empty_cache()\ntorch.cuda.reset_accumulated_memory_stats()\ngc.collect()","metadata":{"id":"PS4LyEvk1y88","outputId":"4224b9df-5f72-419d-b753-7e415545734f","execution":{"iopub.status.busy":"2023-01-11T10:06:03.253224Z","iopub.execute_input":"2023-01-11T10:06:03.253884Z","iopub.status.idle":"2023-01-11T10:06:04.534916Z","shell.execute_reply.started":"2023-01-11T10:06:03.253842Z","shell.execute_reply":"2023-01-11T10:06:04.530511Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Generation","metadata":{"id":"1ezjXyEag7rc"}},{"cell_type":"markdown","source":"Load test dataset.","metadata":{"id":"ZpudKS7ZhCN6"}},{"cell_type":"code","source":"test_df = loadDataset(\"test.json\")\ntest_df.count()","metadata":{"id":"KdxS-LfwuUYc","outputId":"3c241dad-a623-4853-c8b4-e6fb294a7cb2","execution":{"iopub.status.busy":"2023-01-11T10:06:04.536726Z","iopub.execute_input":"2023-01-11T10:06:04.542833Z","iopub.status.idle":"2023-01-11T10:06:05.207468Z","shell.execute_reply.started":"2023-01-11T10:06:04.542778Z","shell.execute_reply":"2023-01-11T10:06:05.206574Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"500 stories / 12 questions in the first row\n499 distinct stories\n5 distinct sources: Index(['mctest', 'race', 'cnn', 'wikipedia', 'gutenberg'], dtype='object')\n(7917, 5) question-answer pairs x columns\nFirst row: ['0' '0' 'What color was Cotton?' 'white'\n 'a little white kitten named Cotton']\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"source    7917\np         7917\nq         7917\na         7917\nspan      7917\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"context_test = list(test_df['p'])\nquestion_test = list(test_df['q'])\nanswer_test = list(test_df['a'])","metadata":{"id":"N4oPk8gWl7Ob","execution":{"iopub.status.busy":"2023-01-11T10:06:05.224203Z","iopub.execute_input":"2023-01-11T10:06:05.225114Z","iopub.status.idle":"2023-01-11T10:06:05.238335Z","shell.execute_reply.started":"2023-01-11T10:06:05.225074Z","shell.execute_reply":"2023-01-11T10:06:05.237105Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"input_values = tokenizer(question_test,context_test, padding=True, truncation=True, max_length = 499)\ninput_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']","metadata":{"id":"XeQC8zqgghs6","execution":{"iopub.status.busy":"2023-01-11T10:06:05.239754Z","iopub.execute_input":"2023-01-11T10:06:05.240424Z","iopub.status.idle":"2023-01-11T10:06:11.318441Z","shell.execute_reply.started":"2023-01-11T10:06:05.240386Z","shell.execute_reply":"2023-01-11T10:06:11.317423Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nl = []\nmodel.to(device)\nmodel.eval()\nfor input, mask in zip(input_ids,input_attention_mask):\n  input = np.expand_dims(np.array(input), axis=0)\n  mask = np.expand_dims(np.array(mask), axis=0)\n  generated = model.generate(input_ids=torch.tensor(input).to(device),\n                             #attention_mask=torch.tensor(mask).to(device), \n                                                 max_length=20,\n                                                 repetition_penalty=5.,\n                                                 min_length=1,\n                                                 no_repeat_ngram_size=3,\n                                                 early_stopping=True,\n                                                decoder_start_token_id = model.config.decoder_start_token_id,\n                                                 num_beams=2,\n                                                 )\n  generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n  l.append(generated)","metadata":{"id":"Af_Oia8-GeN0","execution":{"iopub.status.busy":"2023-01-11T10:06:11.320007Z","iopub.execute_input":"2023-01-11T10:06:11.320418Z","iopub.status.idle":"2023-01-11T10:09:10.497565Z","shell.execute_reply.started":"2023-01-11T10:06:11.320378Z","shell.execute_reply":"2023-01-11T10:09:10.496353Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"x = pd.DataFrame(l, columns = ['generated'])\nx['questions'] = question_test\nx['answers'] = answer_test\n#pd.set_option('display.max_rows', None)\n#x.head(300)","metadata":{"id":"gWei4De6ghs6","execution":{"iopub.status.busy":"2023-01-11T10:09:10.499240Z","iopub.execute_input":"2023-01-11T10:09:10.499955Z","iopub.status.idle":"2023-01-11T10:09:10.512754Z","shell.execute_reply.started":"2023-01-11T10:09:10.499911Z","shell.execute_reply":"2023-01-11T10:09:10.511430Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"#utility functions taken from the allennlp library for computing the F1-score\nimport collections\nimport re\nimport string\nfrom typing import Callable, Sequence, TypeVar, Tuple\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(a_pred: str, a_gold: str) -> float:\n    pred_toks = get_tokens(a_pred)\n    gold_toks = get_tokens(a_gold)\n    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n    num_same = sum(common.values())\n    if len(pred_toks) == 0 or len(gold_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return float(pred_toks == gold_toks)\n    if num_same == 0:\n        return 0.0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"id":"svvWfoUD89sR","execution":{"iopub.status.busy":"2023-01-11T10:09:10.514980Z","iopub.execute_input":"2023-01-11T10:09:10.515428Z","iopub.status.idle":"2023-01-11T10:09:10.528715Z","shell.execute_reply.started":"2023-01-11T10:09:10.515390Z","shell.execute_reply":"2023-01-11T10:09:10.527433Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"score = []\npredictions = x['generated']\ntrue_answers = x['answers']\nfor a_pred, a_gold in zip(predictions, true_answers):\n  score.append(compute_f1(a_pred, a_gold))\n\naverage_score = np.mean(score)\nprint(f'average_score: {average_score}')\nx['score'] = score\ntotal = len(x[x['score'] != 0])\nprint(f'length: {total} / {len(x)}')\n","metadata":{"outputId":"247cee84-a44c-444f-9902-3d44a5a0785d","id":"-A27frM4BF37","execution":{"iopub.status.busy":"2023-01-11T10:09:10.530492Z","iopub.execute_input":"2023-01-11T10:09:10.531165Z","iopub.status.idle":"2023-01-11T10:09:10.742088Z","shell.execute_reply.started":"2023-01-11T10:09:10.531125Z","shell.execute_reply":"2023-01-11T10:09:10.740951Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"average_score: 0.15128731932084713\nlength: 1537 / 7917\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ncorrect = x[x['score'] != 0]\ncorrect = correct.reset_index(drop=True)\ncorrect.head(100)","metadata":{"id":"dGdF3olw61_b","outputId":"93298116-94e3-437a-ef3d-6890abd50846","execution":{"iopub.status.busy":"2023-01-11T10:09:10.743815Z","iopub.execute_input":"2023-01-11T10:09:10.744261Z","iopub.status.idle":"2023-01-11T10:09:10.775091Z","shell.execute_reply.started":"2023-01-11T10:09:10.744223Z","shell.execute_reply":"2023-01-11T10:09:10.774097Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"                       generated  \\\n0                             no   \n1                        her mom   \n2                             no   \n3                             no   \n4                             no   \n5                        a paper   \n6               a piece of paper   \n7                             no   \n8               he was murdered.   \n9                 he was a movie   \n10                            no   \n11                            no   \n12                            no   \n13                            no   \n14              go to the school   \n15                           yes   \n16                    his mother   \n17                      new york   \n18                 new york city   \n19                 new york city   \n20                            no   \n21         the city of a village   \n22                   the morning   \n23                            no   \n24  the american research center   \n25                          1967   \n26                          1967   \n27     the united states library   \n28                            no   \n29                            no   \n30                        summer   \n31                            no   \n32                     the lake.   \n33                           yes   \n34                            no   \n35                       germany   \n36                            no   \n37                            no   \n38                            no   \n39                he was killed.   \n40                            no   \n41                            no   \n42                            no   \n43                            no   \n44                            no   \n45                            no   \n46                            no   \n47                            no   \n48                            no   \n49                         a dog   \n50                            no   \n51                            no   \n52                 in a hospital   \n53                            no   \n54                 he was killed   \n55                            no   \n56                            no   \n57                            no   \n58                   three weeks   \n59                   in a garage   \n60                            no   \n61                            no   \n62              to be a new room   \n63               she was broken.   \n64                   three weeks   \n65                            no   \n66                            no   \n67                            no   \n68                            no   \n69                         false   \n70                            no   \n71                            no   \n72                        china.   \n73                            no   \n74                         false   \n75                            no   \n76                            no   \n77                       january   \n78                            no   \n79                           yes   \n80                           yes   \n81                           yes   \n82                           yes   \n83                       a dress   \n84                            no   \n85                    her mother   \n86                           yes   \n87                   the morning   \n88             go to the kitchen   \n89                   three miles   \n90                     two hours   \n91                            no   \n92           he had to be found.   \n93                            no   \n94                           yes   \n95    the battle of westminster.   \n96           england and ireland   \n97               she was hungry.   \n98                       tuesday   \n99                            no   \n\n                                            questions  \\\n0                                 Did she live alone?   \n1                              Who did she live with?   \n2   Was Cotton happy that she looked different tha...   \n3   Did they want Cotton to change the color of he...   \n4                     Did a little boy write the note   \n5                                               What?   \n6                 What kind of dishes does she bring?   \n7                                  Is he still alive?   \n8                    What happened in the early 80's?   \n9                              What happened in 2004?   \n10                Were the characters clothes frumpy?   \n11                 Was he on the show for five years?   \n12                            Was he always an actor?   \n13       Does Quinton live further from the bus stop?   \n14      What do they do every afternoon after school?   \n15                           Did Kendra tell him why?   \n16                                    Who had called?   \n17                                      in what city?   \n18                                         and state?   \n19                                       Where is it?   \n20                          Is it the most populated?   \n21                                               why?   \n22                   When did Reginald Eppes wake up?   \n23                                 Was RJ badly hurt?   \n24                            What does it stand for?   \n25                                 When did it begin?   \n26                   When did the group first gather?   \n27   Which was the first online library through them?   \n28                       Did they go inside the shop?   \n29                               Did the purchase it?   \n30                       What season will it be soon?   \n31                                 Was it a real set?   \n32                                Where'd Tommy live?   \n33                           Did the mice have names?   \n34                  Was he with his unit at the time?   \n35                            What country was he in?   \n36                                Did he drive there?   \n37                      Was his house still standing?   \n38  Did the two brothers keep in contact the past ...   \n39                    Why didn't Franz stay in touch?   \n40                     Did she give up at that point?   \n41                            Were the passages wide?   \n42                              Did he have red hair?   \n43                  Were Harley and Villa in a hurry?   \n44  Did she tell him to think of himself being in ...   \n45  Did she ask Villa to think of her being called...   \n46                    Did Brownie disappear at night?   \n47                           Did his family hunt him?   \n48           Was he home by the end of the next week?   \n49                           What is the story about?   \n50                      Was the woods open and light?   \n51                                           did she?   \n52                                   Where was he at?   \n53                                At his dad's house?   \n54       What did the child do after he shot his dad?   \n55                                       DId he live?   \n56                                  Was he a heretic?   \n57                              Was he superstitious?   \n58                         How long was it published?   \n59                         where were they presently?   \n60                                    in a big house?   \n61                                   was Jenny happy?   \n62                                 what did she want?   \n63                               How did Natasha die?   \n64                  how long was he at the pet store?   \n65                         Was he the smallest puppy?   \n66                        was Peter excited that day?   \n67                Did Sammie take another puppy home?   \n68                     Did he eat from Sammie's hand?   \n69                    True or false: the dog is ugly.   \n70                          Are they book characters?   \n71                             Is the movie American?   \n72                           Where was the film made?   \n73                    Did the dog stay a dog forever?   \n74             True or False: the boy loses the doll.   \n75                                Is the family rich?   \n76                          Did the dad buy the ball?   \n77                  What month was the film released?   \n78                     Was this an efficient process?   \n79                        Did she change her opinion?   \n80                       Did Heather do these things?   \n81             Does she feel the site is a community?   \n82  Did he plan on killing it if it came close to ...   \n83               what did the woman go shopping for ?   \n84                                   did her dad go ?   \n85                                  Who bought a pair   \n86              did she buy the 1st dress she liked ?   \n87                            what time was it then ?   \n88                         what did they do for fun ?   \n89                                 How far is Nevers?   \n90              How long before someone would arrive?   \n91         Was Phillip in a hurry to finish his meal?   \n92                                          For what?   \n93                         Were those folks friendly?   \n94  Did she ask him not to tell her husband someth...   \n95                      What battle was fought in 675   \n96                                Who was it between?   \n97                              Why did she make him?   \n98                          When was spaghetti night?   \n99          did she think her companion was superior?   \n\n                                              answers     score  \n0                                                  no  1.000000  \n1                        with her mommy and 5 sisters  0.250000  \n2                                                  no  1.000000  \n3                                                  no  1.000000  \n4                                                  No  1.000000  \n5                                 a paper carrier bag  0.500000  \n6   hot soup and a container with rice, vegetables...  0.095238  \n7                                                  No  1.000000  \n8                           Farina was cast in a film  0.250000  \n9                           He joined a TV show cast.  0.250000  \n10                                                 No  1.000000  \n11                                                 No  1.000000  \n12                                                 No  1.000000  \n13                                                 No  1.000000  \n14                              go to Quentin's house  0.571429  \n15                                                yes  1.000000  \n16                                   Quinton's mother  0.500000  \n17                                      New York City  0.800000  \n18                                           New York  0.800000  \n19                       In the southwest of the city  0.285714  \n20                                                 no  1.000000  \n21  because the inhabitants feel neglected by the ...  0.200000  \n22                                Five in the morning  0.500000  \n23                                                 No  1.000000  \n24                     Online Computer Library Center  0.285714  \n25                                               1967  1.000000  \n26                                       July 5, 1967  0.500000  \n27                                      Alden Library  0.400000  \n28                                                 No  1.000000  \n29                                                 No  1.000000  \n30                                             summer  1.000000  \n31                                                 No  1.000000  \n32                         by a big lake by the woods  0.333333  \n33                                                yes  1.000000  \n34                                                No.  1.000000  \n35                                            Germany  1.000000  \n36                                                 No  1.000000  \n37                                                 No  1.000000  \n38                                                 No  1.000000  \n39                          He assumed Hans was dead.  0.500000  \n40                                                 No  1.000000  \n41                                                 No  1.000000  \n42                                                 No  1.000000  \n43                                                 No  1.000000  \n44                                                 No  1.000000  \n45                                                 No  1.000000  \n46                                                 No  1.000000  \n47     yes, They went looking for him with no success  0.200000  \n48                                                 no  1.000000  \n49                                  A girl and a dog.  0.500000  \n50                                                 No  1.000000  \n51                                                 no  1.000000  \n52                         in the front seat of a SUV  0.285714  \n53                                                 no  1.000000  \n54  He exited the back of the vehicle and continue...  0.142857  \n55                                                 No  1.000000  \n56                                                 no  1.000000  \n57                                                 no  1.000000  \n58                                     for some weeks  0.400000  \n59                                         in Seattle  0.500000  \n60                                                 no  1.000000  \n61                                                 no  1.000000  \n62                                 to play with Jenny  0.250000  \n63                    She  fell on a beginners' slope  0.250000  \n64                                       three months  0.500000  \n65                                                 no  1.000000  \n66                                                 no  1.000000  \n67                                                 no  1.000000  \n68                                                 no  1.000000  \n69                                              false  1.000000  \n70                                                 no  1.000000  \n71                                                 no  1.000000  \n72                                             China.  1.000000  \n73                                                 no  1.000000  \n74                                              False  1.000000  \n75                                                 no  1.000000  \n76                                                 no  1.000000  \n77                                            January  1.000000  \n78                                                 no  1.000000  \n79                                                yes  1.000000  \n80                                                yes  1.000000  \n81                                                yes  1.000000  \n82                                                Yes  1.000000  \n83                                            a dress  1.000000  \n84                                                 No  1.000000  \n85                                            Her mom  0.500000  \n86                                                yes  1.000000  \n87                                nine in the morning  0.500000  \n88                        went to more stores to shop  0.222222  \n89                                  twelve miles away  0.400000  \n90                               four hours, at least  0.333333  \n91                                                 no  1.000000  \n92                     for carrying  the news to them  0.200000  \n93                                                 no  1.000000  \n94                                                yes  1.000000  \n95                               The Battle of Bedwyn  0.666667  \n96                           Escuin and King Wulfhere  0.285714  \n97                            It was spaghetti night.  0.285714  \n98                                     Tuesday night.  0.666667  \n99                                                 no  1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>generated</th>\n      <th>questions</th>\n      <th>answers</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>no</td>\n      <td>Did she live alone?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>her mom</td>\n      <td>Who did she live with?</td>\n      <td>with her mommy and 5 sisters</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>no</td>\n      <td>Was Cotton happy that she looked different tha...</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>no</td>\n      <td>Did they want Cotton to change the color of he...</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>no</td>\n      <td>Did a little boy write the note</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>a paper</td>\n      <td>What?</td>\n      <td>a paper carrier bag</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>a piece of paper</td>\n      <td>What kind of dishes does she bring?</td>\n      <td>hot soup and a container with rice, vegetables...</td>\n      <td>0.095238</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>no</td>\n      <td>Is he still alive?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>he was murdered.</td>\n      <td>What happened in the early 80's?</td>\n      <td>Farina was cast in a film</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>he was a movie</td>\n      <td>What happened in 2004?</td>\n      <td>He joined a TV show cast.</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>no</td>\n      <td>Were the characters clothes frumpy?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>no</td>\n      <td>Was he on the show for five years?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>no</td>\n      <td>Was he always an actor?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>no</td>\n      <td>Does Quinton live further from the bus stop?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>go to the school</td>\n      <td>What do they do every afternoon after school?</td>\n      <td>go to Quentin's house</td>\n      <td>0.571429</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>yes</td>\n      <td>Did Kendra tell him why?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>his mother</td>\n      <td>Who had called?</td>\n      <td>Quinton's mother</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>new york</td>\n      <td>in what city?</td>\n      <td>New York City</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>new york city</td>\n      <td>and state?</td>\n      <td>New York</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>new york city</td>\n      <td>Where is it?</td>\n      <td>In the southwest of the city</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>no</td>\n      <td>Is it the most populated?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>the city of a village</td>\n      <td>why?</td>\n      <td>because the inhabitants feel neglected by the ...</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>the morning</td>\n      <td>When did Reginald Eppes wake up?</td>\n      <td>Five in the morning</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>no</td>\n      <td>Was RJ badly hurt?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>the american research center</td>\n      <td>What does it stand for?</td>\n      <td>Online Computer Library Center</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1967</td>\n      <td>When did it begin?</td>\n      <td>1967</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1967</td>\n      <td>When did the group first gather?</td>\n      <td>July 5, 1967</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>the united states library</td>\n      <td>Which was the first online library through them?</td>\n      <td>Alden Library</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>no</td>\n      <td>Did they go inside the shop?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>no</td>\n      <td>Did the purchase it?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>summer</td>\n      <td>What season will it be soon?</td>\n      <td>summer</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>no</td>\n      <td>Was it a real set?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>the lake.</td>\n      <td>Where'd Tommy live?</td>\n      <td>by a big lake by the woods</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>yes</td>\n      <td>Did the mice have names?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>no</td>\n      <td>Was he with his unit at the time?</td>\n      <td>No.</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>germany</td>\n      <td>What country was he in?</td>\n      <td>Germany</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>no</td>\n      <td>Did he drive there?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>no</td>\n      <td>Was his house still standing?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>no</td>\n      <td>Did the two brothers keep in contact the past ...</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>he was killed.</td>\n      <td>Why didn't Franz stay in touch?</td>\n      <td>He assumed Hans was dead.</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>no</td>\n      <td>Did she give up at that point?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>no</td>\n      <td>Were the passages wide?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>no</td>\n      <td>Did he have red hair?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>no</td>\n      <td>Were Harley and Villa in a hurry?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>no</td>\n      <td>Did she tell him to think of himself being in ...</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>no</td>\n      <td>Did she ask Villa to think of her being called...</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>no</td>\n      <td>Did Brownie disappear at night?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>no</td>\n      <td>Did his family hunt him?</td>\n      <td>yes, They went looking for him with no success</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>no</td>\n      <td>Was he home by the end of the next week?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>a dog</td>\n      <td>What is the story about?</td>\n      <td>A girl and a dog.</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>no</td>\n      <td>Was the woods open and light?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>no</td>\n      <td>did she?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>in a hospital</td>\n      <td>Where was he at?</td>\n      <td>in the front seat of a SUV</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>no</td>\n      <td>At his dad's house?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>he was killed</td>\n      <td>What did the child do after he shot his dad?</td>\n      <td>He exited the back of the vehicle and continue...</td>\n      <td>0.142857</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>no</td>\n      <td>DId he live?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>no</td>\n      <td>Was he a heretic?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>no</td>\n      <td>Was he superstitious?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>three weeks</td>\n      <td>How long was it published?</td>\n      <td>for some weeks</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>in a garage</td>\n      <td>where were they presently?</td>\n      <td>in Seattle</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>no</td>\n      <td>in a big house?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>no</td>\n      <td>was Jenny happy?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>to be a new room</td>\n      <td>what did she want?</td>\n      <td>to play with Jenny</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>she was broken.</td>\n      <td>How did Natasha die?</td>\n      <td>She  fell on a beginners' slope</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>three weeks</td>\n      <td>how long was he at the pet store?</td>\n      <td>three months</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>no</td>\n      <td>Was he the smallest puppy?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>no</td>\n      <td>was Peter excited that day?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>no</td>\n      <td>Did Sammie take another puppy home?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>no</td>\n      <td>Did he eat from Sammie's hand?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>false</td>\n      <td>True or false: the dog is ugly.</td>\n      <td>false</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>no</td>\n      <td>Are they book characters?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>no</td>\n      <td>Is the movie American?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>china.</td>\n      <td>Where was the film made?</td>\n      <td>China.</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>no</td>\n      <td>Did the dog stay a dog forever?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>false</td>\n      <td>True or False: the boy loses the doll.</td>\n      <td>False</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>no</td>\n      <td>Is the family rich?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>no</td>\n      <td>Did the dad buy the ball?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>january</td>\n      <td>What month was the film released?</td>\n      <td>January</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>no</td>\n      <td>Was this an efficient process?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>yes</td>\n      <td>Did she change her opinion?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>yes</td>\n      <td>Did Heather do these things?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>yes</td>\n      <td>Does she feel the site is a community?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>yes</td>\n      <td>Did he plan on killing it if it came close to ...</td>\n      <td>Yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>a dress</td>\n      <td>what did the woman go shopping for ?</td>\n      <td>a dress</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>no</td>\n      <td>did her dad go ?</td>\n      <td>No</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>her mother</td>\n      <td>Who bought a pair</td>\n      <td>Her mom</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>yes</td>\n      <td>did she buy the 1st dress she liked ?</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>the morning</td>\n      <td>what time was it then ?</td>\n      <td>nine in the morning</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>go to the kitchen</td>\n      <td>what did they do for fun ?</td>\n      <td>went to more stores to shop</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>three miles</td>\n      <td>How far is Nevers?</td>\n      <td>twelve miles away</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>two hours</td>\n      <td>How long before someone would arrive?</td>\n      <td>four hours, at least</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>no</td>\n      <td>Was Phillip in a hurry to finish his meal?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>he had to be found.</td>\n      <td>For what?</td>\n      <td>for carrying  the news to them</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>no</td>\n      <td>Were those folks friendly?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>yes</td>\n      <td>Did she ask him not to tell her husband someth...</td>\n      <td>yes</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>the battle of westminster.</td>\n      <td>What battle was fought in 675</td>\n      <td>The Battle of Bedwyn</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>england and ireland</td>\n      <td>Who was it between?</td>\n      <td>Escuin and King Wulfhere</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>she was hungry.</td>\n      <td>Why did she make him?</td>\n      <td>It was spaghetti night.</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>tuesday</td>\n      <td>When was spaghetti night?</td>\n      <td>Tuesday night.</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>no</td>\n      <td>did she think her companion was superior?</td>\n      <td>no</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### BERT2BERT Distilroberta-base","metadata":{"id":"G2-Rwv5gM8fS"}},{"cell_type":"code","source":"#entire dataset\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])","metadata":{"id":"7fSwmQReM8fS","execution":{"iopub.status.busy":"2023-01-11T10:09:10.776864Z","iopub.execute_input":"2023-01-11T10:09:10.777366Z","iopub.status.idle":"2023-01-11T10:09:10.804810Z","shell.execute_reply.started":"2023-01-11T10:09:10.777324Z","shell.execute_reply":"2023-01-11T10:09:10.803843Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"#take a subset from the training set\nstart = 0\nend = 95000\ncontexts = list(train_df['p'])\nquestions = list(train_df['q'])\nanswers = list(train_df['a'])\ncontexts = contexts[start:end]\nquestions = questions[start:end]\nanswers = answers[start:end]\nlen(contexts)","metadata":{"id":"x_gmV73fM8fS","execution":{"iopub.status.busy":"2023-01-11T10:09:10.806334Z","iopub.execute_input":"2023-01-11T10:09:10.806751Z","iopub.status.idle":"2023-01-11T10:09:10.839201Z","shell.execute_reply.started":"2023-01-11T10:09:10.806710Z","shell.execute_reply":"2023-01-11T10:09:10.838011Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"82240"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import EncoderDecoderModel, AutoTokenizer\nfrom tqdm import tqdm\n\n\nmodel_name = 'distilroberta-base'\n\n# tie_encoder_decoder to share weights and half the number of parameters\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name, tie_encoder_decoder=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# set special tokens\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# set decoding params                               \nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.repetition_penalty = 5.0\nmodel.config.num_beams = 2\nmodel.config.vocab_size = model.config.encoder.vocab_size\n","metadata":{"id":"5SwrherKM8fT","execution":{"iopub.status.busy":"2023-01-11T10:09:10.841042Z","iopub.execute_input":"2023-01-11T10:09:10.841457Z","iopub.status.idle":"2023-01-11T10:09:18.177529Z","shell.execute_reply.started":"2023-01-11T10:09:10.841420Z","shell.execute_reply":"2023-01-11T10:09:18.176438Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe following encoder weights were not tied to the decoder ['roberta/pooler']\n","output_type":"stream"}]},{"cell_type":"code","source":"encodings = tokenizer(questions, contexts, \n                          padding=True,\n                          truncation= True,\n                          max_length = 512,\n                          )\ninput_ids, input_attention_mask = encodings['input_ids'], encodings['attention_mask']\nlabel_values = tokenizer(answers,\n                          padding=True,\n                          truncation=True,\n                          max_length = 25,\n                          )\nlabels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n\n\n\n#Tokens with indices set to ``-100`` are ignored (masked) during training, the loss is only computed for the tokens with labels\nmasked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\nprint(f'length of input_ids: {np.shape(input_ids)}')","metadata":{"id":"OxXjWb25M8fT","execution":{"iopub.status.busy":"2023-01-11T10:09:18.179104Z","iopub.execute_input":"2023-01-11T10:09:18.179548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.keys()","metadata":{"id":"XfyOa50nM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings.update({'labels': masked_labels})\nencodings.keys()","metadata":{"outputId":"15a092d1-a926-4d0a-9dcc-6c0350013947","id":"2TQ6POAgM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CreateDataset(Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)","metadata":{"id":"dxTWgEfWM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parameters\nbatch_size = 16\nnum_epochs = 3\n#also try with lr = 4e-4\nlr = 4e-5","metadata":{"id":"b5qfLBOJM8fT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer\n#create training dataset\ntrain_dataset = CreateDataset(encodings)\n#create training dataloader\ntrain_ld = torch.utils.data.DataLoader(train_dataset,\n                                     batch_size=batch_size,\n                                     )\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\noptim = torch.optim.AdamW(model.parameters(), lr=lr)\nloop_start = timer()\nfor epoch in range(num_epochs):\n    model.train()\n    loss_score = []\n    loop = tqdm(train_ld)\n    for batch in loop:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        loss, outputs = model(input_ids,\n                              attention_mask=attention_mask,\n                              labels = labels\n                        )[:2]\n        loss_score.append(loss.item())\n        loss.backward()\n        optim.step()\n\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())\n    average_loss = np.mean(loss_score)\n    print(f\"Epoch: {epoch}, average Loss: {average_loss}\")\nloop_end = timer()\ntime_loop = loop_end - loop_start\nprint(f'\\nTime for {num_epochs} epochs (s): {(time_loop):.3f}')","metadata":{"id":"xKdl21v2M8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Free some memory\nimport gc\ndel encodings,input_ids,input_attention_mask,labels\ntorch.cuda.empty_cache()\ntorch.cuda.reset_accumulated_memory_stats()\ngc.collect()","metadata":{"outputId":"4224b9df-5f72-419d-b753-7e415545734f","id":"Z9UrY8RhM8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####Generation","metadata":{"id":"Xqq0pd7RM8fU"}},{"cell_type":"markdown","source":"Load test dataset.","metadata":{"id":"IBgEOzIDM8fU"}},{"cell_type":"code","source":"test_df = loadDataset(\"test.json\")\ntest_df.count()","metadata":{"outputId":"3c241dad-a623-4853-c8b4-e6fb294a7cb2","id":"j0jqxRQLM8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_test = list(test_df['p'])\nquestion_test = list(test_df['q'])\nanswer_test = list(test_df['a'])","metadata":{"id":"gkMeNLtqM8fU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_values = tokenizer(question_test,context_test, padding=True, truncation=True, max_length = 512)\ninput_ids, input_attention_mask = input_values['input_ids'], input_values['attention_mask']","metadata":{"id":"LYdbBdsgM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#utility functions taken from the allennlp library for computing the F1-score\nimport collections\nimport re\nimport string\nfrom typing import Callable, Sequence, TypeVar, Tuple\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef compute_f1(a_pred: str, a_gold: str) -> float:\n    pred_toks = get_tokens(a_pred)\n    gold_toks = get_tokens(a_gold)\n    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n    num_same = sum(common.values())\n    if len(pred_toks) == 0 or len(gold_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return float(pred_toks == gold_toks)\n    if num_same == 0:\n        return 0.0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1","metadata":{"id":"WcL-JtVqM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nl = []\nmodel.to(device)\n#set the model in evaluation mode\nmodel.eval()\nfor input, mask in zip(input_ids,input_attention_mask):\n  input = np.expand_dims(np.array(input), axis=0)\n  mask = np.expand_dims(np.array(mask), axis=0)\n  generated = model.generate(input_ids=torch.tensor(input).to(device),\n                                                 max_length=20,\n                                                 repetition_penalty=5.,\n                                                 min_length=1,\n                                                 no_repeat_ngram_size=3,\n                                                 early_stopping=True,\n                                                decoder_start_token_id = model.config.decoder_start_token_id,\n                                                 num_beams=2,\n                                                 )\n  generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n  l.append(generated)","metadata":{"id":"H1h4HFwTM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.DataFrame(l, columns = ['generated'])\nx['answers'] = answer_test\n#pd.set_option('display.max_rows', None)\n#x.head(300)","metadata":{"id":"ESKHnK6uM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = []\npredictions = x['generated']\ntrue_answers = x['answers']\nfor a_pred, a_gold in zip(predictions, true_answers):\n  score.append(compute_f1(a_pred, a_gold))\naverage_score = np.mean(score)\nprint(f'average_score: {average_score}')\nx['score'] = score\ntotal = len(x[x['score'] != 0])\nprint(f'length: {total} / {len(x)}')\n","metadata":{"id":"uUnYQmXGM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ncorrect = x[x['score'] != 0]\ncorrect = correct.reset_index(drop=True)\ncorrect.head(500)","metadata":{"id":"rH5l9jFTM8fV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Question generation $f_\\theta(P, Q, H)$ with text passage $P$, question $Q$ and dialogue history $H$","metadata":{"id":"scfOBhZf1mPp"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"9maQVnw61mPp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$","metadata":{"id":"WpTttkRT1mPq"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"j9Mb67HE1mPq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions","metadata":{"id":"JmviMChy1mPq"}},{"cell_type":"code","source":"# TODO","metadata":{"id":"jDn-h3WL1mPq","trusted":true},"execution_count":null,"outputs":[]}]}