{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbMpqPm1mPC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danysan1/ai-unibo-nlp-project/blob/main/a2/execution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtlMx4kv1mPK"
      },
      "source": [
        "# Assignment 2 execution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test file implementing BERT2BERT using Bert-Tiny model\n",
        "\n",
        "Adapted the given example. DistilRoBERTa seems too heavy to work with TF, Bert-Tiny can actually be runned with really ugly results, getting out of memory using 1000 samples..\n",
        "Only 3 epochs used as said in the assignement."
      ],
      "metadata": {
        "id": "czVKE_lWGDuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feevAAsT1mPL",
        "outputId": "8bee43fc-908d-4a7c-8d49-1c29e70f87f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 9.5 MB/s \n",
            "\u001b[?25hCollecting dataset\n",
            "  Downloading dataset-1.5.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Collecting banal>=1.0.1\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from dataset) (1.4.45)\n",
            "Collecting alembic>=0.6.2\n",
            "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 73.6 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=0.6.2->dataset) (5.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=0.6.2->dataset) (5.10.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.2->dataset) (2.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: Mako, tokenizers, huggingface-hub, banal, alembic, transformers, tensorflow-addons, dataset\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.1 banal-1.0.6 dataset-1.5.2 huggingface-hub-0.11.1 tensorflow-addons-0.19.0 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy matplotlib transformers dataset tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJSsWQg_1mPO"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mXm1Fpq1mPP"
      },
      "source": [
        "### Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oOqlik_a1mPP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sskRUOtB1mPR"
      },
      "outputs": [],
      "source": [
        "data_folder = 'Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CzsJWqI61mPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa707273-468f-47af-a4c4-0d5199783f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CoQA train data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-train-v1.0.json: 49.0MB [00:08, 5.86MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n",
            "Downloading CoQA test data split... (it may take a while)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "coqa-dev-v1.0.json: 9.09MB [00:03, 2.84MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path=data_folder, url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path=data_folder, url_path=test_url, suffix='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsM9znVV1mPT"
      },
      "source": [
        "### Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VRsmY5Wn1mPV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from os import path\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lI74kMng1mPW"
      },
      "outputs": [],
      "source": [
        "def loadDataset(filename):\n",
        "    with open(path.join(data_folder, filename)) as file_obj:\n",
        "        df = json.load(file_obj)[\"data\"]\n",
        "    print(f'{len(df)} stories / {len(df[0][\"questions\"])} questions in the first row')\n",
        "\n",
        "    storyDType = pd.CategoricalDtype(pd.unique([story[\"story\"] for story in df]))\n",
        "    sourceDType = pd.CategoricalDtype(pd.unique([story[\"source\"] for story in df]))\n",
        "    print(f\"Sources: {sourceDType.categories}\")\n",
        "\n",
        "    df = np.array([\n",
        "        [\n",
        "            sourceDType.categories.get_loc(story[\"source\"]), # Sources factorization\n",
        "            storyDType.categories.get_loc(story[\"story\"]), # Sources factorization\n",
        "            story[\"questions\"][question_index][\"input_text\"],\n",
        "            story[\"answers\"][question_index][\"input_text\"],\n",
        "            story[\"answers\"][question_index][\"span_text\"],\n",
        "        ]\n",
        "        for story in df\n",
        "        for question_index in range(len(story[\"questions\"]))\n",
        "        if story[\"answers\"][question_index][\"input_text\"] != 'unknown'\n",
        "    ])\n",
        "    print(f'{df.shape} question-answer pairs x columns')\n",
        "    print(f'First row: {df[0]}')\n",
        "    \n",
        "    # https://marcobonzanini.com/2021/09/15/tips-for-saving-memory-with-pandas/\n",
        "    # https://pandas.pydata.org/docs/user_guide/categorical.html\n",
        "    df = pd.DataFrame({\n",
        "        \"source\": pd.Series(pd.Categorical.from_codes(df[:,0].astype(np.int16), dtype=sourceDType)),\n",
        "        \"p\": pd.Series(pd.Categorical.from_codes(df[:,1].astype(np.int16), dtype=storyDType)),\n",
        "        \"q\": df[:,2],\n",
        "        \"a\": df[:,3],\n",
        "        \"span\": df[:,4],\n",
        "    })\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsZrgF5d1mPX",
        "outputId": "cf5407a2-cc55-44f9-c1e1-715d19c60c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7199 stories / 20 questions in the first row\n",
            "Sources: Index(['wikipedia', 'cnn', 'gutenberg', 'race', 'mctest'], dtype='object')\n",
            "(107276, 5) question-answer pairs x columns\n",
            "First row: ['0' '0' 'When was the Vat formally opened?'\n",
            " 'It was formally established in 1475' 'Formally established in 1475']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source    107276\n",
              "p         107276\n",
              "q         107276\n",
              "a         107276\n",
              "span      107276\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_df = loadDataset(\"train.json\")\n",
        "train_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOQSe3Sc1mPY",
        "outputId": "2b55a379-8fa3-45f0-af24-49f612774d41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6605"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "pd.unique(train_df[\"p\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJaLGY_p1mPZ",
        "outputId": "dff1f19f-29d6-457f-b62a-f5f15179aac7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99470"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pd.unique(train_df[\"span\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ml0A1b1mPZ",
        "outputId": "9d978660-8495-4841-adb5-fca317e3331f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "pd.unique(train_df[\"source\"]).size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "G9VjurfZ1mPZ",
        "outputId": "41157964-10bb-4d3f-f799-b17ec149e091"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      source                                                  p  \\\n",
              "0  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "1  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "2  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "3  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "4  wikipedia  The Vatican Apostolic Library (), more commonl...   \n",
              "\n",
              "                                   q                                    a  \\\n",
              "0  When was the Vat formally opened?  It was formally established in 1475   \n",
              "1           what is the library for?                             research   \n",
              "2                 for what subjects?                     history, and law   \n",
              "3                               and?     philosophy, science and theology   \n",
              "4          what was started in 2014?                           a  project   \n",
              "\n",
              "                                                span  \n",
              "0                       Formally established in 1475  \n",
              "1           he Vatican Library is a research library  \n",
              "2  Vatican Library is a research library for hist...  \n",
              "3  Vatican Library is a research library for hist...  \n",
              "4  March 2014, the Vatican Library began an initi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d90bd3b7-b7a7-4ad5-9821-42bde5f5236b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>p</th>\n",
              "      <th>q</th>\n",
              "      <th>a</th>\n",
              "      <th>span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>When was the Vat formally opened?</td>\n",
              "      <td>It was formally established in 1475</td>\n",
              "      <td>Formally established in 1475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what is the library for?</td>\n",
              "      <td>research</td>\n",
              "      <td>he Vatican Library is a research library</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>for what subjects?</td>\n",
              "      <td>history, and law</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>and?</td>\n",
              "      <td>philosophy, science and theology</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>what was started in 2014?</td>\n",
              "      <td>a  project</td>\n",
              "      <td>March 2014, the Vatican Library began an initi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d90bd3b7-b7a7-4ad5-9821-42bde5f5236b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d90bd3b7-b7a7-4ad5-9821-42bde5f5236b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d90bd3b7-b7a7-4ad5-9821-42bde5f5236b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrRkHhES1mPa",
        "outputId": "da05640d-b904-4d0f-ab17-00c9852d867d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index          128\n",
              "source      107764\n",
              "p         14241201\n",
              "q          9110271\n",
              "a          7714559\n",
              "span      12090637\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_df.memory_usage(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v5aer1n41mPa"
      },
      "outputs": [],
      "source": [
        "#test_df = loadDataset(\"test.json\")\n",
        "#test_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRs7YHKY1mPa"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0GmOXam1mPb"
      },
      "source": [
        "### Check unanswerable questions in the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aq524zd1mPb",
        "outputId": "10f5e64e-4d14-4af3-87ed-1e72a08f049b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "idx = (train_df.a == 'unknown')\n",
        "unanswerable = train_df[idx]\n",
        "unanswerable.q.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTdjLFYI1mPb"
      },
      "source": [
        "All unanswerable questions in the Train Dataset have been already removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maBslb2n1mPb"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "VH5Ep77e1mPb",
        "outputId": "1814f26a-327c-4d00-9573-09f6537204f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"Lassiter, will you be my rider?\" Jane had asked him. \\n\\n\"I reckon so,\" he had replied. \\n\\nFew as the words were, Jane knew how infinitely much they implied. She wanted him to take charge of her cattle and horse and ranges, and save them if that were possible. Yet, though she could not have spoken aloud all she meant, she was perfectly honest with herself. Whatever the price to be paid, she must keep Lassiter close to her; she must shield from him the man who had led Milly Erne to Cottonwoods. In her fear she so controlled her mind that she did not whisper this Mormon\\'s name to her own soul, she did not even think it. Besides, beyond this thing she regarded as a sacred obligation thrust upon her, was the need of a helper, of a friend, of a champion in this critical time. If she could rule this gun-man, as Venters had called him, if she could even keep him from shedding blood, what strategy to play his flame and his presence against the game of oppression her churchmen were waging against her? Never would she forget the effect on Tull and his men when Venters shouted Lassiter\\'s name. If she could not wholly control Lassiter, then what she could do might put off the fatal day. \\n\\nOne of her safe racers was a dark bay, and she called him Bells because of the way he struck his iron shoes on the stones. When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes. A rider\\'s love of a thoroughbred shone in them. Round and round Bells he walked, plainly weakening all the time in his determination not to take one of Jane\\'s favorite racers. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_df[\"p\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zYcgBVcG1mPc",
        "outputId": "6fa985f3-41f4-4914-88b6-75fcb55d2b78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Was Lassiter impressed with the horse?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_df[\"q\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5zvn9Tv01mPc",
        "outputId": "ad49ab05-d52c-4709-c5d9-62d1349601b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_df[\"a\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NokHfoLm1mPc",
        "outputId": "32318a0b-8015-488e-f227-ecb9c14d0503"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When Jerd led out this slender, beautifully built horse Lassiter suddenly became all eyes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_df[\"span\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JskMPh-Y1mPc",
        "outputId": "604b4df9-ac4c-4b98-a68f-2dc7121e0d00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gutenberg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_df[\"source\"][42]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8lIuR9RO1mPd"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm2QCJbR1mPd"
      },
      "source": [
        "## Train-Validation-Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lmr_hj9-1mPd"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SICbZ6vB1mPd"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wuh0_ic1mPd"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4pKuTsYt1mPd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from typing import List, Dict, Callable\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YutAeLih1mPe"
      },
      "outputs": [],
      "source": [
        "def predict_data(model: keras.Model,\n",
        "                x: np.ndarray,\n",
        "                prediction_info: Dict):\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "    print(f'Starting prediction: \\n{prediction_info}')\n",
        "    print(f'Predicting on {x.shape[0]} samples')\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CS_dd2On1mPe"
      },
      "outputs": [],
      "source": [
        "def compute_f1(model: keras.Model, \n",
        "             x: np.ndarray, \n",
        "             y: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute F1_score on the given data with corresponding labels\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: data in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "\n",
        "    :return\n",
        "        score: f1_macro_score\n",
        "    \"\"\"\n",
        "    #predictions on the x set\n",
        "    prediction_info = {\n",
        "        'batch_size': 64,\n",
        "        'verbose': 1\n",
        "    }\n",
        "    y_pred = predict_data(model=model, x=x, prediction_info=prediction_info)\n",
        "\n",
        "    #compute argmax to take the best class for each sample\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    #compute the f1_macro\n",
        "    score = f1_score(y, y_pred, average ='macro')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1-akLLJo1mPe"
      },
      "outputs": [],
      "source": [
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "61_GSxvB1mPe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from transformers import TFAutoModel, AutoTokenizer, TFEncoderDecoderModel\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aFTVbjH1mPf"
      },
      "source": [
        "### Question generation $f_\\theta(P, Q)$ with text passage $P$ and question $Q$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT2BERT Bert-Tiny"
      ],
      "metadata": {
        "id": "UzY46KMnEvqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0hRPN5wr0o6w"
      },
      "outputs": [],
      "source": [
        "class MyTrainer(object):\n",
        "    \"\"\"\n",
        "    Simple wrapper class\n",
        "\n",
        "    train_op -> uses tf.GradientTape to compute the loss\n",
        "    batch_fit -> receives a batch and performs forward-backward passes (gradient included)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, keras_model):\n",
        "        self.keras_model = keras_model\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=5e-05)\n",
        "\n",
        "    @tf.function\n",
        "    def compute_loss(self, inputs):\n",
        "        loss = self.keras_model(inputs=inputs)\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train_op(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(inputs=inputs)\n",
        "\n",
        "        grads = tape.gradient(loss, self.keras_model.trainable_variables)\n",
        "        return loss, grads\n",
        "\n",
        "    @tf.function\n",
        "    def batch_fit(self, inputs):\n",
        "        loss, grads = self.train_op(inputs=inputs)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.keras_model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Custom keras model that wraps the TFEncoderDecoderModel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # tie_encoder_decoder to share weights and half the number of parameters\n",
        "        self.model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name,\n",
        "                                                                           encoder_from_pt=True,\n",
        "                                                                           decoder_from_pt=True,\n",
        "                                                                           tie_encoder_decoder=True)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        loss = self.model(input_ids=inputs['input_ids'],\n",
        "                          attention_mask=inputs['input_attention_mask'],\n",
        "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
        "                          decoder_attention_mask=inputs['labels_mask'],\n",
        "                          labels=inputs['labels']).loss\n",
        "        return loss\n",
        "\n",
        "    def generate(self, **kwargs):\n",
        "        return self.model.generate(decoder_start_token_id=self.model.config.decoder.pad_token_id,\n",
        "                                   **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QTL2_cAP0o6w"
      },
      "outputs": [],
      "source": [
        "# Download the model only if not already present.\n",
        "# Saving  it in ./models/\n",
        "\n",
        "def get_tokenizer(model_name):  \n",
        "    data_path = \"models\"  \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, model_name)\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading model {model_name}... (it may take a while)\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, tie_encoder_decoder=True)\n",
        "        tokenizer.save_pretrained(data_path)\n",
        "        print(f\"Download completed, saved in {data_path}!\")\n",
        "    else:\n",
        "        print(f'Model already downloaded, loading from {data_path}')\n",
        "        tokenizer = AutoTokenizer.from_pretrained(data_path, tie_encoder_decoder=True)\n",
        "\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'prajjwal1/bert-tiny'\n",
        "\n",
        "#tokenizer = get_tokenizer(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, tie_encoder_decoder=True)\n",
        "model = MyModel(model_name=model_name)\n",
        "\n",
        "model.model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.model.config_eos_token_id = tokenizer.sep_token_id\n",
        "model.model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.model.config.vocab_size = model.model.config.encoder.vocab_size\n",
        "\n",
        "trainer = MyTrainer(keras_model=model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2DrRPNL1bz1",
        "outputId": "37ec8816-b1f0-49e3-87c5-f64273e48950"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertModel.\n",
            "\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at /tmp/tmpt_4a0brz.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertLMHeadModel: ['bert.embeddings.position_ids', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing TFBertLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFBertLMHeadModel were not initialized from the PyTorch model and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
            "\n",
            "All the layers of TFBertLMHeadModel were initialized from the model checkpoint at /tmp/tmpzk9_f9fk.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 100\n",
        "\n",
        "sample_questions = list(train_df['q'][:samples])\n",
        "sample_spans = list(train_df['span'][:samples])\n",
        "sample_answers = list(train_df['a'][:samples])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mB7NJ3XA8jQZ"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IziJzgiF0o6x",
        "outputId": "a8cb5204-1a3b-4e00-c6a8-09162f63e449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2043\twhen\n",
            "2001\twas\n",
            "1996\tthe\n",
            "12436\tva\n",
            "2102\t##t\n",
            "6246\tformally\n",
            "2441\topened\n",
            "1029\t?\n",
            "6246\tformally\n",
            "2511\testablished\n",
            "1999\tin\n",
            "16471\t147\n",
            "2629\t##5\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n",
            "0\t[PAD]\n"
          ]
        }
      ],
      "source": [
        "# input containing the question and the span\n",
        "input_qs = tokenizer(sample_questions, sample_spans, add_special_tokens=False, padding=True)\n",
        "input_ids, input_attention_mask = input_qs['input_ids'], input_qs['attention_mask']\n",
        "\n",
        "# labels containing the answer\n",
        "label_values = tokenizer(sample_answers, padding=True)\n",
        "labels, labels_mask = label_values['input_ids'], label_values['attention_mask']\n",
        "\n",
        "# Every labels has the same len(16), cause of padding=True\n",
        "max_length = len(labels[0])\n",
        "\n",
        "# Assigning id -100 to every padding token in labels\n",
        "masked_labels = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in labels]\n",
        "\n",
        "for idx in input_ids[0]:\n",
        "    print(\"{}\\t{}\".format(idx, tokenizer.convert_ids_to_tokens(idx) if idx != -100 else \"PAD\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FNZIJr0o6x",
        "outputId": "1ef64620-783d-4d2c-a95f-52008acbc76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['tf_encoder_decoder_model/encoder/bert/pooler/dense/kernel:0', 'tf_encoder_decoder_model/encoder/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 -- Loss 19.424400329589844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [01:09<02:18, 69.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: [') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - west west west - south', ') ( ( back north ) east east east - south east east west -', ') ( ( back north ) east east east - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - west west west - south', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( see see ) ) here here somewhere somewhere somewhere else else else', ') ( ( water water ) water water and water water or, water water', ') ( ( see see ) ) here here somewhere somewhere somewhere else else else', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( see see ) ) here here somewhere somewhere somewhere else else else', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( see see ) ) here here somewhere somewhere somewhere else else else', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east']\n",
            "Epoch 1 -- Loss 18.556257247924805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [02:16<01:08, 68.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: [') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - south east east west -', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) east east east - south east east west -', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back right right right - to way the of rest the of', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east']\n",
            "Epoch 2 -- Loss 17.696500778198242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [03:39<00:00, 73.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: [') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) east east east - south east east west -', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - west west west - south', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - west west west - south', ') ( ( water water ) water water water and water water or, water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) west west west - south west - east east', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( water water ) water water and water water or, water water', ') ( ( back north ) east east east - west west west - south']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    batch = {'input_ids': tf.convert_to_tensor(input_ids, dtype=tf.int32),\n",
        "                'input_attention_mask': tf.convert_to_tensor(input_attention_mask, dtype=tf.int32),\n",
        "                'labels': tf.convert_to_tensor(labels_mask, dtype=tf.int32),\n",
        "                'decoder_input_ids': tf.convert_to_tensor(deepcopy(labels), dtype=tf.int32),\n",
        "                'labels_mask': tf.convert_to_tensor(labels_mask, dtype=tf.int32)\n",
        "                }\n",
        "    loss = trainer.batch_fit(inputs=batch)\n",
        "    print(f'Epoch {epoch} -- Loss {loss}')\n",
        "    # You can play with generation arguments to enforce\n",
        "    #  beam search\n",
        "    #  repetition penalty\n",
        "    #  other sampling approaches\n",
        "    generated = trainer.keras_model.generate(input_ids=tf.convert_to_tensor(input_ids, dtype=tf.int32),\n",
        "                                                max_length=max_length,\n",
        "                                                repetition_penalty=3.,\n",
        "                                                min_length=5,\n",
        "                                                no_repeat_ngram_size=3,\n",
        "                                                early_stopping=True,\n",
        "                                                num_beams=4\n",
        "                                                )\n",
        "    generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "    print(f'Generated: {generated}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - Model evaluation"
      ],
      "metadata": {
        "id": "O4hA5oozMoAs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "9dc38c7bb1eb184e6af951a9961d56502591ce7eff562d9f88b7a8755afb206b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}